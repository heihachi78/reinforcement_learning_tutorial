{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Markov Decision Processes and Bellman Equations\n",
    "\n",
    "Welcome to Part 2 of our Reinforcement Learning tutorial series! In this notebook, we'll formalize the RL problem using **Markov Decision Processes (MDPs)** and derive the fundamental **Bellman equations**.\n",
    "\n",
    "## What This Notebook Covers\n",
    "- Markov Processes (Markov Chains) - the simplest building block\n",
    "- Markov Reward Processes (MRP) - adding rewards to the mix\n",
    "- Markov Decision Processes (MDP) - the full RL framework\n",
    "- The Return and discounting future rewards\n",
    "- Bellman Expectation Equations - recursive value relationships\n",
    "- Optimal Value Functions and Bellman Optimality Equations\n",
    "\n",
    "## What This Notebook Does NOT Cover\n",
    "- Algorithms to solve MDPs (covered in notebook 03: Dynamic Programming)\n",
    "- Learning from experience without a model (covered in notebooks 04-06)\n",
    "- Deep reinforcement learning\n",
    "\n",
    "## Prerequisites\n",
    "- Completed notebooks 01_1, 01_2, and 01_3\n",
    "- Basic linear algebra (matrix multiplication, solving linear systems)\n",
    "- Comfort with mathematical notation ($\\sum$, $\\mathbb{E}$, etc.)\n",
    "\n",
    "## Recap from Previous Notebooks\n",
    "\n",
    "**Notebook 1.1 (Foundations):**\n",
    "- RL agents learn through trial and error by interacting with an environment\n",
    "- The Markov property: the future depends only on the present state\n",
    "- FrozenLake: 4x4 grid with slippery ice, sparse rewards (+1 at Goal only)\n",
    "\n",
    "**Notebook 1.2 (Policies & Exploration):**\n",
    "- Policies map states to actions (deterministic or stochastic)\n",
    "- Œµ-greedy balances exploration and exploitation\n",
    "- Prediction evaluates policies; control finds optimal policies\n",
    "\n",
    "**Notebook 1.3 (Value Functions & Models):**\n",
    "- Value functions quantify long-term desirability: $V(s)$ and $Q(s,a)$\n",
    "- The discount factor $\\gamma$ weighs future vs immediate rewards\n",
    "- Models predict transitions $P(s'|s,a)$ and rewards $R(s,a)$\n",
    "\n",
    "## How to Read This Notebook\n",
    "1. **Build intuition first**: Each section starts with conceptual explanations using FrozenLake\n",
    "2. **Then formalize**: We introduce mathematical notation after you understand the concept\n",
    "3. **Verify understanding**: Checkpoints help you confirm key ideas before moving on\n",
    "4. **Code demonstrates theory**: Run cells to see the math come alive\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment.\n",
    "\n",
    "> **Note:** If you're running this in a fresh environment (like Google Colab or a new virtualenv), uncomment and run the installation cell below first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (uncomment if needed)\n",
    "# !pip install gymnasium[toy-text] numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize the FrozenLake grid\n",
    "def visualize_frozenlake(env, current_state=None, title=\"FrozenLake Environment\"):\n",
    "    \"\"\"Visualize the FrozenLake grid with the current state highlighted.\"\"\"\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            cell = desc[i, j]\n",
    "            color = colors.get(cell, 'white')\n",
    "            state_idx = i * ncol + j\n",
    "            if current_state is not None and state_idx == current_state:\n",
    "                rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True, \n",
    "                                     facecolor='yellow', edgecolor='black', linewidth=2)\n",
    "            else:\n",
    "                rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                     facecolor=color, edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                   ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightblue', label='S: Start'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='white', edgecolor='black', label='F: Frozen (safe)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightcoral', label='H: Hole (game over)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightgreen', label='G: Goal (reward!)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='yellow', edgecolor='black', linewidth=2, label='Current position')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def visualize_value_function(V, title=\"State Value Function V(s)\"):\n",
    "    \"\"\"Visualize the value function as a heatmap on the FrozenLake grid.\"\"\"\n",
    "    V_grid = V.reshape((4, 4))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    \n",
    "    im = sns.heatmap(V_grid, annot=True, fmt=\".3f\", cmap=\"Greens\", \n",
    "                     cbar_kws={'label': 'Value'}, ax=ax,\n",
    "                     linewidths=0.5, linecolor='gray')\n",
    "    \n",
    "    labels = [['S', 'F', 'F', 'F'],\n",
    "              ['F', 'H', 'F', 'H'],\n",
    "              ['F', 'F', 'F', 'H'],\n",
    "              ['H', 'F', 'F', 'G']]\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ax.text(j + 0.5, i + 0.15, labels[i][j], \n",
    "                   ha='center', va='center', fontsize=10, \n",
    "                   color='red' if labels[i][j] == 'H' else 'blue',\n",
    "                   fontweight='bold')\n",
    "    \n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Column')\n",
    "    ax.set_ylabel('Row')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Markov Processes (Markov Chains)\n",
    "\n",
    "Before diving into the full MDP framework, let's build up from simpler structures. A **Markov Process** (also called a Markov Chain) is the simplest model of sequential state transitions.\n",
    "\n",
    "## Definition\n",
    "\n",
    "A **Markov Process** is a tuple $(S, P)$ where:\n",
    "- $S$ is a finite set of states\n",
    "- $P$ is a **state transition probability matrix**: $P_{ss'} = P[S_{t+1} = s' | S_t = s]$\n",
    "\n",
    "The Markov Process describes a random walk through states, where the next state depends **only on the current state** (the Markov property we learned in Notebook 1.1).\n",
    "\n",
    "## Key Properties\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| **No actions** | The agent doesn't make decisions - transitions happen automatically |\n",
    "| **No rewards** | We only track which states we visit, not how \"good\" they are |\n",
    "| **Memoryless** | Future states depend only on the current state, not history |\n",
    "| **Stochastic** | Transitions are probabilistic (each row of $P$ sums to 1) |\n",
    "\n",
    "## FrozenLake as a Markov Process\n",
    "\n",
    "If we take FrozenLake and fix a policy (say, uniform random), the agent no longer makes decisions - it just follows the policy automatically. This gives us a Markov Process!\n",
    "\n",
    "- **States**: The 16 positions on the grid (0-15)\n",
    "- **Transitions**: Determined by the fixed policy + slippery ice dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=True)\n",
    "env.reset(seed=42)\n",
    "\n",
    "print(\"FrozenLake with Random Policy = Markov Process\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nStates S: {{{', '.join(map(str, range(16)))}}}\")\n",
    "print(f\"Number of states |S|: {env.observation_space.n}\")\n",
    "print(\"\\nWith a fixed random policy, the agent doesn't choose -\")\n",
    "print(\"it just follows the policy, creating a Markov Chain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the transition matrix P for FrozenLake under a uniform random policy\n",
    "def build_markov_chain_matrix(env):\n",
    "    \"\"\"\n",
    "    Build transition matrix P for FrozenLake with uniform random policy.\n",
    "    P[s, s'] = probability of going from state s to state s'\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    # Under uniform random policy, each action has probability 1/n_actions\n",
    "    policy_prob = 1.0 / n_actions\n",
    "    \n",
    "    # Initialize transition matrix\n",
    "    P = np.zeros((n_states, n_states))\n",
    "    \n",
    "    # For each state and action, accumulate transition probabilities\n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            # Get transitions for this state-action pair\n",
    "            for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "                # Weight by policy probability\n",
    "                P[state, next_state] += policy_prob * prob\n",
    "    \n",
    "    return P\n",
    "\n",
    "P_chain = build_markov_chain_matrix(env)\n",
    "\n",
    "print(\"Transition Matrix P (Markov Chain under Random Policy)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nShape: {P_chain.shape} (16 states √ó 16 states)\")\n",
    "print(f\"Row sums (should all be 1.0): {P_chain.sum(axis=1)[:4]}... (first 4 shown)\")\n",
    "print(\"\\nTransition probabilities FROM state 0 (Start):\")\n",
    "for next_s in range(16):\n",
    "    if P_chain[0, next_s] > 0:\n",
    "        print(f\"  P(0 ‚Üí {next_s}) = {P_chain[0, next_s]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transition matrix as a heatmap\n",
    "print(\"**Question this plot answers:** 'What are the transition probabilities between all states?'\\n\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(P_chain, annot=False, cmap='Blues', ax=ax,\n",
    "            xticklabels=range(16), yticklabels=range(16))\n",
    "ax.set_xlabel('Next State s\\'')\n",
    "ax.set_ylabel('Current State s')\n",
    "ax.set_title('Markov Chain Transition Matrix P\\n(FrozenLake with Random Policy)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Terminal states (5, 7, 11, 12 = Holes, 15 = Goal) have self-loops\")\n",
    "print(\"(probability 1.0 of staying in the same state).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate random walks on the Markov Chain\n",
    "def simulate_markov_chain(P, start_state, max_steps=50):\n",
    "    \"\"\"Simulate a trajectory through the Markov Chain.\"\"\"\n",
    "    trajectory = [start_state]\n",
    "    current = start_state\n",
    "    terminal_states = {5, 7, 11, 12, 15}  # Holes and Goal\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        if current in terminal_states:\n",
    "            break\n",
    "        # Sample next state according to transition probabilities\n",
    "        next_state = np.random.choice(len(P), p=P[current])\n",
    "        trajectory.append(next_state)\n",
    "        current = next_state\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "# Simulate several episodes\n",
    "np.random.seed(42)\n",
    "print(\"Sample Trajectories through Markov Chain (starting from state 0)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for episode in range(5):\n",
    "    traj = simulate_markov_chain(P_chain, start_state=0)\n",
    "    final_state = traj[-1]\n",
    "    outcome = \"Goal!\" if final_state == 15 else f\"Hole at {final_state}\" if final_state in {5,7,11,12} else \"Timeout\"\n",
    "    print(f\"\\nEpisode {episode + 1}: {' ‚Üí '.join(map(str, traj))}\")\n",
    "    print(f\"   Outcome: {outcome} ({len(traj)} steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Checkpoint ‚Äî You should now understand:**\n",
    "> - A Markov Process (Markov Chain) is defined by states $S$ and transitions $P$\n",
    "> - With a fixed policy, an MDP becomes a Markov Chain\n",
    "> - The transition matrix $P$ captures all state-to-state probabilities\n",
    "> - Each row of $P$ sums to 1 (we must go somewhere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Markov Reward Process (MRP)\n",
    "\n",
    "A **Markov Reward Process** adds rewards to a Markov Chain. Now we care not just about *where* we go, but *how good* each state is.\n",
    "\n",
    "## Definition\n",
    "\n",
    "An MRP is a tuple $(S, P, R, \\gamma)$ where:\n",
    "- $S$ is a finite set of states\n",
    "- $P$ is the state transition probability matrix\n",
    "- $R$ is a **reward function**: $R_s = \\mathbb{E}[R_{t+1} | S_t = s]$ (expected reward in state $s$)\n",
    "- $\\gamma$ is a **discount factor**, $\\gamma \\in [0, 1]$\n",
    "\n",
    "## Building an MRP from FrozenLake\n",
    "\n",
    "When we combine FrozenLake with a fixed policy, we get an MRP:\n",
    "\n",
    "| MRP Component | FrozenLake (Random Policy) |\n",
    "|---------------|---------------------------|\n",
    "| States $S$ | 16 grid positions |\n",
    "| Transitions $P$ | From random policy + slippery ice |\n",
    "| Rewards $R$ | +1 at Goal (state 15), 0 elsewhere |\n",
    "| Discount $\\gamma$ | Typically 0.99 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the MRP from FrozenLake with random policy\n",
    "def build_mrp_reward_vector(env):\n",
    "    \"\"\"\n",
    "    Build reward vector R for FrozenLake with uniform random policy.\n",
    "    R[s] = expected immediate reward when in state s\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    policy_prob = 1.0 / n_actions\n",
    "    \n",
    "    R = np.zeros(n_states)\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "                R[state] += policy_prob * prob * reward\n",
    "    \n",
    "    return R\n",
    "\n",
    "R_mrp = build_mrp_reward_vector(env)\n",
    "\n",
    "print(\"MRP Reward Vector R (Expected Immediate Reward)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nState  Expected Reward  Interpretation\")\n",
    "print(\"-\" * 50)\n",
    "for s in range(16):\n",
    "    desc = \"Start\" if s == 0 else \"Goal\" if s == 15 else \"Hole\" if s in {5,7,11,12} else \"Frozen\"\n",
    "    print(f\"  {s:2d}      {R_mrp[s]:.4f}         {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Return $G_t$\n",
    "\n",
    "The **return** $G_t$ is the total discounted reward from time step $t$ onwards:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "### Why Discount?\n",
    "\n",
    "| Reason | Explanation | FrozenLake Example |\n",
    "|--------|-------------|-------------------|\n",
    "| **Uncertainty** | Future is uncertain; prefer certain rewards now | Slippery ice might cause failure later |\n",
    "| **Preference** | Humans/animals prefer sooner rewards | Reaching Goal in 5 steps > 20 steps |\n",
    "| **Math** | Prevents infinite returns in continuing tasks | Ensures $G_t$ is finite even if episode never ends |\n",
    "\n",
    "### Effect of $\\gamma$\n",
    "\n",
    "| $\\gamma$ | Behavior | Reaching Goal in 10 steps gives... |\n",
    "|----------|----------|------------------------------------|\n",
    "| 0 | Only immediate reward matters | $G = 0$ (no immediate reward at step 0) |\n",
    "| 0.9 | Future matters but is discounted | $G = 0.9^{10} \\times 1 = 0.35$ |\n",
    "| 0.99 | Future almost as important | $G = 0.99^{10} \\times 1 = 0.90$ |\n",
    "| 1.0 | All rewards equally important | $G = 1.0$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute returns from sample episodes\n",
    "def compute_return(rewards, gamma):\n",
    "    \"\"\"Compute discounted return from a sequence of rewards.\"\"\"\n",
    "    G = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        G = rewards[t] + gamma * G\n",
    "    return G\n",
    "\n",
    "def run_mrp_episode(env, max_steps=100):\n",
    "    \"\"\"Run one episode with random policy, return trajectory and rewards.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    trajectory = [obs]\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        trajectory.append(obs)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return trajectory, rewards\n",
    "\n",
    "# Run sample episodes and compute returns\n",
    "np.random.seed(42)\n",
    "gamma = 0.99\n",
    "\n",
    "print(f\"Sample Episodes and Returns (Œ≥ = {gamma})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for episode in range(5):\n",
    "    traj, rewards = run_mrp_episode(env)\n",
    "    G = compute_return(rewards, gamma)\n",
    "    outcome = \"Goal!\" if traj[-1] == 15 else f\"Hole\" if traj[-1] in {5,7,11,12} else \"Timeout\"\n",
    "    \n",
    "    print(f\"\\nEpisode {episode + 1}:\")\n",
    "    print(f\"  Path: {' ‚Üí '.join(map(str, traj[:8]))}{'...' if len(traj) > 8 else ''}\")\n",
    "    print(f\"  Steps: {len(rewards)}, Outcome: {outcome}\")\n",
    "    print(f\"  Rewards: {rewards}\")\n",
    "    print(f\"  Return G‚ÇÄ = {G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function for MRP\n",
    "\n",
    "The **state value function** $v(s)$ gives the expected return starting from state $s$:\n",
    "\n",
    "$$v(s) = \\mathbb{E}[G_t | S_t = s]$$\n",
    "\n",
    "This single number captures the **long-term desirability** of being in state $s$.\n",
    "\n",
    "## The Bellman Equation for MRP\n",
    "\n",
    "The value function can be decomposed into two parts:\n",
    "1. **Immediate reward** $R_s$\n",
    "2. **Discounted value of successor states** $\\gamma \\sum_{s'} P_{ss'} v(s')$\n",
    "\n",
    "$$v(s) = R_s + \\gamma \\sum_{s' \\in S} P_{ss'} v(s')$$\n",
    "\n",
    "In plain English: *\"The value of being here = what I get now + what I expect to get later (discounted)\"*\n",
    "\n",
    "### Matrix Form\n",
    "\n",
    "The Bellman equation can be written as:\n",
    "\n",
    "$$v = R + \\gamma P v$$\n",
    "\n",
    "This is a **linear equation** that can be solved directly:\n",
    "\n",
    "$$(I - \\gamma P) v = R$$\n",
    "$$v = (I - \\gamma P)^{-1} R$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the Bellman equation for MRP directly\n",
    "def solve_mrp_bellman(P, R, gamma):\n",
    "    \"\"\"Solve MRP Bellman equation: v = (I - gamma*P)^(-1) * R\"\"\"\n",
    "    n = len(R)\n",
    "    I = np.eye(n)\n",
    "    V = np.linalg.solve(I - gamma * P, R)\n",
    "    return V\n",
    "\n",
    "# Solve for value function\n",
    "gamma = 0.99\n",
    "V_mrp = solve_mrp_bellman(P_chain, R_mrp, gamma)\n",
    "\n",
    "print(f\"MRP State Values (Solved via Bellman Equation, Œ≥ = {gamma})\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\nState   V(s)      Interpretation\")\n",
    "print(\"-\" * 55)\n",
    "for s in range(16):\n",
    "    desc = \"Start\" if s == 0 else \"Goal\" if s == 15 else \"Hole\" if s in {5,7,11,12} else \"Frozen\"\n",
    "    print(f\"  {s:2d}    {V_mrp[s]:.4f}    {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the MRP value function\n",
    "print(\"**Question this heatmap answers:** 'How valuable is each state under random policy?'\\n\")\n",
    "\n",
    "visualize_value_function(V_mrp, title=f\"MRP Value Function (Random Policy, Œ≥={gamma})\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Holes and Goal have V=0 (terminal states, no future rewards)\")\n",
    "print(\"- States closer to Goal have higher values\")\n",
    "print(\"- Values are low overall because random policy rarely reaches Goal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the Bellman equation for a specific state\n",
    "state = 10  # A non-terminal state near the Goal\n",
    "\n",
    "print(f\"Verifying Bellman Equation for State {state}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nBellman equation: v(s) = R_s + Œ≥ √ó Œ£ P(s'|s) √ó v(s')\")\n",
    "print(f\"\\nFor state {state}:\")\n",
    "print(f\"  R_{state} = {R_mrp[state]:.6f}\")\n",
    "print(f\"  Œ≥ = {gamma}\")\n",
    "\n",
    "# Calculate expected value of successor states\n",
    "expected_next_value = 0\n",
    "print(f\"\\n  Expected next state value:\")\n",
    "for next_s in range(16):\n",
    "    if P_chain[state, next_s] > 0:\n",
    "        contribution = P_chain[state, next_s] * V_mrp[next_s]\n",
    "        expected_next_value += contribution\n",
    "        print(f\"    P({next_s}|{state}) √ó V({next_s}) = {P_chain[state, next_s]:.3f} √ó {V_mrp[next_s]:.4f} = {contribution:.6f}\")\n",
    "\n",
    "print(f\"\\n  Œ£ P(s'|s) √ó V(s') = {expected_next_value:.6f}\")\n",
    "\n",
    "# Calculate total using Bellman equation\n",
    "v_calculated = R_mrp[state] + gamma * expected_next_value\n",
    "print(f\"\\n  v({state}) = {R_mrp[state]:.6f} + {gamma} √ó {expected_next_value:.6f} = {v_calculated:.6f}\")\n",
    "print(f\"  Direct solution: V({state}) = {V_mrp[state]:.6f}\")\n",
    "print(f\"\\n  ‚úì Bellman equation verified!\" if abs(v_calculated - V_mrp[state]) < 1e-6 else \"  ‚úó Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Checkpoint ‚Äî You should now understand:**\n",
    "> - An MRP adds rewards and discounting to a Markov Chain: $(S, P, R, \\gamma)$\n",
    "> - The return $G_t$ is the sum of discounted future rewards\n",
    "> - The value function $v(s) = \\mathbb{E}[G_t | S_t = s]$ captures long-term desirability\n",
    "> - The Bellman equation relates $v(s)$ to immediate reward + discounted future value\n",
    "> - We can solve for $v$ directly using matrix inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Markov Decision Process (MDP)\n",
    "\n",
    "A **Markov Decision Process** adds **actions** to the MRP. Now the agent can **choose** what to do, rather than following a fixed behavior.\n",
    "\n",
    "This is the full framework for reinforcement learning!\n",
    "\n",
    "## Definition\n",
    "\n",
    "An MDP is a tuple $(S, A, P, R, \\gamma)$ where:\n",
    "- $S$ is a finite set of **states**\n",
    "- $A$ is a finite set of **actions**\n",
    "- $P$ is the state transition probability: $P^a_{ss'} = P[S_{t+1} = s' | S_t = s, A_t = a]$\n",
    "- $R$ is the reward function: $R^a_s = \\mathbb{E}[R_{t+1} | S_t = s, A_t = a]$\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0, 1]$\n",
    "\n",
    "## Key Difference from MRP\n",
    "\n",
    "| Aspect | MRP | MDP |\n",
    "|--------|-----|-----|\n",
    "| **Actions** | None (transitions happen automatically) | Agent chooses actions |\n",
    "| **Transitions** | $P_{ss'}$ (state to state) | $P^a_{ss'}$ (depends on action) |\n",
    "| **Rewards** | $R_s$ (depends on state) | $R^a_s$ (depends on state AND action) |\n",
    "| **Agent's role** | Passive observer | Active decision maker |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrozenLake as an MDP\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=True)\n",
    "env.reset(seed=42)\n",
    "\n",
    "print(\"FrozenLake MDP Components\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nS (States):  {env.observation_space}  (16 grid positions)\")\n",
    "print(f\"A (Actions): {env.action_space}  (4 directions)\")\n",
    "print(f\"\\nActions: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\")\n",
    "print(f\"\\nŒ≥ (Discount): Typically 0.99\")\n",
    "\n",
    "# Show the map\n",
    "print(\"\\nEnvironment Map:\")\n",
    "desc = env.unwrapped.desc.astype(str)\n",
    "for i, row in enumerate(desc):\n",
    "    states = [str(i*4 + j).rjust(2) for j in range(4)]\n",
    "    print(f\"   {' '.join(row)}    (states {', '.join(states)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç INSTRUCTOR-ONLY: Extract MDP transition and reward matrices\n",
    "# A real RL agent would NOT have access to this - it would learn through experience!\n",
    "\n",
    "def extract_mdp_components(env):\n",
    "    \"\"\"Extract P and R from a Gymnasium environment.\n",
    "    \n",
    "    Returns:\n",
    "        T: Transition tensor T[s, a, s'] = P(s'|s, a)\n",
    "        R: Reward matrix R[s, a] = expected reward\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    T = np.zeros((n_states, n_actions, n_states))\n",
    "    R = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                T[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    \n",
    "    return T, R\n",
    "\n",
    "T_mdp, R_mdp = extract_mdp_components(env)\n",
    "\n",
    "print(\"üîç INSTRUCTOR-ONLY: MDP Transition and Reward Tensors\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\n‚ö†Ô∏è  A real RL agent does NOT have access to these!\")\n",
    "print(\"    We use them here for educational demonstration only.\")\n",
    "print(f\"\\nTransition tensor T[s,a,s'] shape: {T_mdp.shape}\")\n",
    "print(f\"Reward matrix R[s,a] shape: {R_mdp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç INSTRUCTOR-ONLY: Examine transitions from a specific state\n",
    "action_names = {0: 'LEFT', 1: 'DOWN', 2: 'RIGHT', 3: 'UP'}\n",
    "state = 6\n",
    "\n",
    "print(f\"üîç INSTRUCTOR-ONLY: Transitions from State {state}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nState {state} is in the middle of the grid (row 1, col 2)\")\n",
    "\n",
    "for action in range(4):\n",
    "    print(f\"\\n  Action {action} ({action_names[action]}):\")\n",
    "    print(f\"    Expected reward: R({state},{action}) = {R_mdp[state, action]:.4f}\")\n",
    "    print(f\"    Possible next states:\")\n",
    "    for next_s in range(16):\n",
    "        if T_mdp[state, action, next_s] > 0:\n",
    "            cell_type = desc.flatten()[next_s]\n",
    "            print(f\"      ‚Üí State {next_s} ({cell_type}): P = {T_mdp[state, action, next_s]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transition probabilities from one state for all actions\n",
    "print(\"**Question this plot answers:** 'How does each action affect where I might end up?'\\n\")\n",
    "\n",
    "def visualize_mdp_transitions(state, T, env):\n",
    "    \"\"\"Visualize transitions from a state for all actions.\"\"\"\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    for action, ax in enumerate(axes):\n",
    "        for i in range(nrow):\n",
    "            for j in range(ncol):\n",
    "                cell = desc[i, j]\n",
    "                state_idx = i * ncol + j\n",
    "                prob = T[state, action, state_idx]\n",
    "                \n",
    "                if state_idx == state:\n",
    "                    facecolor = 'yellow'\n",
    "                elif prob > 0:\n",
    "                    facecolor = plt.cm.Blues(0.3 + 0.7 * prob)\n",
    "                else:\n",
    "                    facecolor = colors.get(cell, 'white')\n",
    "                \n",
    "                rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                     facecolor=facecolor, edgecolor='black')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.7, cell,\n",
    "                       ha='center', va='center', fontsize=12)\n",
    "                \n",
    "                if prob > 0 and state_idx != state:\n",
    "                    ax.text(j + 0.5, nrow - 1 - i + 0.3, f'{prob:.2f}',\n",
    "                           ha='center', va='center', fontsize=10, \n",
    "                           color='blue', fontweight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, ncol)\n",
    "        ax.set_ylim(0, nrow)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Action: {action_names[action]}')\n",
    "    \n",
    "    plt.suptitle(f'Transition Probabilities from State {state} (Yellow)\\n'\n",
    "                 f'Blue intensity = probability, numbers show P(s\\'|s,a)', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "visualize_mdp_transitions(6, T_mdp, env)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Due to slippery ice, the intended action only succeeds 1/3 of the time!\")\n",
    "print(\"The other 2/3, the agent slips perpendicular to the intended direction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Checkpoint ‚Äî You should now understand:**\n",
    "> - An MDP adds actions to an MRP: $(S, A, P, R, \\gamma)$\n",
    "> - Transitions and rewards now depend on both state AND action\n",
    "> - The agent actively chooses actions rather than following fixed behavior\n",
    "> - In FrozenLake, the slippery ice makes transitions stochastic even with a chosen action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Policies in MDPs\n",
    "\n",
    "A **policy** $\\pi$ defines the agent's behavior - how it chooses actions based on states.\n",
    "\n",
    "$$\\pi(a|s) = P[A_t = a | S_t = s]$$\n",
    "\n",
    "## Types of Policies\n",
    "\n",
    "| Type | Notation | Description | Example |\n",
    "|------|----------|-------------|---------|\n",
    "| **Deterministic** | $a = \\pi(s)$ | Always same action for a state | \"In state 6, always go RIGHT\" |\n",
    "| **Stochastic** | $\\pi(a|s)$ | Probability distribution over actions | \"In state 6: 40% RIGHT, 30% DOWN, ...\" |\n",
    "\n",
    "## Key Insight: MDP + Policy = MRP\n",
    "\n",
    "Once we fix a policy $\\pi$, the MDP becomes an MRP!\n",
    "\n",
    "- **Transitions**: $P^\\pi_{ss'} = \\sum_a \\pi(a|s) P^a_{ss'}$\n",
    "- **Rewards**: $R^\\pi_s = \\sum_a \\pi(a|s) R^a_s$\n",
    "\n",
    "The policy \"averages out\" the action choice, leaving us with state-to-state transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some policies for FrozenLake\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Uniform random policy\n",
    "def create_uniform_policy():\n",
    "    \"\"\"Return a uniform random policy.\"\"\"\n",
    "    return np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "# \"Go DOWN then RIGHT\" policy\n",
    "def create_down_right_policy():\n",
    "    \"\"\"Policy that prefers DOWN in upper half, RIGHT in lower half.\"\"\"\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        row = s // 4\n",
    "        if row < 2:  # Upper half: go DOWN\n",
    "            policy[s, 1] = 1.0  # DOWN\n",
    "        else:  # Lower half: go RIGHT\n",
    "            policy[s, 2] = 1.0  # RIGHT\n",
    "    return policy\n",
    "\n",
    "pi_random = create_uniform_policy()\n",
    "pi_down_right = create_down_right_policy()\n",
    "\n",
    "print(\"Example Policies for FrozenLake\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nUniform Random Policy œÄ_random(a|s=6):\")\n",
    "for a in range(n_actions):\n",
    "    print(f\"  œÄ({action_names[a]}|s=6) = {pi_random[6, a]:.2f}\")\n",
    "\n",
    "print(\"\\nDown-then-Right Policy œÄ_dr(a|s=6):\")\n",
    "for a in range(n_actions):\n",
    "    print(f\"  œÄ({action_names[a]}|s=6) = {pi_down_right[6, a]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MDP + Policy to MRP\n",
    "def mdp_to_mrp(T, R_sa, policy):\n",
    "    \"\"\"\n",
    "    Convert MDP with policy to MRP.\n",
    "    \n",
    "    Args:\n",
    "        T: Transition tensor T[s,a,s']\n",
    "        R_sa: Reward matrix R[s,a]\n",
    "        policy: Policy matrix œÄ[s,a]\n",
    "    \n",
    "    Returns:\n",
    "        P_mrp: MRP transition matrix P[s,s']\n",
    "        R_mrp: MRP reward vector R[s]\n",
    "    \"\"\"\n",
    "    n_states = T.shape[0]\n",
    "    n_actions = T.shape[1]\n",
    "    \n",
    "    # P_ss'^œÄ = Œ£_a œÄ(a|s) √ó P_ss'^a\n",
    "    P_mrp = np.zeros((n_states, n_states))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            P_mrp[s] += policy[s, a] * T[s, a]\n",
    "    \n",
    "    # R_s^œÄ = Œ£_a œÄ(a|s) √ó R_s^a\n",
    "    R_mrp = np.sum(policy * R_sa, axis=1)\n",
    "    \n",
    "    return P_mrp, R_mrp\n",
    "\n",
    "# Convert with random policy\n",
    "P_from_random, R_from_random = mdp_to_mrp(T_mdp, R_mdp, pi_random)\n",
    "P_from_dr, R_from_dr = mdp_to_mrp(T_mdp, R_mdp, pi_down_right)\n",
    "\n",
    "print(\"MDP + Policy ‚Üí MRP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nMDP: T shape = {T_mdp.shape}, R shape = {R_mdp.shape}\")\n",
    "print(f\"\\nWith Random Policy:\")\n",
    "print(f\"  MRP: P shape = {P_from_random.shape}, R shape = {R_from_random.shape}\")\n",
    "print(f\"\\nWith Down-Right Policy:\")\n",
    "print(f\"  MRP: P shape = {P_from_dr.shape}, R shape = {R_from_dr.shape}\")\n",
    "print(f\"\\nRow sums of P (should be 1): {P_from_random.sum(axis=1)[:4]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Checkpoint ‚Äî You should now understand:**\n",
    "> - A policy $\\pi(a|s)$ defines how the agent chooses actions\n",
    "> - Policies can be deterministic or stochastic\n",
    "> - MDP + fixed policy = MRP (the policy \"collapses\" the action dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Value Functions for MDPs\n",
    "\n",
    "In MDPs, we have two value functions that depend on the policy:\n",
    "\n",
    "## State-Value Function $V^\\pi(s)$\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$$\n",
    "\n",
    "**\"How good is it to BE in state $s$ if I follow policy $\\pi$?\"**\n",
    "\n",
    "## Action-Value Function $Q^\\pi(s, a)$\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$$\n",
    "\n",
    "**\"How good is it to DO action $a$ in state $s$, then follow policy $\\pi$?\"**\n",
    "\n",
    "## Comparison\n",
    "\n",
    "| Aspect | $V^\\pi(s)$ | $Q^\\pi(s, a)$ |\n",
    "|--------|-----------|---------------|\n",
    "| **Input** | State only | State AND action |\n",
    "| **Question** | \"How good is this state?\" | \"How good is this action here?\" |\n",
    "| **Size (FrozenLake)** | 16 values | 64 values (16 √ó 4) |\n",
    "| **Use** | Evaluate being in a state | Compare different actions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute V^œÄ using MRP solution\n",
    "def compute_V_pi(T, R_sa, policy, gamma):\n",
    "    \"\"\"Compute V^œÄ using direct solution of Bellman equation.\"\"\"\n",
    "    P_mrp, R_mrp = mdp_to_mrp(T, R_sa, policy)\n",
    "    n_states = len(R_mrp)\n",
    "    I = np.eye(n_states)\n",
    "    V = np.linalg.solve(I - gamma * P_mrp, R_mrp)\n",
    "    return V\n",
    "\n",
    "# Compute Q^œÄ from V^œÄ\n",
    "def compute_Q_from_V(T, R_sa, V, gamma):\n",
    "    \"\"\"Compute Q^œÄ from V^œÄ.\"\"\"\n",
    "    n_states, n_actions = R_sa.shape\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            # Q(s,a) = R(s,a) + Œ≥ √ó Œ£ P(s'|s,a) √ó V(s')\n",
    "            Q[s, a] = R_sa[s, a] + gamma * np.sum(T[s, a] * V)\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Compute values for random policy\n",
    "gamma = 0.99\n",
    "V_random = compute_V_pi(T_mdp, R_mdp, pi_random, gamma)\n",
    "Q_random = compute_Q_from_V(T_mdp, R_mdp, V_random, gamma)\n",
    "\n",
    "print(f\"Value Functions for Random Policy (Œ≥ = {gamma})\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\nState-Value Function V^œÄ(s):\")\n",
    "print(V_random.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize V^œÄ\n",
    "print(\"**Question this heatmap answers:** 'Under random policy, which states are most valuable?'\\n\")\n",
    "\n",
    "visualize_value_function(V_random, title=f\"V^œÄ (Random Policy, Œ≥={gamma})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q^œÄ values\n",
    "print(\"**Question this plot answers:** 'Which action is best in each state under random policy?'\\n\")\n",
    "\n",
    "def visualize_q_values(Q, env, title=\"Q-Values\"):\n",
    "    \"\"\"Visualize Q-values showing best action in each state.\"\"\"\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    action_arrows = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            ax.text(j + 0.5, nrow - 1 - i + 0.5, cell, ha='center', va='center',\n",
    "                   fontsize=14, fontweight='bold', alpha=0.3)\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                best_action = np.argmax(Q[state])\n",
    "                positions = [(0.2, 0.5), (0.5, 0.2), (0.8, 0.5), (0.5, 0.8)]  # L, D, R, U\n",
    "                for a in range(4):\n",
    "                    x, y = positions[a]\n",
    "                    color = 'green' if a == best_action else 'black'\n",
    "                    weight = 'bold' if a == best_action else 'normal'\n",
    "                    ax.text(j + x, nrow - 1 - i + y, f'{Q[state, a]:.2f}',\n",
    "                           ha='center', va='center', fontsize=7, \n",
    "                           color=color, fontweight=weight)\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"{title}\\n(Green = best action, positions: ‚Üêleft, ‚Üìbottom, ‚Üíright, ‚Üëtop)\", fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "visualize_q_values(Q_random, env, title=\"Q^œÄ (Random Policy)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Between V and Q\n",
    "\n",
    "The two value functions are related:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\cdot Q^\\pi(s, a)$$\n",
    "\n",
    "**V is the weighted average of Q values, weighted by the policy.**\n",
    "\n",
    "For a **deterministic policy** that always chooses action $a^*$:\n",
    "$$V^\\pi(s) = Q^\\pi(s, a^*)$$\n",
    "\n",
    "For a **uniform random policy**:\n",
    "$$V^\\pi(s) = \\frac{1}{|A|} \\sum_a Q^\\pi(s, a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify V = Œ£ œÄ(a|s) √ó Q(s,a)\n",
    "state = 10\n",
    "\n",
    "print(f\"Verifying V = Œ£ œÄ(a|s) √ó Q(s,a) for State {state}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "V_direct = V_random[state]\n",
    "print(f\"\\nDirect V^œÄ({state}) = {V_direct:.6f}\")\n",
    "\n",
    "print(f\"\\nUsing V = Œ£ œÄ(a|s) √ó Q(s,a):\")\n",
    "V_from_Q = 0\n",
    "for a in range(n_actions):\n",
    "    contribution = pi_random[state, a] * Q_random[state, a]\n",
    "    V_from_Q += contribution\n",
    "    print(f\"  œÄ({action_names[a]}|{state}) √ó Q({state},{action_names[a]}) = {pi_random[state,a]:.2f} √ó {Q_random[state,a]:.4f} = {contribution:.6f}\")\n",
    "\n",
    "print(f\"\\nSum = {V_from_Q:.6f}\")\n",
    "print(f\"\\n‚úì Verified!\" if abs(V_direct - V_from_Q) < 1e-6 else \"‚úó Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Checkpoint ‚Äî You should now understand:**\n",
    "> - $V^\\pi(s)$: expected return starting from state $s$ under policy $\\pi$\n",
    "> - $Q^\\pi(s, a)$: expected return after taking action $a$ in state $s$, then following $\\pi$\n",
    "> - $V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s, a)$ ‚Äî V is the policy-weighted average of Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Bellman Expectation Equations\n",
    "\n",
    "The Bellman equations express the **recursive relationship** between value functions.\n",
    "\n",
    "## Intuition: Value = Immediate + Future\n",
    "\n",
    "The value of a state (or state-action pair) can be decomposed:\n",
    "\n",
    "**Value now = What I get now + (Discounted) What I expect later**\n",
    "\n",
    "## Bellman Expectation Equation for $V^\\pi$\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\left[ R^a_s + \\gamma \\sum_{s'} P^a_{ss'} V^\\pi(s') \\right]$$\n",
    "\n",
    "Breaking this down:\n",
    "1. $\\sum_a \\pi(a|s)$ ‚Äî average over actions according to policy\n",
    "2. $R^a_s$ ‚Äî immediate reward for taking action $a$\n",
    "3. $\\gamma \\sum_{s'} P^a_{ss'} V^\\pi(s')$ ‚Äî discounted expected value of next states\n",
    "\n",
    "## Bellman Expectation Equation for $Q^\\pi$\n",
    "\n",
    "$$Q^\\pi(s, a) = R^a_s + \\gamma \\sum_{s'} P^a_{ss'} V^\\pi(s')$$\n",
    "\n",
    "Or equivalently, substituting $V$:\n",
    "\n",
    "$$Q^\\pi(s, a) = R^a_s + \\gamma \\sum_{s'} P^a_{ss'} \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Bellman Expectation Equation for V^œÄ\n",
    "state = 10\n",
    "gamma = 0.99\n",
    "\n",
    "print(f\"Verifying Bellman Expectation Equation for V^œÄ at State {state}\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"\\nEquation: V^œÄ(s) = Œ£_a œÄ(a|s) √ó [R(s,a) + Œ≥ √ó Œ£_s' P(s'|s,a) √ó V^œÄ(s')]\")\n",
    "\n",
    "V_direct = V_random[state]\n",
    "print(f\"\\nDirect solution: V^œÄ({state}) = {V_direct:.6f}\")\n",
    "\n",
    "print(f\"\\nUsing Bellman equation:\")\n",
    "V_bellman = 0\n",
    "for a in range(n_actions):\n",
    "    # Immediate reward + discounted future value\n",
    "    immediate = R_mdp[state, a]\n",
    "    future = gamma * np.sum(T_mdp[state, a] * V_random)\n",
    "    q_sa = immediate + future\n",
    "    \n",
    "    contribution = pi_random[state, a] * q_sa\n",
    "    V_bellman += contribution\n",
    "    \n",
    "    print(f\"  a={action_names[a]}: œÄ={pi_random[state,a]:.2f} √ó [R={immediate:.4f} + Œ≥√óŒ£PV={future:.4f}] = {contribution:.6f}\")\n",
    "\n",
    "print(f\"\\nSum = {V_bellman:.6f}\")\n",
    "print(f\"\\n‚úì Bellman equation verified!\" if abs(V_direct - V_bellman) < 1e-6 else \"‚úó Error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Bellman Expectation Equation for Q^œÄ\n",
    "state = 10\n",
    "action = 2  # RIGHT\n",
    "\n",
    "print(f\"Verifying Bellman Expectation Equation for Q^œÄ at State {state}, Action {action_names[action]}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nEquation: Q^œÄ(s,a) = R(s,a) + Œ≥ √ó Œ£_s' P(s'|s,a) √ó V^œÄ(s')\")\n",
    "\n",
    "Q_direct = Q_random[state, action]\n",
    "print(f\"\\nDirect solution: Q^œÄ({state},{action_names[action]}) = {Q_direct:.6f}\")\n",
    "\n",
    "print(f\"\\nUsing Bellman equation:\")\n",
    "print(f\"  R({state},{action_names[action]}) = {R_mdp[state, action]:.6f}\")\n",
    "\n",
    "future_value = 0\n",
    "print(f\"\\n  Œ≥ √ó Œ£ P(s'|s,a) √ó V^œÄ(s'):\")\n",
    "for next_s in range(n_states):\n",
    "    if T_mdp[state, action, next_s] > 0:\n",
    "        contribution = T_mdp[state, action, next_s] * V_random[next_s]\n",
    "        future_value += contribution\n",
    "        print(f\"    P({next_s}|{state},{action_names[action]}) √ó V({next_s}) = {T_mdp[state,action,next_s]:.2f} √ó {V_random[next_s]:.4f} = {contribution:.6f}\")\n",
    "\n",
    "print(f\"\\n  Œ£ = {future_value:.6f}\")\n",
    "print(f\"  Œ≥ √ó Œ£ = {gamma * future_value:.6f}\")\n",
    "\n",
    "Q_bellman = R_mdp[state, action] + gamma * future_value\n",
    "print(f\"\\nQ^œÄ({state},{action_names[action]}) = {R_mdp[state,action]:.6f} + {gamma * future_value:.6f} = {Q_bellman:.6f}\")\n",
    "print(f\"\\n‚úì Bellman equation verified!\" if abs(Q_direct - Q_bellman) < 1e-6 else \"‚úó Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Checkpoint ‚Äî You should now understand:**\n",
    "> - Bellman equations express value recursively: Value = Immediate + Œ≥ √ó Expected Future\n",
    "> - For $V^\\pi$: average over actions, then over next states\n",
    "> - For $Q^\\pi$: immediate reward + discounted expected next state value\n",
    "> - These equations are the foundation for RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Optimal Value Functions and Bellman Optimality\n",
    "\n",
    "So far, we've evaluated specific policies. Now let's find the **best** policy!\n",
    "\n",
    "## Optimal Value Functions\n",
    "\n",
    "The **optimal state-value function** $V^*(s)$ is the maximum value achievable:\n",
    "\n",
    "$$V^*(s) = \\max_\\pi V^\\pi(s)$$\n",
    "\n",
    "The **optimal action-value function** $Q^*(s, a)$ is the maximum action-value:\n",
    "\n",
    "$$Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)$$\n",
    "\n",
    "## Key Theorem\n",
    "\n",
    "For any finite MDP:\n",
    "1. There exists an **optimal policy** $\\pi^*$ that is better than or equal to all other policies\n",
    "2. All optimal policies achieve the same $V^*$ and $Q^*$\n",
    "3. There always exists a **deterministic** optimal policy\n",
    "\n",
    "## Finding Optimal Policy from $Q^*$\n",
    "\n",
    "If we know $Q^*$, the optimal policy is simple:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "**Just pick the action with the highest Q-value!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equations\n",
    "\n",
    "The optimal value functions satisfy special Bellman equations:\n",
    "\n",
    "### For $V^*$:\n",
    "\n",
    "$$V^*(s) = \\max_a \\left[ R^a_s + \\gamma \\sum_{s'} P^a_{ss'} V^*(s') \\right]$$\n",
    "\n",
    "### For $Q^*$:\n",
    "\n",
    "$$Q^*(s, a) = R^a_s + \\gamma \\sum_{s'} P^a_{ss'} \\max_{a'} Q^*(s', a')$$\n",
    "\n",
    "### Key Difference from Bellman Expectation Equations\n",
    "\n",
    "| Aspect | Expectation Equations | Optimality Equations |\n",
    "|--------|----------------------|---------------------|\n",
    "| **Operator** | $\\sum_a \\pi(a|s)$ (average) | $\\max_a$ (maximum) |\n",
    "| **Linearity** | Linear in $V$ | **Non-linear** (due to max) |\n",
    "| **Solution** | Matrix inversion | Iterative methods required |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration to find V* (preview of notebook 03)\n",
    "def value_iteration(T, R, gamma, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"Find optimal value function using Value Iteration.\"\"\"\n",
    "    n_states = T.shape[0]\n",
    "    n_actions = T.shape[1]\n",
    "    \n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        for s in range(n_states):\n",
    "            # V*(s) = max_a [R(s,a) + Œ≥ √ó Œ£ P(s'|s,a) √ó V*(s')]\n",
    "            q_values = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                q_values[a] = R[s, a] + gamma * np.sum(T[s, a] * V)\n",
    "            V_new[s] = np.max(q_values)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            print(f\"Value Iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        V = V_new\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Find optimal value function\n",
    "V_star = value_iteration(T_mdp, R_mdp, gamma=0.99)\n",
    "\n",
    "# Compute Q* from V*\n",
    "Q_star = compute_Q_from_V(T_mdp, R_mdp, V_star, gamma=0.99)\n",
    "\n",
    "# Extract optimal policy\n",
    "pi_star = np.zeros((n_states, n_actions))\n",
    "for s in range(n_states):\n",
    "    best_action = np.argmax(Q_star[s])\n",
    "    pi_star[s, best_action] = 1.0\n",
    "\n",
    "print(\"\\nOptimal Value Function V*:\")\n",
    "print(V_star.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimal values and policy\n",
    "print(\"**Question these plots answer:** 'What is the optimal value and policy for FrozenLake?'\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot V*\n",
    "desc = env.unwrapped.desc.astype(str)\n",
    "nrow, ncol = desc.shape\n",
    "V_grid = V_star.reshape(nrow, ncol)\n",
    "\n",
    "im = axes[0].imshow(V_grid, cmap='Greens', vmin=0, vmax=V_star.max())\n",
    "plt.colorbar(im, ax=axes[0], label='V*(s)')\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        state = i * ncol + j\n",
    "        axes[0].text(j, i, f'{desc[i,j]}\\n{V_star[state]:.3f}', \n",
    "                    ha='center', va='center', fontsize=10)\n",
    "\n",
    "axes[0].set_title('Optimal Value Function V*', fontsize=12)\n",
    "axes[0].set_xticks(range(ncol))\n",
    "axes[0].set_yticks(range(nrow))\n",
    "\n",
    "# Plot optimal policy\n",
    "action_arrows = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']\n",
    "colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        state = i * ncol + j\n",
    "        cell = desc[i, j]\n",
    "        \n",
    "        rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                             facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "        axes[1].add_patch(rect)\n",
    "        \n",
    "        best_action = np.argmax(Q_star[state])\n",
    "        if cell not in ['H', 'G']:\n",
    "            axes[1].text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                        f'{cell}\\n{action_arrows[best_action]}',\n",
    "                        ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            axes[1].text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                        ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[1].set_xlim(0, ncol)\n",
    "axes[1].set_ylim(0, nrow)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Optimal Policy œÄ*\\n(arrows show best action)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random policy vs optimal policy performance\n",
    "print(\"**Question this comparison answers:** 'How much better is the optimal policy?'\\n\")\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=10000):\n",
    "    \"\"\"Evaluate a policy by running episodes.\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.random.choice(env.action_space.n, p=policy[obs])\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "# Evaluate both policies\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "print(\"Policy Comparison (10,000 episodes each)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rewards_random = evaluate_policy(env, pi_random, n_episodes=10000)\n",
    "rewards_optimal = evaluate_policy(env, pi_star, n_episodes=10000)\n",
    "\n",
    "print(f\"\\nRandom Policy:\")\n",
    "print(f\"  Success rate: {np.mean(rewards_random)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOptimal Policy:\")\n",
    "print(f\"  Success rate: {np.mean(rewards_optimal)*100:.2f}%\")\n",
    "\n",
    "improvement = (np.mean(rewards_optimal) - np.mean(rewards_random)) / np.mean(rewards_random) * 100\n",
    "print(f\"\\nImprovement: {improvement:.1f}% relative increase\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "policies = ['Random Policy', 'Optimal Policy']\n",
    "success_rates = [np.mean(rewards_random) * 100, np.mean(rewards_optimal) * 100]\n",
    "colors = ['gray', 'green']\n",
    "\n",
    "bars = ax.bar(policies, success_rates, color=colors, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Success Rate (%)')\n",
    "ax.set_title('Random vs Optimal Policy on FrozenLake')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Checkpoint ‚Äî You should now understand:**\n",
    "> - $V^*(s) = \\max_\\pi V^\\pi(s)$ and $Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)$\n",
    "> - The optimal policy is $\\pi^*(s) = \\arg\\max_a Q^*(s,a)$\n",
    "> - Bellman Optimality Equations use **max** instead of policy-weighted average\n",
    "> - This non-linearity requires iterative solution methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Summary and Concept Map\n",
    "\n",
    "In this notebook, we built up the mathematical framework for reinforcement learning:\n",
    "\n",
    "```\n",
    "FROM SIMPLE TO COMPLEX: THE MDP HIERARCHY\n",
    "==========================================\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  MARKOV PROCESS (Markov Chain)                                  ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ\n",
    "‚îÇ  (S, P) - States and Transitions                                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Random walk through states                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ No rewards, no actions                                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì Add Rewards\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  MARKOV REWARD PROCESS (MRP)                                    ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ\n",
    "‚îÇ  (S, P, R, Œ≥) - Add Rewards and Discount                        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Value function v(s) = E[G_t | S_t = s]                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Bellman equation: v = R + Œ≥Pv                                ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì Add Actions\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  MARKOV DECISION PROCESS (MDP)                                  ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ\n",
    "‚îÇ  (S, A, P, R, Œ≥) - The Full RL Framework                        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Policy œÄ(a|s) defines behavior                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ V^œÄ(s) and Q^œÄ(s,a) value functions                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Bellman Expectation: V = Œ£œÄ[R + Œ≥PV]                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Bellman Optimality: V* = max[R + Œ≥PV*]                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "\n",
    "KEY EQUATIONS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Return:           G_t = R_{t+1} + Œ≥R_{t+2} + Œ≥¬≤R_{t+3} + ...\n",
    "\n",
    "Value (MRP):      v(s) = R_s + Œ≥ Œ£ P_{ss'} v(s')\n",
    "\n",
    "V^œÄ (MDP):        V^œÄ(s) = Œ£_a œÄ(a|s) [R_s^a + Œ≥ Œ£_{s'} P_{ss'}^a V^œÄ(s')]\n",
    "\n",
    "Q^œÄ (MDP):        Q^œÄ(s,a) = R_s^a + Œ≥ Œ£_{s'} P_{ss'}^a V^œÄ(s')\n",
    "\n",
    "V* (Optimal):     V*(s) = max_a [R_s^a + Œ≥ Œ£_{s'} P_{ss'}^a V*(s')]\n",
    "\n",
    "Optimal Policy:   œÄ*(s) = argmax_a Q*(s,a)\n",
    "```\n",
    "\n",
    "## What's Coming Next\n",
    "\n",
    "In the next notebook (**03_dynamic_programming.ipynb**), we'll learn algorithms to **solve** MDPs:\n",
    "\n",
    "| Algorithm | Purpose | Requires Model? |\n",
    "|-----------|---------|----------------|\n",
    "| **Policy Evaluation** | Compute $V^\\pi$ for a given policy | Yes |\n",
    "| **Policy Iteration** | Find $\\pi^*$ by alternating evaluation and improvement | Yes |\n",
    "| **Value Iteration** | Find $V^*$ directly using Bellman Optimality | Yes |\n",
    "\n",
    "These are **dynamic programming** methods that require knowing the MDP model (transitions and rewards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Your Turn\n",
    "\n",
    "Now it's time to test your understanding!\n",
    "\n",
    "## Exercise 1: Conceptual Question\n",
    "\n",
    "Consider the Bellman Expectation Equation for $V^\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\left[ R^a_s + \\gamma \\sum_{s'} P^a_{ss'} V^\\pi(s') \\right]$$\n",
    "\n",
    "**Question:** Why does this equation have $V^\\pi(s')$ on the right-hand side if we're trying to compute $V^\\pi(s)$? Doesn't this create a circular dependency?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see hint</summary>\n",
    "\n",
    "Think about what \"solving\" a system of equations means. Consider a simple example with 2 states.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Yes, it's circular - and that's the point!**\n",
    "\n",
    "The Bellman equation creates a **system of simultaneous equations**. For FrozenLake with 16 states, we have 16 equations (one per state), each relating $V(s)$ to the values of other states.\n",
    "\n",
    "This system can be solved by:\n",
    "1. **Matrix inversion**: Rearrange to $V = (I - \\gamma P)^{-1} R$\n",
    "2. **Iteration**: Start with a guess, repeatedly apply the Bellman update until convergence\n",
    "\n",
    "The circular dependency is not a problem - it's what makes the equations capture the recursive nature of value!\n",
    "\n",
    "</details>\n",
    "\n",
    "## Exercise 2: Code Task\n",
    "\n",
    "Verify the Bellman Optimality Equation for state 14 (adjacent to Goal).\n",
    "\n",
    "**Task:** Compute $V^*(14)$ using the Bellman Optimality Equation and verify it matches the value from Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Verify Bellman Optimality Equation for state 14\n",
    "\n",
    "# state = 14\n",
    "# gamma = 0.99\n",
    "\n",
    "# print(f\"Verifying Bellman Optimality Equation for State {state}\")\n",
    "# print(\"=\" * 55)\n",
    "# print(f\"\\nBellman Optimality: V*(s) = max_a [R(s,a) + Œ≥ √ó Œ£ P(s'|s,a) √ó V*(s')]\")\n",
    "\n",
    "# # For each action, compute Q*(s,a) = R(s,a) + Œ≥ √ó Œ£ P(s'|s,a) √ó V*(s')\n",
    "# q_values = []\n",
    "# for a in range(4):\n",
    "#     # TODO: Compute q_value for this action\n",
    "#     # q_value = R_mdp[state, a] + gamma * np.sum(T_mdp[state, a] * V_star)\n",
    "#     # q_values.append(q_value)\n",
    "#     pass\n",
    "\n",
    "# # V*(s) = max over actions\n",
    "# # v_star_calculated = max(q_values)\n",
    "\n",
    "# # Compare with Value Iteration result\n",
    "# # print(f\"\\nV*(14) from Bellman: {v_star_calculated:.6f}\")\n",
    "# # print(f\"V*(14) from Value Iteration: {V_star[14]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Conceptual Question\n",
    "\n",
    "**Question:** In FrozenLake, why is the optimal policy success rate (~74%) much lower than 100%, even though we found the \"optimal\" policy?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see hint</summary>\n",
    "\n",
    "Think about what \"optimal\" means in a stochastic environment.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**\"Optimal\" means the best possible, not perfect.**\n",
    "\n",
    "The optimal policy maximizes the expected return, but the environment is stochastic (slippery ice). Even with the best possible action choices:\n",
    "\n",
    "1. The agent only moves in the intended direction 1/3 of the time\n",
    "2. The other 2/3, it slips perpendicular to the intended direction\n",
    "3. Some slips inevitably lead to Holes\n",
    "\n",
    "No policy can overcome the inherent randomness in the environment. The optimal policy achieves ~74% because that's the theoretical maximum given the stochastic transitions.\n",
    "\n",
    "Compare to the deterministic case (`is_slippery=False`) where the optimal policy achieves 100% success!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Congratulations! You've completed Part 2 of the RL Tutorial!**\n",
    "\n",
    "**Key takeaways:**\n",
    "- MDPs $(S, A, P, R, \\gamma)$ provide the mathematical framework for RL\n",
    "- Value functions $V^\\pi(s)$ and $Q^\\pi(s,a)$ quantify long-term desirability\n",
    "- Bellman Expectation Equations relate values recursively (for a given policy)\n",
    "- Bellman Optimality Equations define optimal values (using max instead of policy average)\n",
    "- The optimal policy simply picks $\\arg\\max_a Q^*(s,a)$\n",
    "\n",
    "**Next: 03_dynamic_programming.ipynb** - Algorithms to solve MDPs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
