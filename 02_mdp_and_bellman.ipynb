{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Markov Decision Processes and Bellman Equations\n",
    "\n",
    "In this notebook, we'll formalize the reinforcement learning problem using **Markov Decision Processes (MDPs)** and derive the fundamental **Bellman equations**.\n",
    "\n",
    "## What You'll Learn\n",
    "- Markov Processes (Markov Chains)\n",
    "- Markov Reward Processes (MRP)\n",
    "- Markov Decision Processes (MDP)\n",
    "- The Return and Value Functions\n",
    "- Bellman Expectation Equations\n",
    "- Optimal Value Functions and Bellman Optimality Equations\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Markov Processes (Markov Chains)\n",
    "\n",
    "Before we get to MDPs, let's start with simpler structures.\n",
    "\n",
    "## Definition\n",
    "\n",
    "A **Markov Process** (or Markov Chain) is a tuple $(S, P)$ where:\n",
    "- $S$ is a finite set of states\n",
    "- $P$ is a state transition probability matrix: $P_{ss'} = P[S_{t+1} = s' | S_t = s]$\n",
    "\n",
    "The Markov Process describes a random walk through states, where the next state depends only on the current state (Markov property).\n",
    "\n",
    "## State Transition Matrix\n",
    "\n",
    "The transition matrix $P$ defines the probability of moving from any state $s$ to any state $s'$:\n",
    "\n",
    "$$P = \\begin{pmatrix} P_{11} & P_{12} & \\cdots & P_{1n} \\\\ P_{21} & P_{22} & \\cdots & P_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ P_{n1} & P_{n2} & \\cdots & P_{nn} \\end{pmatrix}$$\n",
    "\n",
    "Each row sums to 1 (we must go somewhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Weather Markov Chain\n",
    "# States: 0 = Sunny, 1 = Rainy\n",
    "\n",
    "states = ['Sunny', 'Rainy']\n",
    "\n",
    "# Transition matrix\n",
    "# P[i,j] = probability of going from state i to state j\n",
    "P_weather = np.array([\n",
    "    [0.8, 0.2],  # From Sunny: 80% stay sunny, 20% become rainy\n",
    "    [0.4, 0.6]   # From Rainy: 40% become sunny, 60% stay rainy\n",
    "])\n",
    "\n",
    "print(\"Weather Markov Chain\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\\nTransition Matrix P:\")\n",
    "print(P_weather)\n",
    "print(f\"\\nRow sums (should be 1.0): {P_weather.sum(axis=1)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(P_weather, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=states, yticklabels=states, ax=ax)\n",
    "ax.set_title('Weather Transition Matrix')\n",
    "ax.set_xlabel('Next State')\n",
    "ax.set_ylabel('Current State')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a Markov Chain\n",
    "def simulate_markov_chain(P, initial_state, n_steps):\n",
    "    \"\"\"Simulate a Markov chain for n_steps.\"\"\"\n",
    "    states_sequence = [initial_state]\n",
    "    current_state = initial_state\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Sample next state according to transition probabilities\n",
    "        next_state = np.random.choice(len(P), p=P[current_state])\n",
    "        states_sequence.append(next_state)\n",
    "        current_state = next_state\n",
    "    \n",
    "    return states_sequence\n",
    "\n",
    "# Simulate 30 days of weather\n",
    "np.random.seed(42)\n",
    "weather_sequence = simulate_markov_chain(P_weather, initial_state=0, n_steps=30)\n",
    "\n",
    "print(\"Simulated Weather for 30 Days\")\n",
    "print(\"=\" * 50)\n",
    "weather_names = [states[s] for s in weather_sequence]\n",
    "print(f\"Day 1-10:  {weather_names[:10]}\")\n",
    "print(f\"Day 11-20: {weather_names[10:20]}\")\n",
    "print(f\"Day 21-31: {weather_names[20:]}\")\n",
    "\n",
    "# Count occurrences\n",
    "sunny_count = weather_sequence.count(0)\n",
    "rainy_count = weather_sequence.count(1)\n",
    "print(f\"\\nSunny days: {sunny_count} ({sunny_count/len(weather_sequence)*100:.1f}%)\")\n",
    "print(f\"Rainy days: {rainy_count} ({rainy_count/len(weather_sequence)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weather sequence\n",
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "\n",
    "colors = ['gold' if s == 0 else 'steelblue' for s in weather_sequence]\n",
    "ax.bar(range(len(weather_sequence)), [1]*len(weather_sequence), color=colors, edgecolor='black')\n",
    "ax.set_xticks(range(0, len(weather_sequence), 5))\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Weather Simulation: 31 Days')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='gold', label='Sunny'),\n",
    "                   Patch(facecolor='steelblue', label='Rainy')]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Markov Reward Process (MRP)\n",
    "\n",
    "A **Markov Reward Process** is a Markov Chain with values (rewards).\n",
    "\n",
    "## Definition\n",
    "\n",
    "An MRP is a tuple $(S, P, R, \\gamma)$ where:\n",
    "- $S$ is a finite set of states\n",
    "- $P$ is the state transition probability matrix\n",
    "- $R$ is a reward function: $R_s = E[R_{t+1} | S_t = s]$ (expected reward in state $s$)\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0, 1]$\n",
    "\n",
    "The difference from a Markov Process: we now get rewards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Student MRP (from David Silver's course)\n",
    "# States represent a student's activities\n",
    "\n",
    "mrp_states = ['Facebook', 'Class 1', 'Class 2', 'Class 3', 'Pass', 'Pub', 'Sleep']\n",
    "n_states = len(mrp_states)\n",
    "\n",
    "# Transition matrix\n",
    "P_student = np.array([\n",
    "    # FB    C1    C2    C3    Pass  Pub   Sleep\n",
    "    [0.9,  0.1,  0.0,  0.0,  0.0,  0.0,  0.0],   # Facebook\n",
    "    [0.5,  0.0,  0.5,  0.0,  0.0,  0.0,  0.0],   # Class 1\n",
    "    [0.0,  0.0,  0.0,  0.8,  0.0,  0.0,  0.2],   # Class 2\n",
    "    [0.0,  0.0,  0.0,  0.0,  0.6,  0.4,  0.0],   # Class 3\n",
    "    [0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  1.0],   # Pass (terminal)\n",
    "    [0.2,  0.4,  0.4,  0.0,  0.0,  0.0,  0.0],   # Pub\n",
    "    [0.0,  0.0,  0.0,  0.0,  0.0,  0.0,  1.0],   # Sleep (terminal)\n",
    "])\n",
    "\n",
    "# Rewards for being in each state\n",
    "R_student = np.array([-1, -2, -2, -2, 10, 1, 0])\n",
    "\n",
    "print(\"Student MRP\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nStates and Rewards:\")\n",
    "for i, (state, reward) in enumerate(zip(mrp_states, R_student)):\n",
    "    print(f\"  State {i} ({state}): Reward = {reward}\")\n",
    "\n",
    "print(\"\\nTransition probabilities from Class 3:\")\n",
    "for j, prob in enumerate(P_student[3]):\n",
    "    if prob > 0:\n",
    "        print(f\"  → {mrp_states[j]}: {prob:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Return $G_t$\n",
    "\n",
    "The **return** $G_t$ is the total discounted reward from time-step $t$:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "The discount factor $\\gamma$ determines the present value of future rewards:\n",
    "- $\\gamma = 0$: Only immediate reward matters\n",
    "- $\\gamma = 1$: Future rewards are as important as immediate (undiscounted)\n",
    "- Typically $\\gamma \\in [0.9, 0.99]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the student MRP and calculate returns\n",
    "def simulate_mrp(P, R, initial_state, max_steps=100):\n",
    "    \"\"\"Simulate an MRP episode, returning states and rewards.\"\"\"\n",
    "    states = [initial_state]\n",
    "    rewards = []\n",
    "    current_state = initial_state\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        # Get reward for current state\n",
    "        reward = R[current_state]\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Check for terminal states (Sleep or Pass - self-loops with prob 1)\n",
    "        if current_state in [4, 6]:  # Pass or Sleep\n",
    "            break\n",
    "        \n",
    "        # Transition to next state\n",
    "        next_state = np.random.choice(len(P), p=P[current_state])\n",
    "        states.append(next_state)\n",
    "        current_state = next_state\n",
    "    \n",
    "    return states, rewards\n",
    "\n",
    "def compute_return(rewards, gamma):\n",
    "    \"\"\"Compute the discounted return from a sequence of rewards.\"\"\"\n",
    "    G = 0\n",
    "    for t, r in enumerate(rewards):\n",
    "        G += (gamma ** t) * r\n",
    "    return G\n",
    "\n",
    "# Simulate several episodes starting from Class 1\n",
    "np.random.seed(42)\n",
    "gamma = 0.9\n",
    "\n",
    "print(\"Student MRP: Sample Episodes from Class 1\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Discount factor γ = {gamma}\\n\")\n",
    "\n",
    "for episode in range(5):\n",
    "    states, rewards = simulate_mrp(P_student, R_student, initial_state=1)\n",
    "    G = compute_return(rewards, gamma)\n",
    "    \n",
    "    path = ' → '.join([mrp_states[s] for s in states])\n",
    "    print(f\"Episode {episode + 1}:\")\n",
    "    print(f\"  Path: {path}\")\n",
    "    print(f\"  Rewards: {rewards}\")\n",
    "    print(f\"  Return G = {G:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function for MRP\n",
    "\n",
    "The **state value function** $v(s)$ gives the expected return starting from state $s$:\n",
    "\n",
    "$$v(s) = E[G_t | S_t = s]$$\n",
    "\n",
    "This tells us how \"good\" it is to be in a particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate value function using Monte Carlo simulation\n",
    "def estimate_value_mc(P, R, gamma, n_episodes=10000):\n",
    "    \"\"\"Estimate value function using Monte Carlo sampling.\"\"\"\n",
    "    n_states = len(R)\n",
    "    returns_sum = np.zeros(n_states)\n",
    "    returns_count = np.zeros(n_states)\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        # Start from a random non-terminal state\n",
    "        initial_state = np.random.choice([0, 1, 2, 3, 5])  # Exclude terminal states\n",
    "        states, rewards = simulate_mrp(P, R, initial_state)\n",
    "        \n",
    "        # Calculate returns for each visited state\n",
    "        G = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            G = rewards[t] + gamma * G\n",
    "            state = states[t]\n",
    "            returns_sum[state] += G\n",
    "            returns_count[state] += 1\n",
    "    \n",
    "    # Average returns\n",
    "    V = np.zeros(n_states)\n",
    "    for s in range(n_states):\n",
    "        if returns_count[s] > 0:\n",
    "            V[s] = returns_sum[s] / returns_count[s]\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Estimate values\n",
    "V_mc = estimate_value_mc(P_student, R_student, gamma=0.9, n_episodes=50000)\n",
    "\n",
    "print(\"Estimated State Values (Monte Carlo, γ=0.9)\")\n",
    "print(\"=\" * 50)\n",
    "for i, (state, value) in enumerate(zip(mrp_states, V_mc)):\n",
    "    print(f\"  V({state}) = {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize state values\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = ['red' if v < 0 else 'green' for v in V_mc]\n",
    "bars = ax.bar(mrp_states, V_mc, color=colors, edgecolor='black')\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_ylabel('Value V(s)')\n",
    "ax.set_title('State Values in Student MRP (γ = 0.9)')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, V_mc):\n",
    "    ypos = bar.get_height() + 0.3 if val >= 0 else bar.get_height() - 0.5\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, ypos, f'{val:.1f}',\n",
    "            ha='center', va='bottom' if val >= 0 else 'top', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation for MRP\n",
    "\n",
    "The value function can be decomposed into two parts:\n",
    "1. Immediate reward $R_{t+1}$\n",
    "2. Discounted value of the successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "$$v(s) = E[R_{t+1} + \\gamma v(S_{t+1}) | S_t = s]$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$v(s) = R_s + \\gamma \\sum_{s' \\in S} P_{ss'} v(s')$$\n",
    "\n",
    "### Matrix Form\n",
    "\n",
    "The Bellman equation can be written in matrix form:\n",
    "\n",
    "$$v = R + \\gamma P v$$\n",
    "\n",
    "This is a linear equation that can be solved directly:\n",
    "\n",
    "$$(I - \\gamma P) v = R$$\n",
    "$$v = (I - \\gamma P)^{-1} R$$\n",
    "\n",
    "This direct solution has complexity $O(n^3)$ and is only practical for small state spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Bellman equation directly using matrix inversion\n",
    "def solve_mrp_bellman(P, R, gamma):\n",
    "    \"\"\"Solve MRP Bellman equation: v = (I - gamma*P)^(-1) * R\"\"\"\n",
    "    n = len(R)\n",
    "    I = np.eye(n)\n",
    "    V = np.linalg.solve(I - gamma * P, R)\n",
    "    return V\n",
    "\n",
    "# Solve directly\n",
    "V_direct = solve_mrp_bellman(P_student, R_student, gamma=0.9)\n",
    "\n",
    "print(\"State Values: Direct Solution vs Monte Carlo\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'State':<12} {'Direct':>12} {'Monte Carlo':>12} {'Difference':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for i, state in enumerate(mrp_states):\n",
    "    diff = abs(V_direct[i] - V_mc[i])\n",
    "    print(f\"{state:<12} {V_direct[i]:>12.2f} {V_mc[i]:>12.2f} {diff:>12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Bellman equation for a specific state\n",
    "state = 1  # Class 1\n",
    "gamma = 0.9\n",
    "\n",
    "print(f\"Verifying Bellman Equation for State '{mrp_states[state]}'\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBellman equation: v(s) = R_s + γ * Σ P(s'|s) * v(s')\")\n",
    "print(f\"\\nFor {mrp_states[state]}:\")\n",
    "print(f\"  R_s = {R_student[state]}\")\n",
    "print(f\"  γ = {gamma}\")\n",
    "\n",
    "# Calculate expected value of successor states\n",
    "expected_next_value = 0\n",
    "print(f\"\\n  Expected next state value:\")\n",
    "for next_state in range(n_states):\n",
    "    prob = P_student[state, next_state]\n",
    "    if prob > 0:\n",
    "        contribution = prob * V_direct[next_state]\n",
    "        expected_next_value += contribution\n",
    "        print(f\"    P({mrp_states[next_state]}|{mrp_states[state]}) × V({mrp_states[next_state]}) = {prob:.1f} × {V_direct[next_state]:.2f} = {contribution:.2f}\")\n",
    "\n",
    "print(f\"\\n  Sum: {expected_next_value:.2f}\")\n",
    "\n",
    "# Calculate total\n",
    "calculated_value = R_student[state] + gamma * expected_next_value\n",
    "print(f\"\\n  v({mrp_states[state]}) = {R_student[state]} + {gamma} × {expected_next_value:.2f} = {calculated_value:.2f}\")\n",
    "print(f\"  Direct solution: V({mrp_states[state]}) = {V_direct[state]:.2f}\")\n",
    "print(f\"\\n  ✓ Bellman equation verified!\" if abs(calculated_value - V_direct[state]) < 0.01 else \"  ✗ Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Markov Decision Process (MDP)\n",
    "\n",
    "A **Markov Decision Process** is an MRP with decisions (actions). It's the formal framework for reinforcement learning.\n",
    "\n",
    "## Definition\n",
    "\n",
    "An MDP is a tuple $(S, A, P, R, \\gamma)$ where:\n",
    "- $S$ is a finite set of states\n",
    "- $A$ is a finite set of actions\n",
    "- $P$ is the state transition probability matrix: $P_{ss'}^a = P[S_{t+1} = s' | S_t = s, A_t = a]$\n",
    "- $R$ is a reward function: $R_s^a = E[R_{t+1} | S_t = s, A_t = a]$\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0, 1]$\n",
    "\n",
    "**Key difference from MRP**: The agent can choose actions, and both transitions and rewards depend on the action taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrozenLake as an MDP\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "env.reset()\n",
    "\n",
    "print(\"FrozenLake MDP Components\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nS (State Space): {env.observation_space}\")\n",
    "print(f\"   Number of states: {env.observation_space.n}\")\n",
    "print(f\"\\nA (Action Space): {env.action_space}\")\n",
    "print(f\"   Number of actions: {env.action_space.n}\")\n",
    "print(f\"   Actions: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\")\n",
    "print(f\"\\nγ (Discount Factor): Typically 0.99 for this environment\")\n",
    "\n",
    "# Show the map\n",
    "print(\"\\nEnvironment Map:\")\n",
    "desc = env.unwrapped.desc.astype(str)\n",
    "for row in desc:\n",
    "    print(\"   \" + \" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and examine the MDP's P and R\n",
    "def extract_mdp_components(env):\n",
    "    \"\"\"Extract P (transitions) and R (rewards) from a Gymnasium environment.\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    # P[s][a] = list of (prob, next_state, reward, done)\n",
    "    P = env.unwrapped.P\n",
    "    \n",
    "    # Create transition and reward matrices\n",
    "    # T[s, a, s'] = probability of s -> s' given action a\n",
    "    T = np.zeros((n_states, n_actions, n_states))\n",
    "    R = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            for prob, next_s, reward, done in P[s][a]:\n",
    "                T[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward  # Expected reward\n",
    "    \n",
    "    return T, R\n",
    "\n",
    "T, R = extract_mdp_components(env)\n",
    "\n",
    "print(\"MDP Transition Matrix T[s,a,s']\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {T.shape} (states × actions × next_states)\")\n",
    "\n",
    "# Show transitions from state 6\n",
    "state = 6\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "print(f\"\\nTransitions from state {state}:\")\n",
    "for a in range(4):\n",
    "    print(f\"\\n  Action {a} ({action_names[a]}):\")\n",
    "    for next_s in range(16):\n",
    "        if T[state, a, next_s] > 0:\n",
    "            print(f\"    → State {next_s}: P = {T[state, a, next_s]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transition probabilities from one state\n",
    "def visualize_mdp_transitions(env, state):\n",
    "    \"\"\"Visualize transitions from a state for all actions.\"\"\"\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for action, ax in enumerate(axes):\n",
    "        transitions = env.unwrapped.P[state][action]\n",
    "        trans_dict = {ns: p for p, ns, _, _ in transitions if p > 0}\n",
    "        \n",
    "        for i in range(nrow):\n",
    "            for j in range(ncol):\n",
    "                cell = desc[i, j]\n",
    "                state_idx = i * ncol + j\n",
    "                \n",
    "                if state_idx == state:\n",
    "                    facecolor = 'yellow'\n",
    "                elif state_idx in trans_dict:\n",
    "                    prob = trans_dict[state_idx]\n",
    "                    facecolor = plt.cm.Blues(0.3 + 0.7 * prob)\n",
    "                else:\n",
    "                    facecolor = colors.get(cell, 'white')\n",
    "                \n",
    "                rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                     facecolor=facecolor, edgecolor='black')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add text\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.7, cell,\n",
    "                       ha='center', va='center', fontsize=12)\n",
    "                \n",
    "                if state_idx in trans_dict:\n",
    "                    ax.text(j + 0.5, nrow - 1 - i + 0.3, f'{trans_dict[state_idx]:.2f}',\n",
    "                           ha='center', va='center', fontsize=10, color='blue', fontweight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, ncol)\n",
    "        ax.set_ylim(0, nrow)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Action: {action_names[action]}')\n",
    "    \n",
    "    plt.suptitle(f'Transition Probabilities from State {state} (Yellow)\\n(Blue intensity = probability)', \n",
    "                fontsize=12, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "visualize_mdp_transitions(env, state=6)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Due to slippery ice, the intended action only succeeds 1/3 of the time!\")\n",
    "print(\"The agent might slip and move perpendicular to the intended direction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Policies in MDPs\n",
    "\n",
    "A **policy** $\\pi$ is a distribution over actions given states:\n",
    "\n",
    "$$\\pi(a|s) = P[A_t = a | S_t = s]$$\n",
    "\n",
    "Properties:\n",
    "- A policy fully defines an agent's behavior\n",
    "- MDP policies depend only on the current state (not history) - they are **stationary**\n",
    "- Given an MDP and a policy, we get an MRP\n",
    "\n",
    "## From MDP + Policy to MRP\n",
    "\n",
    "Given MDP $M = (S, A, P, R, \\gamma)$ and policy $\\pi$:\n",
    "\n",
    "The state sequence $S_1, S_2, \\ldots$ is a Markov process with:\n",
    "\n",
    "$$P_{ss'}^\\pi = \\sum_{a \\in A} \\pi(a|s) P_{ss'}^a$$\n",
    "\n",
    "$$R_s^\\pi = \\sum_{a \\in A} \\pi(a|s) R_s^a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different policies for FrozenLake\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Uniform random policy: equal probability for all actions\n",
    "def uniform_policy(n_actions):\n",
    "    \"\"\"Return a uniform random policy.\"\"\"\n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "    return policy\n",
    "\n",
    "# Deterministic policy: always go right then down\n",
    "def right_down_policy():\n",
    "    \"\"\"Policy that prefers RIGHT, then DOWN.\"\"\"\n",
    "    # Encoded as probabilities for each action [LEFT, DOWN, RIGHT, UP]\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        row, col = s // 4, s % 4\n",
    "        if col < 3:  # Can go right\n",
    "            policy[s, 2] = 1.0  # RIGHT\n",
    "        else:  # At rightmost column, go down\n",
    "            policy[s, 1] = 1.0  # DOWN\n",
    "    return policy\n",
    "\n",
    "pi_random = uniform_policy(n_actions)\n",
    "pi_right_down = right_down_policy()\n",
    "\n",
    "print(\"Example Policies\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nUniform Random Policy π_random(a|s=0):\")\n",
    "for a in range(n_actions):\n",
    "    print(f\"  π({action_names[a]}|s=0) = {pi_random[0, a]:.2f}\")\n",
    "\n",
    "print(\"\\nRight-then-Down Policy π_rd(a|s=0):\")\n",
    "for a in range(n_actions):\n",
    "    print(f\"  π({action_names[a]}|s=0) = {pi_right_down[0, a]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MDP + Policy to MRP\n",
    "def mdp_to_mrp(T, R_sa, policy):\n",
    "    \"\"\"Convert MDP with policy to MRP.\n",
    "    \n",
    "    Args:\n",
    "        T: Transition matrix T[s,a,s']\n",
    "        R_sa: Reward matrix R[s,a]\n",
    "        policy: Policy matrix π[s,a]\n",
    "    \n",
    "    Returns:\n",
    "        P_mrp: MRP transition matrix P[s,s']\n",
    "        R_mrp: MRP reward vector R[s]\n",
    "    \"\"\"\n",
    "    n_states = T.shape[0]\n",
    "    n_actions = T.shape[1]\n",
    "    \n",
    "    # P_ss'^π = Σ_a π(a|s) * P_ss'^a\n",
    "    P_mrp = np.zeros((n_states, n_states))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            P_mrp[s] += policy[s, a] * T[s, a]\n",
    "    \n",
    "    # R_s^π = Σ_a π(a|s) * R_s^a\n",
    "    R_mrp = np.sum(policy * R_sa, axis=1)\n",
    "    \n",
    "    return P_mrp, R_mrp\n",
    "\n",
    "# Convert FrozenLake MDP with random policy to MRP\n",
    "P_mrp, R_mrp = mdp_to_mrp(T, R, pi_random)\n",
    "\n",
    "print(\"MDP + Random Policy → MRP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nMRP Transition Matrix shape: {P_mrp.shape}\")\n",
    "print(f\"MRP Reward Vector shape: {R_mrp.shape}\")\n",
    "print(f\"\\nRow sums of P_mrp (should be 1): {P_mrp.sum(axis=1)[:4]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Value Functions for MDPs\n",
    "\n",
    "## State-Value Function $V^\\pi(s)$\n",
    "\n",
    "The **state-value function** $V^\\pi(s)$ is the expected return starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = E_\\pi[G_t | S_t = s]$$\n",
    "\n",
    "## Action-Value Function $Q^\\pi(s, a)$\n",
    "\n",
    "The **action-value function** $Q^\\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, then following policy $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s, a) = E_\\pi[G_t | S_t = s, A_t = a]$$\n",
    "\n",
    "The Q-function is more detailed - it tells us how good each action is in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute state-value function for a policy (using MRP solution)\n",
    "def compute_V_pi(T, R_sa, policy, gamma):\n",
    "    \"\"\"Compute V^π using direct solution of Bellman equation.\"\"\"\n",
    "    P_mrp, R_mrp = mdp_to_mrp(T, R_sa, policy)\n",
    "    n_states = len(R_mrp)\n",
    "    I = np.eye(n_states)\n",
    "    V = np.linalg.solve(I - gamma * P_mrp, R_mrp)\n",
    "    return V\n",
    "\n",
    "# Compute Q from V\n",
    "def compute_Q_from_V(T, R_sa, V, gamma):\n",
    "    \"\"\"Compute Q^π from V^π.\"\"\"\n",
    "    n_states, n_actions = R_sa.shape\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            # Q(s,a) = R(s,a) + γ * Σ P(s'|s,a) * V(s')\n",
    "            Q[s, a] = R_sa[s, a] + gamma * np.sum(T[s, a] * V)\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Compute values for random policy\n",
    "gamma = 0.99\n",
    "V_random = compute_V_pi(T, R, pi_random, gamma)\n",
    "Q_random = compute_Q_from_V(T, R, V_random, gamma)\n",
    "\n",
    "print(f\"Value Functions for Random Policy (γ = {gamma})\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nState-Value Function V^π(s):\")\n",
    "for s in range(16):\n",
    "    print(f\"  V({s:2d}) = {V_random[s]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize state values as a heatmap\n",
    "def plot_value_function(V, env, title=\"State Value Function\"):\n",
    "    \"\"\"Plot V as a heatmap on the FrozenLake grid.\"\"\"\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    \n",
    "    # Reshape V to grid\n",
    "    V_grid = V.reshape(nrow, ncol)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=max(V.max(), 0.01))\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Value V(s)', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            cell = desc[i, j]\n",
    "            state = i * ncol + j\n",
    "            \n",
    "            # Cell type\n",
    "            ax.text(j, i - 0.25, cell, ha='center', va='center', \n",
    "                   fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Value\n",
    "            ax.text(j, i + 0.25, f'{V[state]:.3f}', ha='center', va='center',\n",
    "                   fontsize=10)\n",
    "    \n",
    "    ax.set_xticks(range(ncol))\n",
    "    ax.set_yticks(range(nrow))\n",
    "    ax.set_xticklabels(range(ncol))\n",
    "    ax.set_yticklabels(range(nrow))\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "plot_value_function(V_random, env, title=f\"State Values V^π for Random Policy (γ={gamma})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q-values\n",
    "def plot_q_values(Q, env, title=\"Q-Values\"):\n",
    "    \"\"\"Plot Q-values showing the value of each action in each state.\"\"\"\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    n_actions = Q.shape[1]\n",
    "    action_names = ['←', '↓', '→', '↑']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            # Background color based on cell type\n",
    "            colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add cell label\n",
    "            ax.text(j + 0.5, nrow - 1 - i + 0.5, cell, ha='center', va='center',\n",
    "                   fontsize=14, fontweight='bold', alpha=0.3)\n",
    "            \n",
    "            # Add Q-values for each action (if not hole or goal)\n",
    "            if cell not in ['H', 'G']:\n",
    "                best_action = np.argmax(Q[state])\n",
    "                positions = [(0.25, 0.5), (0.5, 0.25), (0.75, 0.5), (0.5, 0.75)]  # L, D, R, U\n",
    "                for a in range(n_actions):\n",
    "                    x, y = positions[a]\n",
    "                    color = 'green' if a == best_action else 'black'\n",
    "                    weight = 'bold' if a == best_action else 'normal'\n",
    "                    ax.text(j + x, nrow - 1 - i + y, f'{Q[state, a]:.2f}',\n",
    "                           ha='center', va='center', fontsize=7, color=color, fontweight=weight)\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"{title}\\n(Green = best action, positions: ←left, ↓bottom, →right, ↑top)\", fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "plot_q_values(Q_random, env, title=\"Q-Values for Random Policy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Bellman Expectation Equations\n",
    "\n",
    "The Bellman equations express the recursive relationship between value functions.\n",
    "\n",
    "## Bellman Expectation Equation for $V^\\pi$\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s') \\right]$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) Q^\\pi(s, a)$$\n",
    "\n",
    "## Bellman Expectation Equation for $Q^\\pi$\n",
    "\n",
    "$$Q^\\pi(s, a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s')$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$Q^\\pi(s, a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a')$$\n",
    "\n",
    "## Relationship Between V and Q\n",
    "\n",
    "- $V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$ — V is the expected Q over actions\n",
    "- $Q^\\pi(s,a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s')$ — Q is immediate reward plus discounted future V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Bellman Expectation Equation for a specific state\n",
    "state = 6  # A non-terminal state in the middle\n",
    "\n",
    "print(f\"Verifying Bellman Expectation Equation at State {state}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nUsing random policy with γ = {gamma}\")\n",
    "\n",
    "# Method 1: Directly computed V(s)\n",
    "V_direct = V_random[state]\n",
    "print(f\"\\nDirect computation: V^π({state}) = {V_direct:.6f}\")\n",
    "\n",
    "# Method 2: Using Bellman equation\n",
    "# V(s) = Σ_a π(a|s) * Q(s,a)\n",
    "V_bellman = 0\n",
    "print(f\"\\nBellman equation: V^π(s) = Σ_a π(a|s) * Q^π(s,a)\")\n",
    "for a in range(n_actions):\n",
    "    contribution = pi_random[state, a] * Q_random[state, a]\n",
    "    V_bellman += contribution\n",
    "    print(f\"  π({action_names[a]}|{state}) × Q({state},{action_names[a]}) = {pi_random[state,a]:.2f} × {Q_random[state,a]:.4f} = {contribution:.6f}\")\n",
    "\n",
    "print(f\"\\nSum: V^π({state}) = {V_bellman:.6f}\")\n",
    "print(f\"\\n✓ Verified!\" if abs(V_direct - V_bellman) < 1e-6 else \"✗ Error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Q = R + γ * Σ P * V\n",
    "state = 6\n",
    "action = 2  # RIGHT\n",
    "\n",
    "print(f\"\\nVerifying Q = R + γΣP·V at State {state}, Action {action_names[action]}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Direct Q value\n",
    "Q_direct = Q_random[state, action]\n",
    "print(f\"\\nDirect computation: Q^π({state},{action_names[action]}) = {Q_direct:.6f}\")\n",
    "\n",
    "# Using Bellman equation\n",
    "R_sa = R[state, action]\n",
    "expected_V = 0\n",
    "print(f\"\\nBellman equation: Q(s,a) = R(s,a) + γ * Σ P(s'|s,a) * V(s')\")\n",
    "print(f\"\\nR({state},{action_names[action]}) = {R_sa:.4f}\")\n",
    "print(f\"\\nExpected future value:\")\n",
    "for next_s in range(n_states):\n",
    "    if T[state, action, next_s] > 0:\n",
    "        contribution = T[state, action, next_s] * V_random[next_s]\n",
    "        expected_V += contribution\n",
    "        print(f\"  P({next_s}|{state},{action_names[action]}) × V({next_s}) = {T[state,action,next_s]:.2f} × {V_random[next_s]:.4f} = {contribution:.6f}\")\n",
    "\n",
    "Q_bellman = R_sa + gamma * expected_V\n",
    "print(f\"\\nQ({state},{action_names[action]}) = {R_sa:.4f} + {gamma} × {expected_V:.6f} = {Q_bellman:.6f}\")\n",
    "print(f\"\\n✓ Verified!\" if abs(Q_direct - Q_bellman) < 1e-6 else \"✗ Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Optimal Value Functions\n",
    "\n",
    "The **optimal state-value function** $V^*(s)$ is the maximum value function over all policies:\n",
    "\n",
    "$$V^*(s) = \\max_\\pi V^\\pi(s)$$\n",
    "\n",
    "The **optimal action-value function** $Q^*(s, a)$ is the maximum action-value function over all policies:\n",
    "\n",
    "$$Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)$$\n",
    "\n",
    "## Key Theorem\n",
    "\n",
    "For any MDP:\n",
    "1. There exists an optimal policy $\\pi^*$ that is better than or equal to all other policies\n",
    "2. All optimal policies achieve the same optimal value functions $V^*$ and $Q^*$\n",
    "3. There always exists a **deterministic** optimal policy\n",
    "\n",
    "## Finding the Optimal Policy from $Q^*$\n",
    "\n",
    "If we know $Q^*(s, a)$, the optimal policy is simply:\n",
    "\n",
    "$$\\pi^*(a|s) = \\begin{cases} 1 & \\text{if } a = \\arg\\max_{a'} Q^*(s, a') \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Just pick the action with the highest Q-value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equations\n",
    "\n",
    "The optimal value functions satisfy the **Bellman Optimality Equations**:\n",
    "\n",
    "### For $V^*$:\n",
    "\n",
    "$$V^*(s) = \\max_a \\left[ R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^*(s') \\right]$$\n",
    "\n",
    "### For $Q^*$:\n",
    "\n",
    "$$Q^*(s, a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a \\max_{a'} Q^*(s', a')$$\n",
    "\n",
    "### Key Difference from Bellman Expectation Equations:\n",
    "- Expectation equations: **sum** over actions weighted by policy\n",
    "- Optimality equations: **max** over actions\n",
    "\n",
    "The **max** makes these equations **non-linear**, so we can't solve them directly with matrix inversion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration to find V* (preview - we'll cover this in detail in notebook 03)\n",
    "def value_iteration(T, R, gamma, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"Find optimal value function using Value Iteration.\"\"\"\n",
    "    n_states = T.shape[0]\n",
    "    n_actions = T.shape[1]\n",
    "    \n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        for s in range(n_states):\n",
    "            # V*(s) = max_a [R(s,a) + γ * Σ P(s'|s,a) * V*(s')]\n",
    "            q_values = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                q_values[a] = R[s, a] + gamma * np.sum(T[s, a] * V)\n",
    "            V_new[s] = np.max(q_values)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        V = V_new\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Find optimal value function\n",
    "V_star = value_iteration(T, R, gamma=0.99)\n",
    "\n",
    "# Compute Q* from V*\n",
    "Q_star = compute_Q_from_V(T, R, V_star, gamma=0.99)\n",
    "\n",
    "# Extract optimal policy\n",
    "pi_star = np.zeros((n_states, n_actions))\n",
    "for s in range(n_states):\n",
    "    best_action = np.argmax(Q_star[s])\n",
    "    pi_star[s, best_action] = 1.0\n",
    "\n",
    "print(\"\\nOptimal Value Function V*:\")\n",
    "print(V_star.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimal values and policy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot V*\n",
    "desc = env.unwrapped.desc.astype(str)\n",
    "nrow, ncol = desc.shape\n",
    "V_grid = V_star.reshape(nrow, ncol)\n",
    "\n",
    "im = axes[0].imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=V_star.max())\n",
    "plt.colorbar(im, ax=axes[0], label='V*(s)')\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        state = i * ncol + j\n",
    "        axes[0].text(j, i, f'{desc[i,j]}\\n{V_star[state]:.3f}', \n",
    "                    ha='center', va='center', fontsize=10)\n",
    "\n",
    "axes[0].set_title('Optimal Value Function V*', fontsize=12)\n",
    "axes[0].set_xticks(range(ncol))\n",
    "axes[0].set_yticks(range(nrow))\n",
    "\n",
    "# Plot optimal policy\n",
    "action_arrows = ['←', '↓', '→', '↑']\n",
    "colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        state = i * ncol + j\n",
    "        cell = desc[i, j]\n",
    "        \n",
    "        rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                             facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "        axes[1].add_patch(rect)\n",
    "        \n",
    "        # Add cell label and optimal action\n",
    "        best_action = np.argmax(Q_star[state])\n",
    "        if cell not in ['H', 'G']:\n",
    "            axes[1].text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                        f'{cell}\\n{action_arrows[best_action]}',\n",
    "                        ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            axes[1].text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                        ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[1].set_xlim(0, ncol)\n",
    "axes[1].set_ylim(0, nrow)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Optimal Policy π*\\n(arrows show best action)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random policy vs optimal policy performance\n",
    "def evaluate_policy(env, policy, n_episodes=10000):\n",
    "    \"\"\"Evaluate a policy by running episodes.\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Sample action from policy\n",
    "            action = np.random.choice(env.action_space.n, p=policy[obs])\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "# Evaluate both policies\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "print(\"Policy Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rewards_random = evaluate_policy(env, pi_random, n_episodes=10000)\n",
    "rewards_optimal = evaluate_policy(env, pi_star, n_episodes=10000)\n",
    "\n",
    "print(f\"\\nRandom Policy:\")\n",
    "print(f\"  Success rate: {np.mean(rewards_random)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOptimal Policy:\")\n",
    "print(f\"  Success rate: {np.mean(rewards_optimal)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nImprovement: {(np.mean(rewards_optimal) - np.mean(rewards_random))*100:.2f} percentage points\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "policies = ['Random Policy', 'Optimal Policy']\n",
    "success_rates = [np.mean(rewards_random) * 100, np.mean(rewards_optimal) * 100]\n",
    "colors = ['gray', 'green']\n",
    "\n",
    "bars = ax.bar(policies, success_rates, color=colors, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Success Rate (%)')\n",
    "ax.set_title('Random vs Optimal Policy on FrozenLake\\n(10,000 episodes each)')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this notebook, we covered the mathematical foundations of reinforcement learning:\n",
    "\n",
    "## 1. Markov Process\n",
    "- Tuple $(S, P)$: states and transition probabilities\n",
    "- Memoryless random process\n",
    "\n",
    "## 2. Markov Reward Process (MRP)\n",
    "- Tuple $(S, P, R, \\gamma)$: adds rewards and discount factor\n",
    "- **Return**: $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "- **Value function**: $v(s) = E[G_t | S_t = s]$\n",
    "- **Bellman equation**: $v = R + \\gamma P v$\n",
    "\n",
    "## 3. Markov Decision Process (MDP)\n",
    "- Tuple $(S, A, P, R, \\gamma)$: adds actions\n",
    "- **Policy**: $\\pi(a|s)$ - how to choose actions\n",
    "- **State-value**: $V^\\pi(s)$ - expected return from state $s$\n",
    "- **Action-value**: $Q^\\pi(s,a)$ - expected return from state $s$ taking action $a$\n",
    "\n",
    "## 4. Bellman Equations\n",
    "- **Expectation equations**: Recursive definition of $V^\\pi$ and $Q^\\pi$\n",
    "- **Optimality equations**: Define $V^*$ and $Q^*$ using **max** instead of sum\n",
    "\n",
    "## 5. Optimal Policy\n",
    "- $\\pi^*(a|s) = 1$ if $a = \\arg\\max_{a'} Q^*(s, a')$\n",
    "- Always exists and is deterministic\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook (**03_dynamic_programming.ipynb**), we'll learn algorithms to solve MDPs:\n",
    "- Policy Evaluation\n",
    "- Policy Iteration\n",
    "- Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congratulations! You've completed Part 2 of the RL Tutorial!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- MDPs provide the mathematical framework for RL\")\n",
    "print(\"- Value functions tell us how good states/actions are\")\n",
    "print(\"- Bellman equations relate values recursively\")\n",
    "print(\"- Optimal values satisfy the Bellman optimality equations\")\n",
    "print(\"- The optimal policy selects the action with highest Q*\")\n",
    "print(\"\\nNext: 03_dynamic_programming.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
