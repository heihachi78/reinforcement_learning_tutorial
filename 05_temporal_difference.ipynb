{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 5: Temporal Difference Learning\n\nIn this notebook, we'll learn **Temporal Difference (TD)** methods - the most important class of model-free RL algorithms that combine ideas from Monte Carlo and Dynamic Programming.\n\n## Recap from Notebook 04\n- **Monte Carlo methods** learn from complete episodes without needing the MDP model\n- **First-visit MC** estimates values by averaging returns from sampled episodes\n- **MC Control** (GLIE MC) finds optimal policies using ε-greedy exploration\n- **High variance**: Returns have high variance due to stochastic episodes\n- **Limitation**: Must wait until episode ends — cannot learn online during episodes\n\n## What This Notebook Covers\n- TD(0) prediction: online learning from single steps\n- The bias-variance tradeoff (TD vs MC)\n- SARSA (On-policy TD control)\n- Q-Learning (Off-policy TD control)\n- Comparison of on-policy vs off-policy learning\n\n## What This Notebook Does NOT Cover\n\n| Topic | Why Not Here | How It Differs From What We Cover |\n|-------|--------------|-----------------------------------|\n| **Policy gradient methods** | TD learning improves value functions. Policy gradients directly optimize the policy using gradient ascent, which is fundamentally different. | We learn Q(s,a) and derive policy greedily. Policy gradient methods parameterize π(a\\|s;θ) directly and use ∇_θ J(θ) to improve it — better for continuous actions but requires calculus of variations. |\n| **Actor-critic methods** | Actor-critic combines value functions with policy gradients. We focus on pure value-based TD methods first. | We use either on-policy (SARSA) or off-policy (Q-learning) value learning. Actor-critic maintains both a policy (actor) and value function (critic) simultaneously — more complex but often more stable. |\n| **Deep reinforcement learning** | We use tabular Q-tables. Deep RL uses neural networks for function approximation, adding convergence challenges. | In this notebook, we maintain Q[s,a] tables for all pairs. Deep Q-Networks (DQN) use neural networks Q(s,a;θ) with experience replay and target networks — essential for complex domains. |\n| **Eligibility traces (TD(λ))** | We focus on one-step TD (TD(0)). Eligibility traces allow n-step and continuous spectrum between MC and TD. | We update using just the next reward and value: R + γV(s'). TD(λ) uses λ-weighted average of all n-step returns — bridges MC and TD but requires additional bookkeeping. |\n\n## Preview: Bootstrapping - The Key Insight\n\nTemporal Difference learning introduces a powerful concept called **bootstrapping**:\n\n**Monte Carlo** waits until episode end:\n$$V(S_t) \\leftarrow V(S_t) + \\alpha [G_t - V(S_t)]$$\nwhere $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$ (complete return)\n\n**TD Learning** updates immediately using current estimate:\n$$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$\n\nThe magic: $R_{t+1} + \\gamma V(S_{t+1})$ is called the **TD target** — it's an estimate of the return using our current value function! This allows us to:\n- Learn **online** (every step, not just at episode end)\n- Work with **continuing tasks** (don't need episodes to terminate)\n- Reduce **variance** (single reward vs. full trajectory)\n\n## How to Read This Notebook\n1. **Theory and algorithms**: Each section introduces TD prediction and control algorithms with mathematical foundations\n2. **Bootstrapping intuition**: Understand the key difference between MC (actual returns) and TD (estimated returns)\n3. **Step-by-step implementations**: Run code cells to see TD(0), SARSA, and Q-learning solve FrozenLake\n4. **On-policy vs off-policy**: Compare how SARSA learns about its behavior policy while Q-learning learns the optimal policy\n5. **Visualizations**: Observe how policies and values evolve during online learning\n\nLet's begin!\n\n## Prerequisites\n- Understanding of MDPs and Bellman equations (Notebooks 01-02)\n- Monte Carlo methods (Notebook 04)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and helper variables\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['←', '↓', '→', '↑']\n",
    "\n",
    "print(\"FrozenLake Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"States: {n_states}\")\n",
    "print(f\"Actions: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def plot_value_function(V, title=\"Value Function\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    V_grid = V.reshape(nrow, ncol)\n",
    "    \n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=max(V.max(), 0.01))\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            color = 'white' if V_grid[i, j] < V.max() / 2 else 'black'\n",
    "            ax.text(j, i, f'{cell}\\n{V[state]:.3f}', ha='center', va='center',\n",
    "                   fontsize=9, color=color)\n",
    "    \n",
    "    ax.set_xticks(range(ncol))\n",
    "    ax.set_yticks(range(nrow))\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def plot_policy(Q, title=\"Policy\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            best_action = np.argmax(Q[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "print(\"Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. What is Temporal Difference Learning?\n",
    "\n",
    "**Temporal Difference (TD)** learning combines ideas from:\n",
    "- **Monte Carlo**: Learn from experience (model-free)\n",
    "- **Dynamic Programming**: Bootstrap (update estimates based on other estimates)\n",
    "\n",
    "## Key Insight: Bootstrapping\n",
    "\n",
    "**Monte Carlo** waits until the end of episode to update:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))$$\n",
    "\n",
    "where $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$ (full return)\n",
    "\n",
    "**TD** updates immediately using estimated return:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$$\n",
    "\n",
    "The term $R_{t+1} + \\gamma V(S_{t+1})$ is called the **TD target**.\n",
    "\n",
    "The difference $(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$ is the **TD error** ($\\delta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of TD\n",
    "\n",
    "1. **Online learning**: Update after every step, not just at episode end\n",
    "2. **Works for continuing tasks**: Don't need episodes to terminate\n",
    "3. **Lower variance**: Uses single reward + estimate instead of full return\n",
    "4. **Often faster convergence**: Especially in practice\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "1. **Biased**: Bootstrapping introduces bias from current estimates\n",
    "2. **Depends on initialization**: Bad initial values can slow learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. TD(0) Prediction\n",
    "\n",
    "The simplest TD method: update after each step using immediate reward and next state estimate.\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate\n",
    "- $R_{t+1} + \\gamma V(S_{t+1})$ is the TD target\n",
    "- $R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ is the TD error $\\delta_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td0_prediction(env, policy, gamma, alpha, n_episodes):\n",
    "    \"\"\"\n",
    "    TD(0) Prediction for estimating V^π.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy to evaluate (π[s,a] probabilities)\n",
    "        gamma: Discount factor\n",
    "        alpha: Learning rate\n",
    "        n_episodes: Number of episodes\n",
    "    \n",
    "    Returns:\n",
    "        V: Estimated state value function\n",
    "        V_history: V at intervals for visualization\n",
    "        td_errors: TD errors during training\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    V = np.zeros(n_states)\n",
    "    V_history = [V.copy()]\n",
    "    td_errors = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action according to policy\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # TD update\n",
    "            # V(s) <- V(s) + α * [r + γV(s') - V(s)]\n",
    "            td_target = reward + gamma * V[next_state] * (not done)\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] += alpha * td_error\n",
    "            \n",
    "            td_errors.append(td_error)\n",
    "            state = next_state\n",
    "        \n",
    "        # Save history at intervals\n",
    "        if (episode + 1) % (n_episodes // 10) == 0:\n",
    "            V_history.append(V.copy())\n",
    "    \n",
    "    return V, V_history, td_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TD(0) prediction for random policy\n",
    "uniform_policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "print(\"TD(0) Prediction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "V_td, V_history_td, td_errors = td0_prediction(\n",
    "    env, uniform_policy, gamma=0.99, alpha=0.1, n_episodes=50000\n",
    ")\n",
    "\n",
    "print(f\"\\nEstimated V^π (random policy):\")\n",
    "print(V_td.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TD(0) with true values (from DP)\n",
    "def extract_mdp(env):\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    return P, R\n",
    "\n",
    "def policy_evaluation_dp(P, R, policy, gamma, theta=1e-8):\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                V_new[s] += policy[s, a] * (R[s, a] + gamma * np.sum(P[s, a] * V))\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    return V\n",
    "\n",
    "P, R = extract_mdp(env)\n",
    "V_true = policy_evaluation_dp(P, R, uniform_policy, gamma=0.99)\n",
    "\n",
    "print(\"Comparison: TD(0) vs True Values (DP)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean Absolute Error: {np.mean(np.abs(V_td - V_true)):.4f}\")\n",
    "print(f\"Max Absolute Error: {np.max(np.abs(V_td - V_true)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TD(0) convergence\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot V at different stages\n",
    "episodes_at = [0, 5000, 10000, 25000, 50000]\n",
    "for idx, (ax, ep) in enumerate(zip(axes.flat[:-1], episodes_at)):\n",
    "    hist_idx = min(idx, len(V_history_td)-1)\n",
    "    plot_value_function(V_history_td[hist_idx], title=f\"After {ep} episodes\", ax=ax)\n",
    "\n",
    "# TD error over time\n",
    "ax = axes.flat[-1]\n",
    "window = 1000\n",
    "td_errors_smooth = np.convolve(np.abs(td_errors), np.ones(window)/window, mode='valid')\n",
    "ax.plot(td_errors_smooth)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('|TD Error| (moving avg)')\n",
    "ax.set_title('TD Error Over Time')\n",
    "\n",
    "plt.suptitle(\"TD(0) Prediction Convergence\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. SARSA: On-Policy TD Control\n",
    "\n",
    "**SARSA** (State-Action-Reward-State-Action) is an on-policy TD control algorithm.\n",
    "\n",
    "## The SARSA Update\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\n",
    "\n",
    "The name comes from the quintuple: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "\n",
    "## On-Policy\n",
    "\n",
    "SARSA is **on-policy**: it learns about the policy it's following.\n",
    "- Uses $A_{t+1}$ which is selected by the same policy\n",
    "- The Q-values converge to $Q^\\pi$ for the behavior policy $\\pi$\n",
    "- Typically uses ε-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, gamma, alpha, n_episodes, epsilon=0.1, \n",
    "          epsilon_decay=1.0, min_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    SARSA: On-policy TD Control.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        gamma: Discount factor\n",
    "        alpha: Learning rate\n",
    "        n_episodes: Number of episodes\n",
    "        epsilon: Exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        min_epsilon: Minimum epsilon\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned Q-values\n",
    "        stats: Training statistics\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilons = []\n",
    "    \n",
    "    def epsilon_greedy_action(state, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(n_actions)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = epsilon_greedy_action(state, epsilon)\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Choose next action (for SARSA update)\n",
    "            next_action = epsilon_greedy_action(next_state, epsilon)\n",
    "            \n",
    "            # SARSA update\n",
    "            # Q(s,a) <- Q(s,a) + α * [r + γ*Q(s',a') - Q(s,a)]\n",
    "            td_target = reward + gamma * Q[next_state, next_action] * (not done)\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    stats = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'epsilons': epsilons\n",
    "    }\n",
    "    \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SARSA\n",
    "print(\"SARSA Training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "Q_sarsa, stats_sarsa = sarsa(\n",
    "    env, gamma=0.99, alpha=0.1, n_episodes=100000,\n",
    "    epsilon=1.0, epsilon_decay=0.99995, min_epsilon=0.01\n",
    ")\n",
    "sarsa_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {sarsa_time:.2f} seconds\")\n",
    "print(f\"Final epsilon: {stats_sarsa['epsilons'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SARSA training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Learning curve\n",
    "window = 1000\n",
    "rewards_smooth = np.convolve(stats_sarsa['episode_rewards'], \n",
    "                              np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(rewards_smooth)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward (moving avg)')\n",
    "axes[0, 0].set_title(f'SARSA Learning Curve (window={window})')\n",
    "\n",
    "# Epsilon decay\n",
    "axes[0, 1].plot(stats_sarsa['epsilons'])\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].set_title('Exploration Rate Decay')\n",
    "\n",
    "# Value function\n",
    "V_sarsa = np.max(Q_sarsa, axis=1)\n",
    "plot_value_function(V_sarsa, title=\"Learned V = max Q(s,a)\", ax=axes[1, 0])\n",
    "\n",
    "# Policy\n",
    "plot_policy(Q_sarsa, title=\"Learned Policy\", ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle(\"SARSA Results (100,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Q-Learning: Off-Policy TD Control\n",
    "\n",
    "**Q-Learning** is an off-policy TD control algorithm - the most famous RL algorithm!\n",
    "\n",
    "## The Q-Learning Update\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
    "\n",
    "## Off-Policy\n",
    "\n",
    "Q-Learning is **off-policy**: it learns about the optimal policy while following a different (exploratory) policy.\n",
    "\n",
    "Key difference from SARSA:\n",
    "- **SARSA**: Uses $Q(S_{t+1}, A_{t+1})$ where $A_{t+1}$ comes from the behavior policy\n",
    "- **Q-Learning**: Uses $\\max_a Q(S_{t+1}, a)$ - the value of the best action\n",
    "\n",
    "This means Q-Learning directly learns $Q^*$ regardless of the policy being followed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, alpha, n_episodes, epsilon=0.1,\n",
    "               epsilon_decay=1.0, min_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Q-Learning: Off-policy TD Control.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        gamma: Discount factor\n",
    "        alpha: Learning rate\n",
    "        n_episodes: Number of episodes\n",
    "        epsilon: Exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        min_epsilon: Minimum epsilon\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned Q-values\n",
    "        stats: Training statistics\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilons = []\n",
    "    \n",
    "    def epsilon_greedy_action(state, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(n_actions)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action using ε-greedy\n",
    "            action = epsilon_greedy_action(state, epsilon)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-Learning update\n",
    "            # Q(s,a) <- Q(s,a) + α * [r + γ*max_a' Q(s',a') - Q(s,a)]\n",
    "            td_target = reward + gamma * np.max(Q[next_state]) * (not done)\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    stats = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'epsilons': epsilons\n",
    "    }\n",
    "    \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Q-Learning\n",
    "print(\"Q-Learning Training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "Q_qlearn, stats_qlearn = q_learning(\n",
    "    env, gamma=0.99, alpha=0.1, n_episodes=100000,\n",
    "    epsilon=1.0, epsilon_decay=0.99995, min_epsilon=0.01\n",
    ")\n",
    "qlearn_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {qlearn_time:.2f} seconds\")\n",
    "print(f\"Final epsilon: {stats_qlearn['epsilons'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Q-Learning training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Learning curve\n",
    "window = 1000\n",
    "rewards_smooth = np.convolve(stats_qlearn['episode_rewards'], \n",
    "                              np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(rewards_smooth)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward (moving avg)')\n",
    "axes[0, 0].set_title(f'Q-Learning Curve (window={window})')\n",
    "\n",
    "# Epsilon decay\n",
    "axes[0, 1].plot(stats_qlearn['epsilons'])\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].set_title('Exploration Rate Decay')\n",
    "\n",
    "# Value function\n",
    "V_qlearn = np.max(Q_qlearn, axis=1)\n",
    "plot_value_function(V_qlearn, title=\"Learned V = max Q(s,a)\", ax=axes[1, 0])\n",
    "\n",
    "# Policy\n",
    "plot_policy(Q_qlearn, title=\"Learned Policy\", ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle(\"Q-Learning Results (100,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. SARSA vs Q-Learning Comparison\n",
    "\n",
    "Let's compare the two algorithms in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both policies\n",
    "def evaluate_policy(env, Q, n_episodes=10000):\n",
    "    \"\"\"Evaluate a greedy policy derived from Q.\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return np.array(rewards)\n",
    "\n",
    "# Get optimal Q from DP for comparison\n",
    "def value_iteration(P, R, gamma, theta=1e-8):\n",
    "    n_states, n_actions = R.shape\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = np.max([R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)])\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    return Q\n",
    "\n",
    "Q_optimal = value_iteration(P, R, gamma=0.99)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Policy Evaluation Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rewards_sarsa_eval = evaluate_policy(env, Q_sarsa, n_episodes=10000)\n",
    "rewards_qlearn_eval = evaluate_policy(env, Q_qlearn, n_episodes=10000)\n",
    "rewards_optimal_eval = evaluate_policy(env, Q_optimal, n_episodes=10000)\n",
    "\n",
    "print(f\"SARSA: Success rate = {np.mean(rewards_sarsa_eval)*100:.2f}%\")\n",
    "print(f\"Q-Learning: Success rate = {np.mean(rewards_qlearn_eval)*100:.2f}%\")\n",
    "print(f\"Optimal (DP): Success rate = {np.mean(rewards_optimal_eval)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Learning curves side by side\n",
    "window = 1000\n",
    "sarsa_smooth = np.convolve(stats_sarsa['episode_rewards'], \n",
    "                            np.ones(window)/window, mode='valid')\n",
    "qlearn_smooth = np.convolve(stats_qlearn['episode_rewards'], \n",
    "                             np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(sarsa_smooth, label='SARSA', alpha=0.8)\n",
    "axes[0].plot(qlearn_smooth, label='Q-Learning', alpha=0.8)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward (moving avg)')\n",
    "axes[0].set_title('Learning Curves Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# Success rate comparison\n",
    "methods = ['SARSA', 'Q-Learning', 'Optimal (DP)']\n",
    "success_rates = [\n",
    "    np.mean(rewards_sarsa_eval)*100,\n",
    "    np.mean(rewards_qlearn_eval)*100,\n",
    "    np.mean(rewards_optimal_eval)*100\n",
    "]\n",
    "colors = ['steelblue', 'orange', 'green']\n",
    "\n",
    "bars = axes[1].bar(methods, success_rates, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Success Rate (%)')\n",
    "axes[1].set_title('Final Policy Performance')\n",
    "axes[1].set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{rate:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Q-values with optimal\n",
    "print(\"Q-value Comparison with Optimal\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sarsa_error = np.mean(np.abs(Q_sarsa - Q_optimal))\n",
    "qlearn_error = np.mean(np.abs(Q_qlearn - Q_optimal))\n",
    "\n",
    "print(f\"SARSA Mean Absolute Q-error: {sarsa_error:.4f}\")\n",
    "print(f\"Q-Learning Mean Absolute Q-error: {qlearn_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policies side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "plot_policy(Q_sarsa, title=\"SARSA Policy\", ax=axes[0])\n",
    "plot_policy(Q_qlearn, title=\"Q-Learning Policy\", ax=axes[1])\n",
    "plot_policy(Q_optimal, title=\"Optimal Policy (DP)\", ax=axes[2])\n",
    "\n",
    "plt.suptitle(\"Policy Comparison\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Key Differences: SARSA vs Q-Learning\n",
    "\n",
    "| Aspect | SARSA | Q-Learning |\n",
    "|--------|-------|------------|\n",
    "| **Type** | On-policy | Off-policy |\n",
    "| **Update uses** | $Q(S', A')$ where $A'$ from policy | $\\max_a Q(S', a)$ |\n",
    "| **Learns** | $Q^\\pi$ for behavior policy | $Q^*$ optimal Q |\n",
    "| **Behavior** | More conservative/safe | More aggressive/risky |\n",
    "| **Convergence** | To $Q^\\pi$ | To $Q^*$ |\n",
    "\n",
    "## On-Policy vs Off-Policy\n",
    "\n",
    "**On-policy (SARSA)**:\n",
    "- Learns about the policy it's following\n",
    "- Takes exploration into account\n",
    "- May be safer in dangerous environments\n",
    "\n",
    "**Off-policy (Q-Learning)**:\n",
    "- Learns optimal policy while following any policy\n",
    "- Can use experience from any source (replay buffer)\n",
    "- More sample efficient but may be riskier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Effect of Learning Rate\n",
    "\n",
    "The learning rate α controls how much new information overrides old information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "alphas = [0.01, 0.1, 0.5, 0.9]\n",
    "results_alpha = {}\n",
    "\n",
    "print(\"Testing different learning rates (Q-Learning)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for alpha in alphas:\n",
    "    Q, stats = q_learning(\n",
    "        env, gamma=0.99, alpha=alpha, n_episodes=50000,\n",
    "        epsilon=1.0, epsilon_decay=0.9999, min_epsilon=0.01\n",
    "    )\n",
    "    rewards = evaluate_policy(env, Q, n_episodes=5000)\n",
    "    results_alpha[alpha] = {\n",
    "        'Q': Q,\n",
    "        'stats': stats,\n",
    "        'success_rate': np.mean(rewards) * 100\n",
    "    }\n",
    "    print(f\"α = {alpha}: Success rate = {results_alpha[alpha]['success_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Learning curves\n",
    "window = 500\n",
    "for alpha in alphas:\n",
    "    rewards_smooth = np.convolve(results_alpha[alpha]['stats']['episode_rewards'],\n",
    "                                  np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(rewards_smooth, label=f'α = {alpha}')\n",
    "\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward (moving avg)')\n",
    "axes[0].set_title('Learning Curves for Different α')\n",
    "axes[0].legend()\n",
    "\n",
    "# Final success rates\n",
    "success_rates = [results_alpha[a]['success_rate'] for a in alphas]\n",
    "axes[1].bar([str(a) for a in alphas], success_rates, color='steelblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Learning Rate (α)')\n",
    "axes[1].set_ylabel('Success Rate (%)')\n",
    "axes[1].set_title('Final Performance vs Learning Rate')\n",
    "\n",
    "for i, (a, rate) in enumerate(zip(alphas, success_rates)):\n",
    "    axes[1].text(i, rate + 1, f'{rate:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Summary and Concept Map\n\nIn this notebook, we learned the three most important Temporal Difference algorithms:\n\n```\nTEMPORAL DIFFERENCE LEARNING\n============================\n\nKey Idea: Bootstrap = Update estimates using other estimates\nAdvantage: Learn online (every step), model-free, works for continuing tasks\n────────────────────────────────────────────────────\n\n\nTD(0) PREDICTION\n────────────────\nProblem: Estimate V^π(s) for a given policy π\n\nAlgorithm: Update after each step using immediate reward + next state estimate\nV(S_t) ← V(S_t) + α [R_{t+1} + γV(S_{t+1}) - V(S_t)]\n\nComponents:\n- TD target: R_{t+1} + γV(S_{t+1})\n- TD error (δ): R_{t+1} + γV(S_{t+1}) - V(S_t)\n\nProperties:\n- Online learning (updates every step)\n- Lower variance than MC (uses single reward)\n- Biased (depends on current estimate V)\n- Works for continuing tasks\n\n\nSARSA (On-Policy TD Control)\n─────────────────────────────\nProblem: Find optimal policy using on-policy learning\n\nAlgorithm: Update Q using action A_{t+1} from behavior policy\nQ(S_t, A_t) ← Q(S_t, A_t) + α [R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n\nName: State-Action-Reward-State-Action (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\n\nKey property: **On-policy**\n- Learns about the policy it's following (typically ε-greedy)\n- A_{t+1} comes from the same policy we're evaluating\n- Converges to Q^π for behavior policy π\n- More conservative/safe (accounts for exploration)\n\nUse when: You want the agent to learn a policy that includes exploration\n\n\nQ-LEARNING (Off-Policy TD Control)\n───────────────────────────────────\nProblem: Find optimal policy using off-policy learning\n\nAlgorithm: Update Q using max over actions (not actual next action)\nQ(S_t, A_t) ← Q(S_t, A_t) + α [R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n\nKey property: **Off-policy**\n- Learns optimal policy while following any behavior policy\n- Uses max_a Q(S_{t+1}, a) instead of Q(S_{t+1}, A_{t+1})\n- Converges to Q* (optimal Q) regardless of behavior policy\n- More aggressive/risky (assumes greedy actions)\n\nUse when: You want to learn the optimal policy from any experience\n\n\nON-POLICY VS OFF-POLICY\n────────────────────────\n\n| Aspect | On-Policy (SARSA) | Off-Policy (Q-Learning) |\n|--------|-------------------|-------------------------|\n| **Learns** | Q^π (for behavior policy) | Q* (optimal policy) |\n| **Update uses** | A_{t+1} from policy | max_a Q(s',a) |\n| **Behavior** | Conservative (safe) | Aggressive (risky) |\n| **Exploration** | Must explore to learn | Learns from any policy |\n| **Example** | Learn to drive cautiously while being cautious | Learn optimal driving while practicing cautiously |\n\n\nTD VS MC VS DP\n──────────────\n\n| Property | DP | MC | TD |\n|----------|----|----|----|\n| Model-free | No ✗ | Yes ✓ | Yes ✓ |\n| Bootstraps | Yes ✓ | No ✗ | Yes ✓ |\n| Online (step-by-step) | Yes ✓ | No ✗ | Yes ✓ |\n| Works for continuing tasks | Yes ✓ | No ✗ | Yes ✓ |\n| Unbiased | Yes ✓ | Yes ✓ | No ✗ |\n| Low variance | Yes ✓ | No ✗ | Yes ✓ |\n\n\nBIAS-VARIANCE TRADEOFF\n──────────────────────\n\nMonte Carlo:\n- Uses actual returns G_t\n- Unbiased (correct on average)\n- High variance (different episodes vary a lot)\n\nTemporal Difference:\n- Uses estimated returns R + γV(s')\n- Biased (depends on current estimate)\n- Low variance (single step of randomness)\n\nTrade-off: TD often learns faster in practice despite bias!\n```\n\n## What's Next?\n\nIn the final notebook (**06_algorithm_comparison.ipynb**), we'll:\n- Compare all algorithms (DP, MC, TD) side by side\n- Discuss when to use which method\n- Review hyperparameter tuning strategies\n- Summarize the entire RL tutorial\n\n## Key Takeaways\n\n1. **TD combines best of MC and DP**: Model-free like MC, bootstraps like DP\n2. **Updates every step**: Don't need to wait for episode end (online learning)\n3. **SARSA (on-policy)**: Learns about the policy being followed, more conservative\n4. **Q-Learning (off-policy)**: Learns optimal policy regardless of behavior, more aggressive\n5. **Bias-variance tradeoff**: TD has lower variance but is biased; MC is unbiased but high variance\n6. **Learning rate α**: Controls how much new information overrides old estimates"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n# Your Turn\n\nNow it's time to test your understanding with some hands-on exercises!\n\n## Exercise 1: Implement TD(0) Prediction from Scratch\n\nModify the TD(0) prediction implementation to track and visualize the TD error over time.\n\n**Task**: Complete the code below to implement TD(0) with enhanced tracking:\n\n```python\n# YOUR CODE HERE\n# Implement TD(0) prediction that returns both V and TD errors per state\n\ndef td0_with_tracking(env, policy, gamma, alpha, n_episodes):\n    \"\"\"\n    TD(0) Prediction with detailed tracking.\n    \n    Returns:\n        V: Final value function\n        td_errors_by_state: Dictionary mapping state -> list of TD errors\n    \"\"\"\n    n_states = env.observation_space.n\n    n_actions = env.action_space.n\n    \n    V = np.zeros(n_states)\n    td_errors_by_state = {s: [] for s in range(n_states)}\n    \n    # TODO: Implement the algorithm\n    # Hint: For each step, record the TD error for the visited state\n    \n    pass  # Replace with your implementation\n\n# TODO: Run your implementation and plot TD errors for states 0, 6, 14\n```\n\n<details>\n<summary>Click to see hint</summary>\n\nFor each episode:\n1. Reset environment and get initial state\n2. Until done:\n   - Select action from policy\n   - Take step and observe next_state, reward\n   - Compute TD error: δ = r + γV(s') - V(s)\n   - Update: V(s) += α * δ\n   - Record δ in td_errors_by_state[s]\n\n</details>\n\n<details>\n<summary>Click to see solution</summary>\n\n```python\ndef td0_with_tracking(env, policy, gamma, alpha, n_episodes):\n    n_states = env.observation_space.n\n    n_actions = env.action_space.n\n    \n    V = np.zeros(n_states)\n    td_errors_by_state = {s: [] for s in range(n_states)}\n    \n    for episode in range(n_episodes):\n        state, _ = env.reset()\n        done = False\n        \n        while not done:\n            action = np.random.choice(n_actions, p=policy[state])\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            # Compute TD error\n            td_target = reward + gamma * V[next_state] * (not done)\n            td_error = td_target - V[state]\n            \n            # Update V\n            V[state] += alpha * td_error\n            \n            # Track TD error\n            td_errors_by_state[state].append(td_error)\n            \n            state = next_state\n    \n    return V, td_errors_by_state\n\n# Run and visualize\nuniform_policy = np.ones((n_states, n_actions)) / n_actions\nV, td_errors = td0_with_tracking(env, uniform_policy, gamma=0.99, alpha=0.1, n_episodes=10000)\n\n# Plot TD errors for selected states\nfig, ax = plt.subplots(figsize=(12, 5))\nfor s in [0, 6, 14]:\n    if len(td_errors[s]) > 0:\n        # Moving average for smoothness\n        window = 100\n        errors = td_errors[s]\n        if len(errors) >= window:\n            smooth = np.convolve(errors, np.ones(window)/window, mode='valid')\n            ax.plot(smooth, label=f'State {s}', alpha=0.7)\n\nax.set_xlabel('Visit Count')\nax.set_ylabel('TD Error (smoothed)')\nax.set_title('TD Error Convergence for Selected States')\nax.legend()\nplt.show()\n\nprint(f\"Final V(0) = {V[0]:.4f}\")\n```\n\n</details>\n\n## Exercise 2: Tune SARSA or Q-Learning Hyperparameters\n\n**Task**: The learning rate α and exploration rate ε significantly affect learning. Experiment with different values and find the best combination for fastest convergence.\n\n```python\n# YOUR CODE HERE\n# Test combinations of α and ε for Q-Learning\n\nlearning_rates = [0.01, 0.1, 0.5]\nepsilons = [0.05, 0.1, 0.2]\nresults = {}\n\nfor alpha in learning_rates:\n    for epsilon_start in epsilons:\n        # TODO: Run Q-learning with these hyperparameters\n        # TODO: Evaluate the learned policy\n        # TODO: Record success rate and learning curve\n        pass\n\n# TODO: Create a heatmap showing success rate for each (α, ε) combination\n```\n\n<details>\n<summary>Click to see hint</summary>\n\nFor each (α, ε) combination:\n1. Run q_learning() with n_episodes=50000\n2. Evaluate resulting Q using evaluate_policy()\n3. Store success rate in results[(alpha, epsilon)]\n4. Use plt.imshow() or seaborn.heatmap() to visualize\n\n</details>\n\n<details>\n<summary>Click to see solution</summary>\n\n```python\nlearning_rates = [0.01, 0.1, 0.5]\nepsilons = [0.05, 0.1, 0.2]\nresults = np.zeros((len(learning_rates), len(epsilons)))\n\nprint(\"Testing hyperparameter combinations...\")\nfor i, alpha in enumerate(learning_rates):\n    for j, epsilon_start in enumerate(epsilons):\n        # Run Q-learning\n        Q, stats = q_learning(\n            env, gamma=0.99, alpha=alpha, n_episodes=50000,\n            epsilon=epsilon_start, epsilon_decay=0.9999, min_epsilon=0.01\n        )\n        \n        # Evaluate\n        rewards = evaluate_policy(env, Q, n_episodes=5000)\n        success_rate = np.mean(rewards) * 100\n        results[i, j] = success_rate\n        \n        print(f\"α={alpha:.2f}, ε={epsilon_start:.2f}: {success_rate:.1f}% success\")\n\n# Visualize\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(results, cmap='RdYlGn', vmin=0, vmax=100)\nax.set_xticks(range(len(epsilons)))\nax.set_yticks(range(len(learning_rates)))\nax.set_xticklabels([f'{e}' for e in epsilons])\nax.set_yticklabels([f'{a}' for a in learning_rates])\nax.set_xlabel('Initial Epsilon (ε)')\nax.set_ylabel('Learning Rate (α)')\nax.set_title('Q-Learning Success Rate (%) by Hyperparameters')\n\n# Add text annotations\nfor i in range(len(learning_rates)):\n    for j in range(len(epsilons)):\n        text = ax.text(j, i, f'{results[i, j]:.1f}',\n                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n\nplt.colorbar(im, ax=ax)\nplt.tight_layout()\nplt.show()\n\n# Find best combination\nbest_idx = np.unravel_index(np.argmax(results), results.shape)\nprint(f\"\\nBest combination: α={learning_rates[best_idx[0]]}, ε={epsilons[best_idx[1]]}\")\nprint(f\"Success rate: {results[best_idx]:.1f}%\")\n```\n\n</details>\n\n## Exercise 3: Compare On-Policy (SARSA) vs Off-Policy (Q-Learning) on Cliff Walking\n\n**Task**: The classic \"Cliff Walking\" environment highlights the difference between SARSA and Q-Learning. SARSA learns a safer path while Q-Learning learns the optimal (risky) path.\n\n```python\n# YOUR CODE HERE\n# Create CliffWalking environment and compare SARSA vs Q-Learning\n\n# Step 1: Create the environment\n# env_cliff = gym.make('CliffWalking-v0')\n\n# Step 2: Run SARSA\n# Q_sarsa, stats_sarsa = sarsa(env_cliff, gamma=0.99, alpha=0.1, \n#                               n_episodes=5000, epsilon=0.1)\n\n# Step 3: Run Q-Learning  \n# Q_qlearn, stats_qlearn = q_learning(env_cliff, gamma=0.99, alpha=0.1,\n#                                      n_episodes=5000, epsilon=0.1)\n\n# Step 4: Visualize the learned paths\n# Hint: Extract the greedy policy from each Q and visualize the path from start\n```\n\n<details>\n<summary>Click to see hint</summary>\n\nCliffWalking is a 4×12 grid where:\n- Start: bottom-left (state 36)\n- Goal: bottom-right (state 47)\n- Cliff: bottom row between start and goal (states 37-46)\n\nSARSA (on-policy) will learn a safer path above the cliff because it accounts for ε-greedy exploration.\n\nQ-Learning (off-policy) will learn the optimal path along the cliff edge because it assumes greedy actions.\n\n</details>\n\n<details>\n<summary>Click to see solution</summary>\n\n```python\n# Create environment\nenv_cliff = gym.make('CliffWalking-v0')\nn_states_cliff = env_cliff.observation_space.n\nn_actions_cliff = env_cliff.action_space.n\n\nprint(\"Cliff Walking Environment\")\nprint(\"Grid: 4 rows × 12 columns\")\nprint(\"Start: bottom-left, Goal: bottom-right\")\nprint(\"Cliff: bottom row between start and goal\\n\")\n\n# Run SARSA (on-policy)\nprint(\"Training SARSA (on-policy)...\")\nQ_sarsa_cliff, stats_sarsa_cliff = sarsa(\n    env_cliff, gamma=0.99, alpha=0.5, n_episodes=5000,\n    epsilon=0.1, epsilon_decay=1.0, min_epsilon=0.1\n)\n\n# Run Q-Learning (off-policy)\nprint(\"Training Q-Learning (off-policy)...\")\nQ_qlearn_cliff, stats_qlearn_cliff = q_learning(\n    env_cliff, gamma=0.99, alpha=0.5, n_episodes=5000,\n    epsilon=0.1, epsilon_decay=1.0, min_epsilon=0.1\n)\n\n# Visualize learned policies\ndef visualize_cliff_policy(Q, title):\n    \"\"\"Visualize policy on cliff walking grid.\"\"\"\n    policy_grid = np.zeros((4, 12), dtype=int)\n    for s in range(48):\n        row = s // 12\n        col = s % 12\n        policy_grid[row, col] = np.argmax(Q[s])\n    \n    fig, ax = plt.subplots(figsize=(12, 4))\n    \n    # Draw grid\n    for i in range(4):\n        for j in range(12):\n            state = i * 12 + j\n            \n            # Color cells\n            if state == 36:\n                color = 'lightblue'  # Start\n                text = 'S'\n            elif state == 47:\n                color = 'lightgreen'  # Goal\n                text = 'G'\n            elif 37 <= state <= 46:\n                color = 'red'  # Cliff\n                text = 'C'\n            else:\n                color = 'white'\n                action = policy_grid[i, j]\n                text = ['←', '↓', '→', '↑'][action]\n            \n            rect = plt.Rectangle((j, 3-i), 1, 1, facecolor=color, edgecolor='black')\n            ax.add_patch(rect)\n            ax.text(j+0.5, 3-i+0.5, text, ha='center', va='center', \n                   fontsize=12, fontweight='bold')\n    \n    ax.set_xlim(0, 12)\n    ax.set_ylim(0, 4)\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax.set_title(title, fontsize=14)\n    return ax\n\n# Plot both policies\nfig, axes = plt.subplots(1, 2, figsize=(16, 4))\nplt.sca(axes[0])\nvisualize_cliff_policy(Q_sarsa_cliff, \"SARSA Policy (Safe Path)\")\nplt.sca(axes[1])\nvisualize_cliff_policy(Q_qlearn_cliff, \"Q-Learning Policy (Optimal but Risky)\")\nplt.tight_layout()\nplt.show()\n\n# Compare learning curves\nfig, ax = plt.subplots(figsize=(10, 5))\nwindow = 100\nsarsa_smooth = np.convolve(stats_sarsa_cliff['episode_rewards'], \n                            np.ones(window)/window, mode='valid')\nqlearn_smooth = np.convolve(stats_qlearn_cliff['episode_rewards'],\n                             np.ones(window)/window, mode='valid')\n\nax.plot(sarsa_smooth, label='SARSA (on-policy)', linewidth=2)\nax.plot(qlearn_smooth, label='Q-Learning (off-policy)', linewidth=2)\nax.set_xlabel('Episode')\nax.set_ylabel('Episode Reward (moving avg)')\nax.set_title('Learning Curves: SARSA vs Q-Learning on Cliff Walking')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Observation:\")\nprint(\"- SARSA learns a SAFE path (goes up and around the cliff)\")\nprint(\"- Q-Learning learns the OPTIMAL path (along the cliff edge)\")\nprint(\"- During training with ε-greedy, SARSA accounts for exploration risk\")\nprint(\"- Q-Learning assumes greedy actions, so learns the shortest path\")\n```\n\n</details>\n\n## Conceptual Question: When to Use SARSA vs Q-Learning?\n\n**Question**: You're building an RL agent to control a robot in a warehouse. The robot can fall off loading docks (bad!) but you want it to learn the fastest routes. Should you use SARSA or Q-Learning during training? Why?\n\n<details>\n<summary>Click to see hint</summary>\n\nThink about:\n- Safety during learning (robot can get damaged)\n- The final deployed policy (will it be greedy or ε-greedy?)\n- Whether the agent needs to account for its own exploration\n\n</details>\n\n<details>\n<summary>Click to see answer</summary>\n\n**Use SARSA during training** because:\n\n1. **Safety matters**: SARSA is on-policy, meaning it learns about the ε-greedy policy it's actually following during training. This means it will learn to avoid dangerous areas (loading docks) even when exploring randomly.\n\n2. **Realistic learning**: The robot will account for the fact that it sometimes takes random actions (due to ε-greedy), so the learned policy will be more cautious.\n\n3. **Deployment consideration**: If you deploy with ε-greedy (small ε for safety), SARSA's learned policy is appropriate. If you deploy with pure greedy policy, you might want Q-Learning.\n\n**However, Q-Learning could be better if**:\n- You can train in simulation (no real damage from falls)\n- You plan to deploy a fully greedy policy (ε=0)\n- You want the theoretically optimal solution\n\n**Best approach in practice**:\n- Train with SARSA for safe exploration\n- As training progresses, decay ε toward 0\n- Or train with Q-Learning in simulation, then transfer to real robot with high ε initially\n\n**Real-world consideration**: Many modern systems use off-policy methods (like Q-Learning) but with experience replay buffers and safety constraints, getting benefits of both approaches.\n\n</details>"
  },
  {
   "cell_type": "code",
   "source": "print(\"Congratulations! You've completed Part 5 of the RL Tutorial!\")\nprint(\"\\nKey takeaways:\")\nprint(\"- TD methods update after every step using bootstrapping\")\nprint(\"- SARSA is on-policy: learns about the policy it follows\")\nprint(\"- Q-Learning is off-policy: learns optimal policy directly\")\nprint(\"- Both are model-free and work for continuing tasks\")\nprint(\"- Learning rate α controls the speed/stability trade-off\")\nprint(\"\\nNext: 06_algorithm_comparison.ipynb\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}