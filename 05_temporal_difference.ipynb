{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Temporal Difference Learning\n",
    "\n",
    "In this notebook, we'll learn **Temporal Difference (TD)** methods - the most important class of model-free RL algorithms that combine ideas from Monte Carlo and Dynamic Programming.\n",
    "\n",
    "## What You'll Learn\n",
    "- TD(0) prediction\n",
    "- The bias-variance tradeoff (TD vs MC)\n",
    "- SARSA (On-policy TD control)\n",
    "- Q-Learning (Off-policy TD control)\n",
    "- Comparison of SARSA and Q-Learning\n",
    "\n",
    "## Prerequisites\n",
    "- Understanding of MDPs and Bellman equations (Notebooks 01-02)\n",
    "- Monte Carlo methods (Notebook 04)\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and helper variables\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['←', '↓', '→', '↑']\n",
    "\n",
    "print(\"FrozenLake Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"States: {n_states}\")\n",
    "print(f\"Actions: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def plot_value_function(V, title=\"Value Function\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    V_grid = V.reshape(nrow, ncol)\n",
    "    \n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=max(V.max(), 0.01))\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            color = 'white' if V_grid[i, j] < V.max() / 2 else 'black'\n",
    "            ax.text(j, i, f'{cell}\\n{V[state]:.3f}', ha='center', va='center',\n",
    "                   fontsize=9, color=color)\n",
    "    \n",
    "    ax.set_xticks(range(ncol))\n",
    "    ax.set_yticks(range(nrow))\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def plot_policy(Q, title=\"Policy\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            best_action = np.argmax(Q[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "print(\"Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. What is Temporal Difference Learning?\n",
    "\n",
    "**Temporal Difference (TD)** learning combines ideas from:\n",
    "- **Monte Carlo**: Learn from experience (model-free)\n",
    "- **Dynamic Programming**: Bootstrap (update estimates based on other estimates)\n",
    "\n",
    "## Key Insight: Bootstrapping\n",
    "\n",
    "**Monte Carlo** waits until the end of episode to update:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))$$\n",
    "\n",
    "where $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$ (full return)\n",
    "\n",
    "**TD** updates immediately using estimated return:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$$\n",
    "\n",
    "The term $R_{t+1} + \\gamma V(S_{t+1})$ is called the **TD target**.\n",
    "\n",
    "The difference $(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$ is the **TD error** ($\\delta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of TD\n",
    "\n",
    "1. **Online learning**: Update after every step, not just at episode end\n",
    "2. **Works for continuing tasks**: Don't need episodes to terminate\n",
    "3. **Lower variance**: Uses single reward + estimate instead of full return\n",
    "4. **Often faster convergence**: Especially in practice\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "1. **Biased**: Bootstrapping introduces bias from current estimates\n",
    "2. **Depends on initialization**: Bad initial values can slow learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. TD(0) Prediction\n",
    "\n",
    "The simplest TD method: update after each step using immediate reward and next state estimate.\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate\n",
    "- $R_{t+1} + \\gamma V(S_{t+1})$ is the TD target\n",
    "- $R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ is the TD error $\\delta_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td0_prediction(env, policy, gamma, alpha, n_episodes):\n",
    "    \"\"\"\n",
    "    TD(0) Prediction for estimating V^π.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy to evaluate (π[s,a] probabilities)\n",
    "        gamma: Discount factor\n",
    "        alpha: Learning rate\n",
    "        n_episodes: Number of episodes\n",
    "    \n",
    "    Returns:\n",
    "        V: Estimated state value function\n",
    "        V_history: V at intervals for visualization\n",
    "        td_errors: TD errors during training\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    V = np.zeros(n_states)\n",
    "    V_history = [V.copy()]\n",
    "    td_errors = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action according to policy\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # TD update\n",
    "            # V(s) <- V(s) + α * [r + γV(s') - V(s)]\n",
    "            td_target = reward + gamma * V[next_state] * (not done)\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] += alpha * td_error\n",
    "            \n",
    "            td_errors.append(td_error)\n",
    "            state = next_state\n",
    "        \n",
    "        # Save history at intervals\n",
    "        if (episode + 1) % (n_episodes // 10) == 0:\n",
    "            V_history.append(V.copy())\n",
    "    \n",
    "    return V, V_history, td_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TD(0) prediction for random policy\n",
    "uniform_policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "print(\"TD(0) Prediction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "V_td, V_history_td, td_errors = td0_prediction(\n",
    "    env, uniform_policy, gamma=0.99, alpha=0.1, n_episodes=50000\n",
    ")\n",
    "\n",
    "print(f\"\\nEstimated V^π (random policy):\")\n",
    "print(V_td.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TD(0) with true values (from DP)\n",
    "def extract_mdp(env):\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    return P, R\n",
    "\n",
    "def policy_evaluation_dp(P, R, policy, gamma, theta=1e-8):\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                V_new[s] += policy[s, a] * (R[s, a] + gamma * np.sum(P[s, a] * V))\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    return V\n",
    "\n",
    "P, R = extract_mdp(env)\n",
    "V_true = policy_evaluation_dp(P, R, uniform_policy, gamma=0.99)\n",
    "\n",
    "print(\"Comparison: TD(0) vs True Values (DP)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean Absolute Error: {np.mean(np.abs(V_td - V_true)):.4f}\")\n",
    "print(f\"Max Absolute Error: {np.max(np.abs(V_td - V_true)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TD(0) convergence\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot V at different stages\n",
    "episodes_at = [0, 5000, 10000, 25000, 50000]\n",
    "for idx, (ax, ep) in enumerate(zip(axes.flat[:-1], episodes_at)):\n",
    "    hist_idx = min(idx, len(V_history_td)-1)\n",
    "    plot_value_function(V_history_td[hist_idx], title=f\"After {ep} episodes\", ax=ax)\n",
    "\n",
    "# TD error over time\n",
    "ax = axes.flat[-1]\n",
    "window = 1000\n",
    "td_errors_smooth = np.convolve(np.abs(td_errors), np.ones(window)/window, mode='valid')\n",
    "ax.plot(td_errors_smooth)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('|TD Error| (moving avg)')\n",
    "ax.set_title('TD Error Over Time')\n",
    "\n",
    "plt.suptitle(\"TD(0) Prediction Convergence\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. SARSA: On-Policy TD Control\n",
    "\n",
    "**SARSA** (State-Action-Reward-State-Action) is an on-policy TD control algorithm.\n",
    "\n",
    "## The SARSA Update\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\n",
    "\n",
    "The name comes from the quintuple: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "\n",
    "## On-Policy\n",
    "\n",
    "SARSA is **on-policy**: it learns about the policy it's following.\n",
    "- Uses $A_{t+1}$ which is selected by the same policy\n",
    "- The Q-values converge to $Q^\\pi$ for the behavior policy $\\pi$\n",
    "- Typically uses ε-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, gamma, alpha, n_episodes, epsilon=0.1, \n",
    "          epsilon_decay=1.0, min_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    SARSA: On-policy TD Control.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        gamma: Discount factor\n",
    "        alpha: Learning rate\n",
    "        n_episodes: Number of episodes\n",
    "        epsilon: Exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        min_epsilon: Minimum epsilon\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned Q-values\n",
    "        stats: Training statistics\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilons = []\n",
    "    \n",
    "    def epsilon_greedy_action(state, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(n_actions)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = epsilon_greedy_action(state, epsilon)\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Choose next action (for SARSA update)\n",
    "            next_action = epsilon_greedy_action(next_state, epsilon)\n",
    "            \n",
    "            # SARSA update\n",
    "            # Q(s,a) <- Q(s,a) + α * [r + γ*Q(s',a') - Q(s,a)]\n",
    "            td_target = reward + gamma * Q[next_state, next_action] * (not done)\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    stats = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'epsilons': epsilons\n",
    "    }\n",
    "    \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SARSA\n",
    "print(\"SARSA Training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "Q_sarsa, stats_sarsa = sarsa(\n",
    "    env, gamma=0.99, alpha=0.1, n_episodes=100000,\n",
    "    epsilon=1.0, epsilon_decay=0.99995, min_epsilon=0.01\n",
    ")\n",
    "sarsa_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {sarsa_time:.2f} seconds\")\n",
    "print(f\"Final epsilon: {stats_sarsa['epsilons'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SARSA training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Learning curve\n",
    "window = 1000\n",
    "rewards_smooth = np.convolve(stats_sarsa['episode_rewards'], \n",
    "                              np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(rewards_smooth)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward (moving avg)')\n",
    "axes[0, 0].set_title(f'SARSA Learning Curve (window={window})')\n",
    "\n",
    "# Epsilon decay\n",
    "axes[0, 1].plot(stats_sarsa['epsilons'])\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].set_title('Exploration Rate Decay')\n",
    "\n",
    "# Value function\n",
    "V_sarsa = np.max(Q_sarsa, axis=1)\n",
    "plot_value_function(V_sarsa, title=\"Learned V = max Q(s,a)\", ax=axes[1, 0])\n",
    "\n",
    "# Policy\n",
    "plot_policy(Q_sarsa, title=\"Learned Policy\", ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle(\"SARSA Results (100,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Q-Learning: Off-Policy TD Control\n",
    "\n",
    "**Q-Learning** is an off-policy TD control algorithm - the most famous RL algorithm!\n",
    "\n",
    "## The Q-Learning Update\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
    "\n",
    "## Off-Policy\n",
    "\n",
    "Q-Learning is **off-policy**: it learns about the optimal policy while following a different (exploratory) policy.\n",
    "\n",
    "Key difference from SARSA:\n",
    "- **SARSA**: Uses $Q(S_{t+1}, A_{t+1})$ where $A_{t+1}$ comes from the behavior policy\n",
    "- **Q-Learning**: Uses $\\max_a Q(S_{t+1}, a)$ - the value of the best action\n",
    "\n",
    "This means Q-Learning directly learns $Q^*$ regardless of the policy being followed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, alpha, n_episodes, epsilon=0.1,\n",
    "               epsilon_decay=1.0, min_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Q-Learning: Off-policy TD Control.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        gamma: Discount factor\n",
    "        alpha: Learning rate\n",
    "        n_episodes: Number of episodes\n",
    "        epsilon: Exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        min_epsilon: Minimum epsilon\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned Q-values\n",
    "        stats: Training statistics\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilons = []\n",
    "    \n",
    "    def epsilon_greedy_action(state, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(n_actions)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action using ε-greedy\n",
    "            action = epsilon_greedy_action(state, epsilon)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-Learning update\n",
    "            # Q(s,a) <- Q(s,a) + α * [r + γ*max_a' Q(s',a') - Q(s,a)]\n",
    "            td_target = reward + gamma * np.max(Q[next_state]) * (not done)\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    stats = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'epsilons': epsilons\n",
    "    }\n",
    "    \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Q-Learning\n",
    "print(\"Q-Learning Training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "Q_qlearn, stats_qlearn = q_learning(\n",
    "    env, gamma=0.99, alpha=0.1, n_episodes=100000,\n",
    "    epsilon=1.0, epsilon_decay=0.99995, min_epsilon=0.01\n",
    ")\n",
    "qlearn_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {qlearn_time:.2f} seconds\")\n",
    "print(f\"Final epsilon: {stats_qlearn['epsilons'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Q-Learning training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Learning curve\n",
    "window = 1000\n",
    "rewards_smooth = np.convolve(stats_qlearn['episode_rewards'], \n",
    "                              np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(rewards_smooth)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward (moving avg)')\n",
    "axes[0, 0].set_title(f'Q-Learning Curve (window={window})')\n",
    "\n",
    "# Epsilon decay\n",
    "axes[0, 1].plot(stats_qlearn['epsilons'])\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].set_title('Exploration Rate Decay')\n",
    "\n",
    "# Value function\n",
    "V_qlearn = np.max(Q_qlearn, axis=1)\n",
    "plot_value_function(V_qlearn, title=\"Learned V = max Q(s,a)\", ax=axes[1, 0])\n",
    "\n",
    "# Policy\n",
    "plot_policy(Q_qlearn, title=\"Learned Policy\", ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle(\"Q-Learning Results (100,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. SARSA vs Q-Learning Comparison\n",
    "\n",
    "Let's compare the two algorithms in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both policies\n",
    "def evaluate_policy(env, Q, n_episodes=10000):\n",
    "    \"\"\"Evaluate a greedy policy derived from Q.\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return np.array(rewards)\n",
    "\n",
    "# Get optimal Q from DP for comparison\n",
    "def value_iteration(P, R, gamma, theta=1e-8):\n",
    "    n_states, n_actions = R.shape\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = np.max([R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)])\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    return Q\n",
    "\n",
    "Q_optimal = value_iteration(P, R, gamma=0.99)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Policy Evaluation Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rewards_sarsa_eval = evaluate_policy(env, Q_sarsa, n_episodes=10000)\n",
    "rewards_qlearn_eval = evaluate_policy(env, Q_qlearn, n_episodes=10000)\n",
    "rewards_optimal_eval = evaluate_policy(env, Q_optimal, n_episodes=10000)\n",
    "\n",
    "print(f\"SARSA: Success rate = {np.mean(rewards_sarsa_eval)*100:.2f}%\")\n",
    "print(f\"Q-Learning: Success rate = {np.mean(rewards_qlearn_eval)*100:.2f}%\")\n",
    "print(f\"Optimal (DP): Success rate = {np.mean(rewards_optimal_eval)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Learning curves side by side\n",
    "window = 1000\n",
    "sarsa_smooth = np.convolve(stats_sarsa['episode_rewards'], \n",
    "                            np.ones(window)/window, mode='valid')\n",
    "qlearn_smooth = np.convolve(stats_qlearn['episode_rewards'], \n",
    "                             np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(sarsa_smooth, label='SARSA', alpha=0.8)\n",
    "axes[0].plot(qlearn_smooth, label='Q-Learning', alpha=0.8)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward (moving avg)')\n",
    "axes[0].set_title('Learning Curves Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# Success rate comparison\n",
    "methods = ['SARSA', 'Q-Learning', 'Optimal (DP)']\n",
    "success_rates = [\n",
    "    np.mean(rewards_sarsa_eval)*100,\n",
    "    np.mean(rewards_qlearn_eval)*100,\n",
    "    np.mean(rewards_optimal_eval)*100\n",
    "]\n",
    "colors = ['steelblue', 'orange', 'green']\n",
    "\n",
    "bars = axes[1].bar(methods, success_rates, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Success Rate (%)')\n",
    "axes[1].set_title('Final Policy Performance')\n",
    "axes[1].set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{rate:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Q-values with optimal\n",
    "print(\"Q-value Comparison with Optimal\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sarsa_error = np.mean(np.abs(Q_sarsa - Q_optimal))\n",
    "qlearn_error = np.mean(np.abs(Q_qlearn - Q_optimal))\n",
    "\n",
    "print(f\"SARSA Mean Absolute Q-error: {sarsa_error:.4f}\")\n",
    "print(f\"Q-Learning Mean Absolute Q-error: {qlearn_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policies side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "plot_policy(Q_sarsa, title=\"SARSA Policy\", ax=axes[0])\n",
    "plot_policy(Q_qlearn, title=\"Q-Learning Policy\", ax=axes[1])\n",
    "plot_policy(Q_optimal, title=\"Optimal Policy (DP)\", ax=axes[2])\n",
    "\n",
    "plt.suptitle(\"Policy Comparison\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Key Differences: SARSA vs Q-Learning\n",
    "\n",
    "| Aspect | SARSA | Q-Learning |\n",
    "|--------|-------|------------|\n",
    "| **Type** | On-policy | Off-policy |\n",
    "| **Update uses** | $Q(S', A')$ where $A'$ from policy | $\\max_a Q(S', a)$ |\n",
    "| **Learns** | $Q^\\pi$ for behavior policy | $Q^*$ optimal Q |\n",
    "| **Behavior** | More conservative/safe | More aggressive/risky |\n",
    "| **Convergence** | To $Q^\\pi$ | To $Q^*$ |\n",
    "\n",
    "## On-Policy vs Off-Policy\n",
    "\n",
    "**On-policy (SARSA)**:\n",
    "- Learns about the policy it's following\n",
    "- Takes exploration into account\n",
    "- May be safer in dangerous environments\n",
    "\n",
    "**Off-policy (Q-Learning)**:\n",
    "- Learns optimal policy while following any policy\n",
    "- Can use experience from any source (replay buffer)\n",
    "- More sample efficient but may be riskier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Effect of Learning Rate\n",
    "\n",
    "The learning rate α controls how much new information overrides old information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "alphas = [0.01, 0.1, 0.5, 0.9]\n",
    "results_alpha = {}\n",
    "\n",
    "print(\"Testing different learning rates (Q-Learning)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for alpha in alphas:\n",
    "    Q, stats = q_learning(\n",
    "        env, gamma=0.99, alpha=alpha, n_episodes=50000,\n",
    "        epsilon=1.0, epsilon_decay=0.9999, min_epsilon=0.01\n",
    "    )\n",
    "    rewards = evaluate_policy(env, Q, n_episodes=5000)\n",
    "    results_alpha[alpha] = {\n",
    "        'Q': Q,\n",
    "        'stats': stats,\n",
    "        'success_rate': np.mean(rewards) * 100\n",
    "    }\n",
    "    print(f\"α = {alpha}: Success rate = {results_alpha[alpha]['success_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Learning curves\n",
    "window = 500\n",
    "for alpha in alphas:\n",
    "    rewards_smooth = np.convolve(results_alpha[alpha]['stats']['episode_rewards'],\n",
    "                                  np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(rewards_smooth, label=f'α = {alpha}')\n",
    "\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward (moving avg)')\n",
    "axes[0].set_title('Learning Curves for Different α')\n",
    "axes[0].legend()\n",
    "\n",
    "# Final success rates\n",
    "success_rates = [results_alpha[a]['success_rate'] for a in alphas]\n",
    "axes[1].bar([str(a) for a in alphas], success_rates, color='steelblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Learning Rate (α)')\n",
    "axes[1].set_ylabel('Success Rate (%)')\n",
    "axes[1].set_title('Final Performance vs Learning Rate')\n",
    "\n",
    "for i, (a, rate) in enumerate(zip(alphas, success_rates)):\n",
    "    axes[1].text(i, rate + 1, f'{rate:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## TD Methods Overview\n",
    "\n",
    "| Method | Type | Update Rule | Learns |\n",
    "|--------|------|-------------|--------|\n",
    "| **TD(0)** | Prediction | $V(s) \\leftarrow V(s) + \\alpha[r + \\gamma V(s') - V(s)]$ | $V^\\pi$ |\n",
    "| **SARSA** | On-policy Control | $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$ | $Q^\\pi$ |\n",
    "| **Q-Learning** | Off-policy Control | $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$ | $Q^*$ |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **TD combines MC and DP**: Model-free like MC, bootstraps like DP\n",
    "2. **Updates every step**: Don't need to wait for episode end\n",
    "3. **SARSA (on-policy)**: Learns about the policy being followed, more conservative\n",
    "4. **Q-Learning (off-policy)**: Learns optimal policy regardless of behavior, more aggressive\n",
    "5. **Trade-offs**: TD has lower variance but is biased; MC is unbiased but high variance\n",
    "\n",
    "## TD vs MC vs DP\n",
    "\n",
    "| Property | DP | MC | TD |\n",
    "|----------|----|----|----|\n",
    "| Model-free | No | Yes | Yes |\n",
    "| Bootstraps | Yes | No | Yes |\n",
    "| Online (step-by-step) | Yes | No | Yes |\n",
    "| Works for continuing tasks | Yes | No | Yes |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the final notebook (**06_algorithm_comparison.ipynb**), we'll:\n",
    "- Compare all algorithms side by side\n",
    "- Discuss when to use which method\n",
    "- Summarize the entire tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congratulations! You've completed Part 5 of the RL Tutorial!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- TD methods update after every step using bootstrapping\")\n",
    "print(\"- SARSA is on-policy: learns about the policy it follows\")\n",
    "print(\"- Q-Learning is off-policy: learns optimal policy directly\")\n",
    "print(\"- Both are model-free and work for continuing tasks\")\n",
    "print(\"- Learning rate α controls the speed/stability trade-off\")\n",
    "print(\"\\nNext: 06_algorithm_comparison.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
