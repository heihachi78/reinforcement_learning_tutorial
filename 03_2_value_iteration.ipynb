{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Part 3.2: Value Iteration and DP Comparison\n",
    "\n",
    "In this notebook, we'll learn **Value Iteration** - an alternative Dynamic Programming method that applies the Bellman optimality equation directly. We'll also compare it with Policy Iteration and evaluate optimal policies empirically.\n",
    "\n",
    "## Recap from Part 3.1\n",
    "- **Policy Evaluation**: Given policy π, compute V^π using iterative Bellman expectation\n",
    "- **Policy Improvement**: Given V^π, find better policy by acting greedily\n",
    "- **Policy Iteration**: Alternate evaluation and improvement until convergence\n",
    "- Policy Iteration does **full policy evaluation** before each improvement step\n",
    "\n",
    "## What This Notebook Covers\n",
    "- Value Iteration (finding $V^*$ directly)\n",
    "- Comparison: Policy Iteration vs Value Iteration\n",
    "- Empirical evaluation of optimal policies\n",
    "- Effect of discount factor on optimal policy\n",
    "\n",
    "## What This Notebook Does NOT Cover\n",
    "\n",
    "| Topic | Why Not Here | How It Differs From What We Cover |\n",
    "|-------|--------------|-----------------------------------|\n",
    "| **Model-free methods** | This notebook focuses on model-based RL where we know the MDP dynamics (transition probabilities and rewards). Model-free methods come in the next notebooks. | We assume perfect knowledge of P and R, allowing us to compute exact value functions. Model-free methods like Monte Carlo and TD learning work without this knowledge — they learn from experience instead of computation. |\n",
    "| **Deep reinforcement learning** | Deep RL uses neural networks to approximate value functions for high-dimensional state spaces. We start with tabular methods where we can store exact values for each state. | In this notebook, we maintain tables V[s] and Q[s,a] for all states/actions (feasible for FrozenLake's 16 states). Deep RL uses neural networks f(s;θ) to handle millions of states like images — a fundamentally different approach. |\n",
    "| **Continuous action spaces** | We focus on discrete actions (LEFT, DOWN, RIGHT, UP). Continuous actions require different optimization techniques like policy gradients. | Our algorithms iterate over a finite action space. Continuous control (e.g., robot joint angles, steering angles) requires gradient-based optimization or discretization strategies — topics for advanced RL. |\n",
    "| **Asynchronous dynamic programming** | Standard DP updates all states in sweeps. Asynchronous DP updates states in any order, which can be more efficient but adds complexity. | We use synchronous updates: update all states in each iteration. Asynchronous DP (prioritized sweeping, real-time DP) updates states strategically, which is more efficient but requires understanding the basics first. |\n",
    "\n",
    "## Preview: Value Iteration\n",
    "\n",
    "While Policy Iteration alternates between full evaluation and improvement, Value Iteration takes a different approach:\n",
    "\n",
    "- **Value Iteration**: Apply the Bellman **optimality** equation directly as an update rule\n",
    "- Combines evaluation and improvement into a **single step**\n",
    "- More iterations, but each iteration is **much cheaper**\n",
    "\n",
    "Both methods are **guaranteed to find the optimal policy**!\n",
    "\n",
    "## How to Read This Notebook\n",
    "1. **Value Iteration algorithm**: Learn the mathematical foundations and implementation\n",
    "2. **Comparison**: Understand when to use Policy Iteration vs Value Iteration\n",
    "3. **Empirical evaluation**: Test learned policies on the actual environment\n",
    "4. **Discount factor effects**: See how γ shapes optimal behavior\n",
    "\n",
    "Let's begin!\n",
    "\n",
    "## Prerequisites\n",
    "- Understanding of MDPs and Bellman equations (Notebook 02)\n",
    "- Policy Iteration concepts (Notebook 03_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import time\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment and extract MDP components\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['←', '↓', '→', '↑']\n",
    "\n",
    "# Extract transition and reward matrices\n",
    "def extract_mdp(env):\n",
    "    \"\"\"Extract P[s,a,s'] and R[s,a] from environment.\"\"\"\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    \n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    \n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    \n",
    "    return P, R\n",
    "\n",
    "P, R = extract_mdp(env)\n",
    "\n",
    "print(\"FrozenLake MDP Loaded\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"States: {n_states}\")\n",
    "print(f\"Actions: {n_actions}\")\n",
    "print(f\"Transition matrix shape: {P.shape}\")\n",
    "print(f\"Reward matrix shape: {R.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def plot_value_function(V, title=\"Value Function\", ax=None):\n",
    "    \"\"\"Plot value function as a heatmap.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    V_grid = V.reshape(nrow, ncol)\n",
    "    \n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=max(V.max(), 0.01))\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            color = 'white' if V_grid[i, j] < V.max() / 2 else 'black'\n",
    "            ax.text(j, i, f'{cell}\\n{V[state]:.3f}', ha='center', va='center',\n",
    "                   fontsize=9, color=color)\n",
    "    \n",
    "    ax.set_xticks(range(ncol))\n",
    "    ax.set_yticks(range(nrow))\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def plot_policy(policy, title=\"Policy\", ax=None):\n",
    "    \"\"\"Plot policy showing best action in each state.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Get best action (handle both deterministic and stochastic policies)\n",
    "            if len(policy.shape) == 1:\n",
    "                best_action = int(policy[state])\n",
    "            else:\n",
    "                best_action = np.argmax(policy[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "print(\"Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration functions (from 03_1) needed for comparison\n",
    "\n",
    "def policy_evaluation(P, R, policy, gamma, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Iterative Policy Evaluation.\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    V = np.zeros(n_states)\n",
    "    history = [V.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                expected_next_V = np.sum(P[s, a] * V)\n",
    "                V_new[s] += policy[s, a] * (R[s, a] + gamma * expected_next_V)\n",
    "        \n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V, history\n",
    "\n",
    "def policy_improvement(P, R, V, gamma):\n",
    "    \"\"\"\n",
    "    Compute greedy policy with respect to value function V.\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    \n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        best_action = np.argmax(Q[s])\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "def policy_iteration(P, R, gamma, theta=1e-8, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm.\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "    \n",
    "    policy_history = [policy.copy()]\n",
    "    V_history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V, _ = policy_evaluation(P, R, policy, gamma, theta=theta)\n",
    "        V_history.append(V.copy())\n",
    "        \n",
    "        new_policy, Q = policy_improvement(P, R, V, gamma)\n",
    "        \n",
    "        if np.array_equal(new_policy, policy):\n",
    "            print(f\"Policy Iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "        policy_history.append(policy.copy())\n",
    "    \n",
    "    return policy, V, policy_history, V_history\n",
    "\n",
    "print(\"Policy Iteration functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Value Iteration\n",
    "\n",
    "**Value Iteration** combines policy evaluation and improvement into a single update:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\left[ R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V_k(s') \\right]$$\n",
    "\n",
    "This is applying the **Bellman Optimality Equation** as an update rule.\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "- Policy Iteration does full policy evaluation (many iterations) before each improvement\n",
    "- Value Iteration does only **one sweep** of evaluation, then immediately improves\n",
    "- Value Iteration is like \"truncated\" policy iteration with k=1\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "```\n",
    "1. Initialize V arbitrarily\n",
    "2. Repeat:\n",
    "   For each state s:\n",
    "     V(s) = max_a [R(s,a) + γ * Σ P(s'|s,a) * V(s')]\n",
    "   Until V converges\n",
    "3. Extract policy: π(s) = argmax_a [R(s,a) + γ * Σ P(s'|s,a) * V(s')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, R, gamma, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm.\n",
    "    \n",
    "    Args:\n",
    "        P: Transition matrix\n",
    "        R: Reward matrix\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy\n",
    "        history: Value function at each iteration\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    # Initialize V arbitrarily\n",
    "    V = np.zeros(n_states)\n",
    "    history = [V.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        for s in range(n_states):\n",
    "            # Compute Q-values for all actions\n",
    "            Q_s = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                Q_s[a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "            \n",
    "            # Take maximum\n",
    "            V_new[s] = np.max(Q_s)\n",
    "        \n",
    "        # Check convergence\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value Iteration converged in {iteration + 1} iterations (delta={delta:.2e})\")\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        Q_s = np.zeros(n_actions)\n",
    "        for a in range(n_actions):\n",
    "            Q_s[a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "        policy[s, np.argmax(Q_s)] = 1.0\n",
    "    \n",
    "    return V, policy, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Value Iteration\n",
    "print(\"Running Value Iteration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "V_star_vi, optimal_policy_vi, vi_history = value_iteration(P, R, gamma=0.99)\n",
    "vi_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime taken: {vi_time:.4f} seconds\")\n",
    "print(f\"\\nOptimal Value Function V*:\")\n",
    "print(V_star_vi.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Value Iteration convergence\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "iterations_to_show = [0, 1, 5, 10, 20, 50, 100, len(vi_history)-1]\n",
    "iterations_to_show = [min(i, len(vi_history)-1) for i in iterations_to_show]\n",
    "\n",
    "for idx, (ax, it) in enumerate(zip(axes.flat, iterations_to_show)):\n",
    "    V_it = vi_history[it]\n",
    "    plot_value_function(V_it, title=f\"Iteration {it}\", ax=ax)\n",
    "\n",
    "plt.suptitle(\"Value Iteration Convergence (γ=0.99)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Value Iteration convergence\n",
    "vi_deltas = [np.max(np.abs(vi_history[i+1] - vi_history[i])) \n",
    "             for i in range(len(vi_history)-1)]\n",
    "ax.semilogy(vi_deltas, label='Value Iteration', linewidth=2)\n",
    "\n",
    "ax.axhline(y=1e-8, color='r', linestyle='--', label='Threshold')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max |V_new - V_old| (log scale)')\n",
    "ax.set_title('Value Iteration Convergence')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Comparison: Policy Iteration vs Value Iteration\n",
    "\n",
    "Let's compare the two methods in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Policy Iteration for comparison\n",
    "print(\"Running Policy Iteration for comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_policy_pi, V_star_pi, policy_history, V_history = policy_iteration(P, R, gamma=0.99)\n",
    "pi_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime taken: {pi_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final results\n",
    "print(\"Comparison of Policy Iteration vs Value Iteration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if they found the same solution\n",
    "V_diff = np.max(np.abs(V_star_pi - V_star_vi))\n",
    "policy_same = np.array_equal(np.argmax(optimal_policy_pi, axis=1), \n",
    "                              np.argmax(optimal_policy_vi, axis=1))\n",
    "\n",
    "print(f\"\\nMax difference in V*: {V_diff:.2e}\")\n",
    "print(f\"Same optimal policy: {policy_same}\")\n",
    "print(f\"\\nPolicy Iteration time: {pi_time:.4f}s\")\n",
    "print(f\"Value Iteration time: {vi_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both results side by side\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "plot_value_function(V_star_pi, title=\"V* (Policy Iteration)\", ax=axes[0, 0])\n",
    "plot_value_function(V_star_vi, title=\"V* (Value Iteration)\", ax=axes[0, 1])\n",
    "plot_policy(optimal_policy_pi, title=\"π* (Policy Iteration)\", ax=axes[1, 0])\n",
    "plot_policy(optimal_policy_vi, title=\"π* (Value Iteration)\", ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with different discount factors\n",
    "gammas = [0.5, 0.9, 0.95, 0.99, 0.999]\n",
    "results = []\n",
    "\n",
    "print(\"Benchmarking with Different Discount Factors\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'γ':>8} {'PI iters':>12} {'PI time':>12} {'VI iters':>12} {'VI time':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for gamma in gammas:\n",
    "    # Policy Iteration\n",
    "    start = time.time()\n",
    "    _, V_pi, policy_hist, _ = policy_iteration(P, R, gamma, theta=1e-8)\n",
    "    pi_time_g = time.time() - start\n",
    "    pi_iters = len(policy_hist)\n",
    "    \n",
    "    # Value Iteration\n",
    "    start = time.time()\n",
    "    V_vi, _, vi_hist = value_iteration(P, R, gamma, theta=1e-8)\n",
    "    vi_time_g = time.time() - start\n",
    "    vi_iters = len(vi_hist) - 1\n",
    "    \n",
    "    print(f\"{gamma:>8.3f} {pi_iters:>12} {pi_time_g:>12.4f}s {vi_iters:>12} {vi_time_g:>12.4f}s\")\n",
    "    results.append({'gamma': gamma, 'pi_iters': pi_iters, 'pi_time': pi_time_g,\n",
    "                    'vi_iters': vi_iters, 'vi_time': vi_time_g})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot iterations\n",
    "gammas_plot = [r['gamma'] for r in results]\n",
    "pi_iters_plot = [r['pi_iters'] for r in results]\n",
    "vi_iters_plot = [r['vi_iters'] for r in results]\n",
    "\n",
    "x = np.arange(len(gammas_plot))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, pi_iters_plot, width, label='Policy Iteration', color='steelblue')\n",
    "axes[0].bar(x + width/2, vi_iters_plot, width, label='Value Iteration', color='orange')\n",
    "axes[0].set_xlabel('Discount Factor γ')\n",
    "axes[0].set_ylabel('Iterations')\n",
    "axes[0].set_title('Iterations to Convergence')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([f'{g}' for g in gammas_plot])\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot time\n",
    "pi_times_plot = [r['pi_time'] for r in results]\n",
    "vi_times_plot = [r['vi_time'] for r in results]\n",
    "\n",
    "axes[1].bar(x - width/2, pi_times_plot, width, label='Policy Iteration', color='steelblue')\n",
    "axes[1].bar(x + width/2, vi_times_plot, width, label='Value Iteration', color='orange')\n",
    "axes[1].set_xlabel('Discount Factor γ')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_title('Computation Time')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'{g}' for g in gammas_plot])\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Evaluating the Optimal Policy\n",
    "\n",
    "Let's test our learned optimal policy on the actual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_empirically(env, policy, n_episodes=10000):\n",
    "    \"\"\"Evaluate a policy by running episodes.\"\"\"\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from policy\n",
    "            if len(policy.shape) == 1:\n",
    "                action = int(policy[obs])\n",
    "            else:\n",
    "                action = np.random.choice(n_actions, p=policy[obs])\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        steps_list.append(steps)\n",
    "    \n",
    "    return np.array(rewards), np.array(steps_list)\n",
    "\n",
    "# Evaluate different policies\n",
    "env_eval = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "n_eval_episodes = 10000\n",
    "\n",
    "print(f\"Evaluating Policies ({n_eval_episodes} episodes each)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Random policy\n",
    "uniform_policy = np.ones((n_states, n_actions)) / n_actions\n",
    "rewards_random, steps_random = evaluate_policy_empirically(env_eval, uniform_policy, n_eval_episodes)\n",
    "print(f\"Random Policy: Success rate = {np.mean(rewards_random)*100:.2f}%, Avg steps = {np.mean(steps_random):.1f}\")\n",
    "\n",
    "# Optimal policy from Policy Iteration\n",
    "rewards_pi, steps_pi = evaluate_policy_empirically(env_eval, optimal_policy_pi, n_eval_episodes)\n",
    "print(f\"Policy Iteration: Success rate = {np.mean(rewards_pi)*100:.2f}%, Avg steps = {np.mean(steps_pi):.1f}\")\n",
    "\n",
    "# Optimal policy from Value Iteration\n",
    "rewards_vi, steps_vi = evaluate_policy_empirically(env_eval, optimal_policy_vi, n_eval_episodes)\n",
    "print(f\"Value Iteration: Success rate = {np.mean(rewards_vi)*100:.2f}%, Avg steps = {np.mean(steps_vi):.1f}\")\n",
    "\n",
    "env_eval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Success rates\n",
    "policies_names = ['Random', 'Policy Iteration', 'Value Iteration']\n",
    "success_rates = [np.mean(rewards_random)*100, np.mean(rewards_pi)*100, np.mean(rewards_vi)*100]\n",
    "colors = ['gray', 'steelblue', 'orange']\n",
    "\n",
    "bars = axes[0].bar(policies_names, success_rates, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Success Rate (%)')\n",
    "axes[0].set_title('Policy Comparison: Success Rate')\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{rate:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Average steps (only for successful episodes)\n",
    "avg_steps = [np.mean(steps_random), np.mean(steps_pi), np.mean(steps_vi)]\n",
    "\n",
    "bars = axes[1].bar(policies_names, avg_steps, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Average Steps')\n",
    "axes[1].set_title('Policy Comparison: Average Episode Length')\n",
    "\n",
    "for bar, steps in zip(bars, avg_steps):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{steps:.1f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Effect of Discount Factor on Optimal Policy\n",
    "\n",
    "Let's see how different discount factors affect the learned optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal policies for different discount factors\n",
    "gammas_to_compare = [0.1, 0.5, 0.9, 0.99]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for idx, gamma in enumerate(gammas_to_compare):\n",
    "    V, policy, _ = value_iteration(P, R, gamma)\n",
    "    \n",
    "    plot_value_function(V, title=f\"V* (γ={gamma})\", ax=axes[0, idx])\n",
    "    plot_policy(policy, title=f\"π* (γ={gamma})\", ax=axes[1, idx])\n",
    "\n",
    "plt.suptitle(\"Effect of Discount Factor on Optimal Policy\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Lower γ: Agent is 'shortsighted', values states closer to goal more\")\n",
    "print(\"- Higher γ: Agent is 'farsighted', considers long-term consequences\")\n",
    "print(\"- The optimal policy may change based on how much we value future rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Summary and Concept Map\n",
    "\n",
    "In this notebook, we learned Value Iteration and compared it with Policy Iteration:\n",
    "\n",
    "```\n",
    "DYNAMIC PROGRAMMING for MDPs\n",
    "=============================\n",
    "\n",
    "Requirements: Complete knowledge of MDP (S, A, P, R, γ)\n",
    "Goal: Find optimal policy π*\n",
    "────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "POLICY ITERATION (from 03_1)\n",
    "────────────────────────────\n",
    "Algorithm: Alternate two steps\n",
    "1. Policy Evaluation: Compute V^π for current policy π\n",
    "2. Policy Improvement: Update π greedily w.r.t. V^π\n",
    "\n",
    "Convergence: Finite iterations (each improvement strictly better)\n",
    "Efficiency: Fewer iterations but more computation per iteration\n",
    "\n",
    "\n",
    "VALUE ITERATION\n",
    "───────────────\n",
    "Problem: Find optimal value function V*\n",
    "\n",
    "Algorithm: Iterative application of Bellman optimality equation\n",
    "V_{k+1}(s) = max_a [R^a_s + γ Σ P^a_{ss'} V_k(s')]\n",
    "\n",
    "Then extract policy:\n",
    "π*(s) = argmax_a [R^a_s + γ Σ P^a_{ss'} V*(s')]\n",
    "\n",
    "Convergence: V_k → V* as k → ∞\n",
    "\n",
    "Efficiency: Simpler per iteration, often faster than PI\n",
    "\n",
    "\n",
    "COMPARISON\n",
    "──────────\n",
    "| Algorithm | Iterations | Cost/Iteration | When to Use |\n",
    "|-----------|------------|----------------|-------------|\n",
    "| Policy Iteration | Fewer (~5-10) | High (full evaluation) | Small state spaces |\n",
    "| Value Iteration | More (~50-100) | Low (one sweep) | Large state spaces |\n",
    "\n",
    "Both guarantee finding π* given complete MDP model!\n",
    "```\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "These algorithms are powerful but **require knowing P and R perfectly**. In the real world, we rarely have this luxury!\n",
    "\n",
    "In the next notebook (**04_monte_carlo.ipynb**), we'll learn **model-free methods**:\n",
    "- Learn from experience (episodes) instead of computation\n",
    "- No need to know transition probabilities\n",
    "- Trade exact computation for sampling-based learning\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **DP requires full model knowledge** - we need P and R\n",
    "2. **Policy Iteration**: Alternates evaluation and improvement, fewer iterations but each is expensive\n",
    "3. **Value Iteration**: Applies Bellman optimality directly, more iterations but each is cheap\n",
    "4. **Both converge** to the optimal policy and value function\n",
    "5. **Discount factor** affects what the optimal policy looks like\n",
    "\n",
    "## Limitations of DP\n",
    "\n",
    "- Requires **complete model** of the environment (P, R)\n",
    "- Computationally expensive for large state spaces (curse of dimensionality)\n",
    "- Cannot be used when the environment is unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Your Turn\n",
    "\n",
    "Now it's time to test your understanding with some hands-on exercises!\n",
    "\n",
    "## Exercise 1: Modify Value Iteration Convergence Threshold\n",
    "\n",
    "**Task**: The value iteration implementation uses a convergence threshold θ. Experiment with different values and observe the effect on:\n",
    "1. Number of iterations required\n",
    "2. Final value accuracy\n",
    "3. Computational time\n",
    "\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "# Run value iteration with θ = [0.1, 0.01, 0.001, 0.0001]\n",
    "# Record iterations and V(start_state) for each\n",
    "\n",
    "thresholds = [0.1, 0.01, 0.001, 0.0001]\n",
    "results = []\n",
    "\n",
    "for theta in thresholds:\n",
    "    # TODO: Run value iteration and record results\n",
    "    pass\n",
    "\n",
    "# TODO: Print comparison table\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "thresholds = [0.1, 0.01, 0.001, 0.0001, 1e-8]\n",
    "results = []\n",
    "\n",
    "for theta in thresholds:\n",
    "    V, policy, history = value_iteration(P, R, gamma=0.99, theta=theta)\n",
    "    results.append({\n",
    "        'theta': theta,\n",
    "        'iterations': len(history) - 1,\n",
    "        'V_start': V[0]\n",
    "    })\n",
    "\n",
    "print(\"Convergence Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"θ={r['theta']:8.1e} | Iterations: {r['iterations']:3d} | V(0)={r['V_start']:.6f}\")\n",
    "\n",
    "# Key insight: Smaller θ → more iterations but higher accuracy\n",
    "# Diminishing returns: 0.001 vs 0.0001 doesn't change much\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "## Exercise 2: Conceptual Question - Policy Iteration vs Value Iteration\n",
    "\n",
    "**Question**: In the notebook, we saw that policy iteration converged in fewer iterations than value iteration. However, each policy iteration was more expensive. When would you prefer one over the other?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see hint</summary>\n",
    "\n",
    "Think about:\n",
    "- State space size (16 vs 1000 vs 1000000)\n",
    "- Whether you need the value function or just the policy\n",
    "- Computation vs iteration trade-off\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Policy Iteration is better when:**\n",
    "- Small to medium state spaces (each evaluation is affordable)\n",
    "- You need guaranteed convergence in few steps\n",
    "- The optimal policy is \"close\" to your initial policy\n",
    "\n",
    "**Value Iteration is better when:**\n",
    "- Large state spaces (full evaluation is expensive)\n",
    "- You primarily need V* (policy extraction is cheap afterward)\n",
    "- You can tolerate approximate solutions (stop early with larger θ)\n",
    "\n",
    "**In practice for FrozenLake (16 states):**\n",
    "- Both work well!\n",
    "- Policy iteration: ~5-10 iterations, each expensive\n",
    "- Value iteration: ~50-100 iterations, each cheap\n",
    "- Total computation is similar, value iteration slightly faster\n",
    "\n",
    "**For large state spaces (e.g., chess):**\n",
    "- Value iteration is often preferred\n",
    "- Can use approximate methods (asynchronous DP, prioritized sweeping)\n",
    "- Modern RL mostly uses model-free methods instead!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congratulations! You've completed Part 3.2 of the RL Tutorial!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- Value Iteration: Apply Bellman optimality directly\")\n",
    "print(\"- Both Policy Iteration and Value Iteration find the optimal policy\")\n",
    "print(\"- Trade-off: fewer expensive iterations vs more cheap iterations\")\n",
    "print(\"- Both methods require full knowledge of the MDP model\")\n",
    "print(\"\\nNext: 04_monte_carlo.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
