{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 4: Monte Carlo Methods\n\nIn this notebook, we'll learn **Monte Carlo (MC) methods** - our first **model-free** algorithms that learn from experience without knowing the environment dynamics.\n\n## Recap from Notebook 03\n- **Dynamic Programming** algorithms (Policy Evaluation, Policy Iteration, Value Iteration) find optimal policies by computation\n- **Model-based learning** requires complete knowledge of MDP dynamics (P and R)\n- **Bellman equations** allow us to compute exact value functions through iterative updates\n- **Convergence guarantees**: DP methods are guaranteed to find π* given the full model\n- **Limitation**: In most real-world problems, we don't know P and R perfectly\n\n## What This Notebook Covers\n- Why model-free methods are important\n- Monte Carlo prediction (policy evaluation)\n- First-visit vs Every-visit MC\n- Monte Carlo control (finding optimal policy)\n- Exploring starts and ε-soft policies\n\n## What This Notebook Does NOT Cover\n\n| Topic | Why Not Here | How It Differs From What We Cover |\n|-------|--------------|-----------------------------------|\n| **Bootstrapping methods (TD learning)** | Monte Carlo waits until episode ends to update values. Temporal Difference methods update after each step, which is more efficient. This added complexity comes in the next notebook. | We compute returns G_t by summing rewards to episode end. TD methods estimate returns using V(s') instead of waiting — they \"bootstrap\" from current value estimates. This enables online learning but introduces bias. |\n| **Online learning and continuing tasks** | MC requires complete episodes, making it suitable only for episodic tasks. Continuing tasks (no terminal state) need different approaches. | We wait for episode termination to compute total return. Continuing tasks like stock trading or robot control run indefinitely — they need methods like TD learning that update during the episode. |\n| **Deep reinforcement learning** | We use tables to store exact Q(s,a) values. Deep RL uses neural networks to approximate value functions for high-dimensional spaces. | In this notebook, we maintain Q-tables for all state-action pairs (feasible for FrozenLake). Deep RL uses networks f(s,a;θ) for millions of states — essential for Atari, robotics, but builds on MC foundations. |\n| **Importance sampling variants** | We use on-policy learning where behavior policy equals target policy. Off-policy learning with importance sampling is more complex but more data-efficient. | Our ε-greedy policy both generates experience AND improves. Importance sampling allows learning π* while following exploratory policy μ — more sample-efficient but requires weighted averaging. |\n\n## Preview: Learning from Experience\n\nDynamic Programming was powerful but required knowing the MDP model. Now we'll learn our **first model-free method**:\n\n**Monte Carlo (MC) methods:**\n- Learn value functions from **sampled episodes** (actual experience)\n- No need to know transition probabilities P or reward function R\n- Trade exact computation for sample-based learning\n- Use **averaging** to estimate expected returns\n\nThe key idea: If we run many episodes and average the returns, we'll converge to the true value function!\n\nThis is the beginning of **practical RL** — learning from interaction without needing a model.\n\n## How to Read This Notebook\n1. **Model-free intuition**: Understand how we learn from experience instead of computation\n2. **MC algorithms**: See how first-visit MC prediction and control work step-by-step\n3. **Implementation details**: Run code to watch Q-values and policies improve over episodes\n4. **Convergence behavior**: Observe how sample averaging leads to correct values\n\nLet's begin!\n\n## Prerequisites\n- Understanding of MDPs and value functions (Notebooks 01-02)\n- Dynamic Programming concepts (Notebook 03)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and helper variables\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['←', '↓', '→', '↑']\n",
    "\n",
    "print(\"FrozenLake Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"States: {n_states}\")\n",
    "print(f\"Actions: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def plot_value_function(V, title=\"Value Function\", ax=None):\n",
    "    \"\"\"Plot value function as a heatmap.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    V_grid = V.reshape(nrow, ncol)\n",
    "    \n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=max(V.max(), 0.01))\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            color = 'white' if V_grid[i, j] < V.max() / 2 else 'black'\n",
    "            ax.text(j, i, f'{cell}\\n{V[state]:.3f}', ha='center', va='center',\n",
    "                   fontsize=9, color=color)\n",
    "    \n",
    "    ax.set_xticks(range(ncol))\n",
    "    ax.set_yticks(range(nrow))\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def plot_policy(Q, title=\"Policy\", ax=None):\n",
    "    \"\"\"Plot policy derived from Q-values.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            best_action = np.argmax(Q[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "print(\"Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Why Model-Free Methods?\n",
    "\n",
    "## Limitations of Dynamic Programming\n",
    "\n",
    "Dynamic Programming (Policy Iteration, Value Iteration) requires:\n",
    "- Complete knowledge of transition probabilities $P_{ss'}^a$\n",
    "- Complete knowledge of reward function $R_s^a$\n",
    "\n",
    "In many real-world problems:\n",
    "- The environment model is **unknown**\n",
    "- The model is too **complex** to use directly\n",
    "- Building the model is too **expensive**\n",
    "\n",
    "## Model-Free Learning\n",
    "\n",
    "**Model-free** methods learn directly from experience:\n",
    "- No need to know P or R\n",
    "- Learn from sampled episodes\n",
    "- Can handle unknown environments\n",
    "\n",
    "Monte Carlo methods are one class of model-free algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Monte Carlo Prediction\n",
    "\n",
    "**Problem**: Estimate $V^\\pi(s)$ or $Q^\\pi(s,a)$ for a given policy $\\pi$, without knowing the model.\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "The value function is the **expected return**:\n",
    "\n",
    "$$V^\\pi(s) = E_\\pi[G_t | S_t = s]$$\n",
    "\n",
    "Monte Carlo estimates this expectation by **averaging sample returns**:\n",
    "\n",
    "$$V^\\pi(s) \\approx \\frac{1}{N} \\sum_{i=1}^{N} G_t^{(i)}$$\n",
    "\n",
    "where $G_t^{(i)}$ is the return from state $s$ in the $i$-th episode.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Episodes must **terminate** (finite episodes)\n",
    "- We learn from **complete episodes** (unlike TD methods)\n",
    "- Only updates happen at the **end of episodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-Visit vs Every-Visit MC\n",
    "\n",
    "When a state $s$ is visited multiple times in an episode:\n",
    "\n",
    "**First-Visit MC**: Only use the return from the **first** time $s$ is visited\n",
    "\n",
    "**Every-Visit MC**: Use returns from **every** time $s$ is visited\n",
    "\n",
    "Both converge to $V^\\pi(s)$ as the number of episodes → ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode following the given policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy array π[s,a] with probabilities or π[s] with action\n",
    "    \n",
    "    Returns:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from policy\n",
    "        if len(policy.shape) == 1:\n",
    "            action = int(policy[state])\n",
    "        else:\n",
    "            action = np.random.choice(len(policy[state]), p=policy[state])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate episode generation\n",
    "uniform_policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "print(\"Example Episodes with Random Policy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    episode = generate_episode(env, uniform_policy)\n",
    "    states = [s for s, a, r in episode]\n",
    "    actions = [action_arrows[a] for s, a, r in episode]\n",
    "    rewards = [r for s, a, r in episode]\n",
    "    total_reward = sum(rewards)\n",
    "    \n",
    "    print(f\"\\nEpisode {i+1} (length={len(episode)}, reward={total_reward}):\")\n",
    "    print(f\"  States:  {states}\")\n",
    "    print(f\"  Actions: {actions}\")\n",
    "    print(f\"  Rewards: {rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_first_visit(env, policy, gamma, n_episodes, verbose=False):\n",
    "    \"\"\"\n",
    "    First-Visit Monte Carlo Prediction for estimating V^π.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy to evaluate\n",
    "        gamma: Discount factor\n",
    "        n_episodes: Number of episodes to sample\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        V: Estimated state value function\n",
    "        V_history: Value function at intervals for visualization\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    \n",
    "    # Store returns for each state\n",
    "    returns_sum = np.zeros(n_states)\n",
    "    returns_count = np.zeros(n_states)\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    V_history = [V.copy()]\n",
    "    \n",
    "    for episode_num in range(n_episodes):\n",
    "        # Generate an episode\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Calculate returns for each state visited\n",
    "        states_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        # Go backwards through the episode\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            # First-visit: only count if this is the first occurrence of state in episode\n",
    "            if state not in states_visited:\n",
    "                states_visited.add(state)\n",
    "                returns_sum[state] += G\n",
    "                returns_count[state] += 1\n",
    "        \n",
    "        # Update V\n",
    "        for s in range(n_states):\n",
    "            if returns_count[s] > 0:\n",
    "                V[s] = returns_sum[s] / returns_count[s]\n",
    "        \n",
    "        # Save history at intervals\n",
    "        if (episode_num + 1) % (n_episodes // 10) == 0:\n",
    "            V_history.append(V.copy())\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode_num + 1}/{n_episodes}\")\n",
    "    \n",
    "    return V, V_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MC prediction for random policy\n",
    "print(\"Monte Carlo Prediction (First-Visit)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Evaluating uniform random policy...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "V_mc, V_history = mc_prediction_first_visit(env, uniform_policy, gamma=0.99, \n",
    "                                             n_episodes=50000, verbose=True)\n",
    "mc_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime taken: {mc_time:.2f} seconds\")\n",
    "print(f\"\\nEstimated V^π (random policy):\")\n",
    "print(V_mc.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with DP solution (if we had the model)\n",
    "# First, get the \"true\" values using DP\n",
    "def extract_mdp(env):\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    return P, R\n",
    "\n",
    "def policy_evaluation_dp(P, R, policy, gamma, theta=1e-8):\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                V_new[s] += policy[s, a] * (R[s, a] + gamma * np.sum(P[s, a] * V))\n",
    "        \n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    return V\n",
    "\n",
    "P, R = extract_mdp(env)\n",
    "V_true = policy_evaluation_dp(P, R, uniform_policy, gamma=0.99)\n",
    "\n",
    "print(\"Comparison: MC Prediction vs DP (True Values)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'State':<8} {'MC Estimate':>15} {'True (DP)':>15} {'Error':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for s in range(n_states):\n",
    "    error = abs(V_mc[s] - V_true[s])\n",
    "    print(f\"{s:<8} {V_mc[s]:>15.4f} {V_true[s]:>15.4f} {error:>15.4f}\")\n",
    "\n",
    "print(f\"\\nMean Absolute Error: {np.mean(np.abs(V_mc - V_true)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MC convergence\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot value function evolution\n",
    "episodes_at = [0, 5000, 10000, 25000, 50000]\n",
    "for idx, (ax, ep) in enumerate(zip(axes.flat[:-1], episodes_at)):\n",
    "    hist_idx = min(idx, len(V_history)-1)\n",
    "    plot_value_function(V_history[hist_idx], title=f\"After {ep} episodes\", ax=ax)\n",
    "\n",
    "# Plot comparison with true values\n",
    "ax = axes.flat[-1]\n",
    "x = np.arange(n_states)\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, V_mc, width, label='MC Estimate', color='steelblue')\n",
    "ax.bar(x + width/2, V_true, width, label='True (DP)', color='orange')\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('MC vs True Values')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Monte Carlo Prediction Convergence (50,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Monte Carlo Estimation of Q-values\n",
    "\n",
    "For **control** (finding the optimal policy), we need to estimate **action-values** $Q^\\pi(s,a)$, not just state-values.\n",
    "\n",
    "Why? Because to improve a policy, we need to know how good each action is:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a Q^\\pi(s, a)$$\n",
    "\n",
    "With just $V(s)$, we would need the model to compute:\n",
    "$$Q(s,a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V(s')$$\n",
    "\n",
    "But we're model-free! So we estimate Q directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_Q(env, policy, gamma, n_episodes):\n",
    "    \"\"\"\n",
    "    First-Visit Monte Carlo Prediction for Q-values.\n",
    "    \n",
    "    Returns:\n",
    "        Q: Estimated action-value function Q[s, a]\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Track visited (state, action) pairs\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            # First-visit check\n",
    "            if (state, action) not in sa_visited:\n",
    "                sa_visited.add((state, action))\n",
    "                returns_sum[state, action] += G\n",
    "                returns_count[state, action] += 1\n",
    "        \n",
    "        # Update Q\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                if returns_count[s, a] > 0:\n",
    "                    Q[s, a] = returns_sum[s, a] / returns_count[s, a]\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Estimate Q for random policy\n",
    "print(\"Estimating Q^π for random policy...\")\n",
    "Q_random = mc_prediction_Q(env, uniform_policy, gamma=0.99, n_episodes=50000)\n",
    "\n",
    "print(\"\\nQ-values for state 0 (start):\")\n",
    "for a in range(n_actions):\n",
    "    print(f\"  Q(0, {action_names[a]}) = {Q_random[0, a]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Monte Carlo Control\n",
    "\n",
    "**Goal**: Find the optimal policy $\\pi^*$ using only experience.\n",
    "\n",
    "## Approach: Generalized Policy Iteration\n",
    "\n",
    "Like Policy Iteration, we alternate between:\n",
    "1. **Policy Evaluation**: Estimate $Q^\\pi$ using MC\n",
    "2. **Policy Improvement**: Make policy greedy with respect to Q\n",
    "\n",
    "## The Exploration Problem\n",
    "\n",
    "If our policy is deterministic, we might never visit some (state, action) pairs!\n",
    "\n",
    "**Solution 1: Exploring Starts**\n",
    "- Start each episode from a random (state, action) pair\n",
    "- Ensures all pairs have chance to be visited\n",
    "\n",
    "**Solution 2: ε-soft Policies**\n",
    "- Always have non-zero probability of selecting any action\n",
    "- E.g., ε-greedy: with prob ε choose random, else choose best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, gamma, n_episodes, epsilon=0.1, \n",
    "                               epsilon_decay=0.9999, min_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control with ε-greedy exploration.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        gamma: Discount factor\n",
    "        n_episodes: Number of episodes\n",
    "        epsilon: Initial exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        min_epsilon: Minimum epsilon value\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned action-value function\n",
    "        policy: Learned policy\n",
    "        stats: Training statistics\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize Q arbitrarily and returns tracking\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    # Statistics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for episode_num in range(n_episodes):\n",
    "        # Generate episode using ε-greedy policy derived from Q\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # ε-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Record stats\n",
    "        episode_rewards.append(sum(r for _, _, r in episode))\n",
    "        episode_lengths.append(len(episode))\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # Update Q using first-visit MC\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            if (state, action) not in sa_visited:\n",
    "                sa_visited.add((state, action))\n",
    "                returns_sum[state, action] += G\n",
    "                returns_count[state, action] += 1\n",
    "                Q[state, action] = returns_sum[state, action] / returns_count[state, action]\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Extract greedy policy\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        policy[s, np.argmax(Q[s])] = 1.0\n",
    "    \n",
    "    stats = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'epsilons': epsilons\n",
    "    }\n",
    "    \n",
    "    return Q, policy, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MC Control\n",
    "print(\"Monte Carlo Control with ε-greedy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "Q_mc, policy_mc, stats = mc_control_epsilon_greedy(\n",
    "    env, gamma=0.99, n_episodes=100000, \n",
    "    epsilon=1.0, epsilon_decay=0.99995, min_epsilon=0.01\n",
    ")\n",
    "mc_control_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {mc_control_time:.2f} seconds\")\n",
    "print(f\"Final epsilon: {stats['epsilons'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Smooth rewards with moving average\n",
    "window = 1000\n",
    "rewards_smooth = np.convolve(stats['episode_rewards'], \n",
    "                              np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(rewards_smooth)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward (moving avg)')\n",
    "axes[0, 0].set_title(f'Learning Curve (window={window})')\n",
    "\n",
    "# Epsilon decay\n",
    "axes[0, 1].plot(stats['epsilons'])\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].set_title('Exploration Rate Decay')\n",
    "\n",
    "# Final Q-values heatmap\n",
    "V_from_Q = np.max(Q_mc, axis=1)\n",
    "plot_value_function(V_from_Q, title=\"Learned V* = max_a Q(s,a)\", ax=axes[1, 0])\n",
    "\n",
    "# Learned policy\n",
    "plot_policy(Q_mc, title=\"Learned Policy\", ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle(\"Monte Carlo Control Results (100,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the learned policy\n",
    "def evaluate_policy(env, Q, n_episodes=10000):\n",
    "    \"\"\"Evaluate a greedy policy derived from Q.\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return np.array(rewards)\n",
    "\n",
    "# Compare learned policy with optimal (from DP) and random\n",
    "print(\"Policy Evaluation Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# MC learned policy\n",
    "rewards_mc = evaluate_policy(env, Q_mc, n_episodes=10000)\n",
    "print(f\"MC Policy: Success rate = {np.mean(rewards_mc)*100:.2f}%\")\n",
    "\n",
    "# Random policy\n",
    "Q_random = np.random.rand(n_states, n_actions)\n",
    "rewards_random = evaluate_policy(env, Q_random, n_episodes=10000)\n",
    "print(f\"Random Policy: Success rate = {np.mean(rewards_random)*100:.2f}%\")\n",
    "\n",
    "# Optimal policy (from Value Iteration)\n",
    "def value_iteration(P, R, gamma, theta=1e-8):\n",
    "    n_states, n_actions = R.shape\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = np.max([R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)])\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    # Extract Q\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    return Q\n",
    "\n",
    "Q_optimal = value_iteration(P, R, gamma=0.99)\n",
    "rewards_optimal = evaluate_policy(env, Q_optimal, n_episodes=10000)\n",
    "print(f\"Optimal Policy (DP): Success rate = {np.mean(rewards_optimal)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "policies = ['Random', 'MC Learned', 'Optimal (DP)']\n",
    "success_rates = [np.mean(rewards_random)*100, np.mean(rewards_mc)*100, np.mean(rewards_optimal)*100]\n",
    "colors = ['gray', 'steelblue', 'green']\n",
    "\n",
    "# Bar chart\n",
    "bars = axes[0].bar(policies, success_rates, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Success Rate (%)')\n",
    "axes[0].set_title('Policy Performance Comparison')\n",
    "axes[0].set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{rate:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# MC learned policy\n",
    "plot_policy(Q_mc, title=\"MC Learned Policy\", ax=axes[1])\n",
    "\n",
    "# Optimal policy\n",
    "plot_policy(Q_optimal, title=\"Optimal Policy (DP)\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Exploring Starts MC Control\n",
    "\n",
    "An alternative to ε-greedy is **Exploring Starts**: every (state, action) pair has non-zero probability of being the starting point of an episode.\n",
    "\n",
    "This guarantees all pairs are visited infinitely often, allowing convergence to the true optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_exploring_starts(env, gamma, n_episodes):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control with Exploring Starts.\n",
    "    \n",
    "    Note: This requires ability to set the initial state,\n",
    "    which isn't always possible in real environments.\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        # Exploring start: random initial state and action\n",
    "        # Note: FrozenLake doesn't support setting initial state,\n",
    "        # so we'll simulate by using the normal start but random first action\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        # Random first action (exploring start)\n",
    "        first_action = np.random.randint(n_actions)\n",
    "        \n",
    "        # Generate episode\n",
    "        episode = []\n",
    "        action = first_action\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if not done:\n",
    "                # Follow greedy policy after first action\n",
    "                action = np.argmax(Q[state])\n",
    "        \n",
    "        episode_rewards.append(sum(r for _, _, r in episode))\n",
    "        \n",
    "        # Update Q\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            \n",
    "            if (s, a) not in sa_visited:\n",
    "                sa_visited.add((s, a))\n",
    "                returns_sum[s, a] += G\n",
    "                returns_count[s, a] += 1\n",
    "                Q[s, a] = returns_sum[s, a] / returns_count[s, a]\n",
    "    \n",
    "    return Q, episode_rewards\n",
    "\n",
    "# Run Exploring Starts MC\n",
    "print(\"Monte Carlo Control with Exploring Starts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "Q_es, rewards_es = mc_control_exploring_starts(env, gamma=0.99, n_episodes=100000)\n",
    "\n",
    "# Evaluate\n",
    "rewards_es_eval = evaluate_policy(env, Q_es, n_episodes=10000)\n",
    "print(f\"Exploring Starts Policy: Success rate = {np.mean(rewards_es_eval)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. MC vs DP: Key Differences\n",
    "\n",
    "| Aspect | Dynamic Programming | Monte Carlo |\n",
    "|--------|--------------------|--------------|\n",
    "| **Model** | Requires complete model | Model-free |\n",
    "| **Updates** | Bootstraps (uses estimated values) | Uses actual returns |\n",
    "| **Episodes** | Can update mid-episode | Must wait for episode end |\n",
    "| **Variance** | Lower (uses expectations) | Higher (uses samples) |\n",
    "| **Bias** | Biased by initialization | Unbiased (true returns) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare convergence characteristics\n",
    "print(\"MC Characteristics Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run MC with different numbers of episodes\n",
    "episode_counts = [1000, 5000, 10000, 50000, 100000]\n",
    "mc_errors = []\n",
    "\n",
    "for n_ep in episode_counts:\n",
    "    Q_temp, _, _ = mc_control_epsilon_greedy(env, gamma=0.99, n_episodes=n_ep,\n",
    "                                             epsilon=1.0, epsilon_decay=0.9999, min_epsilon=0.01)\n",
    "    # Compare with optimal Q\n",
    "    error = np.mean(np.abs(Q_temp - Q_optimal))\n",
    "    mc_errors.append(error)\n",
    "    print(f\"{n_ep:>7} episodes: Mean |Q - Q*| = {error:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_counts, mc_errors, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Episodes')\n",
    "plt.ylabel('Mean Absolute Error in Q')\n",
    "plt.title('MC Control Convergence to Optimal Q')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Summary and Concept Map\n\nIn this notebook, we learned Monte Carlo methods - our first model-free algorithms that learn from experience!\n\n```\nMONTE CARLO METHODS for RL\n===========================\n\nKey Innovation: Learn from EXPERIENCE, not computation\nNo need for MDP model (P, R) - completely MODEL-FREE!\n────────────────────────────────────────────────────\n\n\nCORE IDEA: SAMPLE-BASED ESTIMATION\n──────────────────────────────────\nValue = Expected Return\nV^π(s) = E_π[G_t | S_t = s]\n\nDP: Uses model to compute expectation\n    V(s) = Σ π(a|s) [R^a_s + γ Σ P^a_{ss'} V(s')]\n    \nMC: Samples episodes and averages actual returns\n    V^π(s) ≈ (1/N) Σ_{i=1}^N G_t^(i)\n    \n\nMC PREDICTION (Policy Evaluation)\n─────────────────────────────────\nProblem: Estimate V^π(s) or Q^π(s,a) without model\n\nAlgorithm: \n1. Generate episode following π: S_0, A_0, R_1, ..., S_T\n2. For each state s visited:\n   - Calculate return: G_t = R_{t+1} + γR_{t+2} + ... + γ^{T-t-1}R_T\n   - Update value estimate: V(s) = average of all G_t from s\n\nTwo variants:\n- First-visit MC: Count only first visit to s in episode\n- Every-visit MC: Count all visits to s in episode\n\nKey property: UNBIASED estimates (uses actual returns)\nLimitation: HIGH VARIANCE (different episodes give different G_t)\n\n\nMC CONTROL (Finding Optimal Policy)\n───────────────────────────────────\nProblem: Find π* without knowing P or R\n\nChallenge: Need Q(s,a), not just V(s), for improvement\nWhy? Can't compute max_a [R + γΣP V] without model!\n\nSolution: Estimate Q^π(s,a) directly from episodes\n\nAlgorithm (Generalized Policy Iteration):\n1. Initialize Q(s,a) arbitrarily, π ε-greedy\n2. Loop:\n   a. Generate episode with current policy\n   b. For each (s,a) pair visited:\n      - Calculate return G_t\n      - Update Q(s,a) = average of returns from (s,a)\n   c. Improve policy: make ε-greedy w.r.t. Q\n3. Return learned Q and π\n\n\nEXPLORATION STRATEGIES\n─────────────────────\nProblem: Deterministic policy never explores alternatives!\n\nSolution 1: Exploring Starts\n- Every (s,a) pair has nonzero probability of starting episode\n- Guarantees all pairs visited infinitely often\n- Limitation: Not feasible in many real environments\n\nSolution 2: ε-Greedy Policies\n- With probability ε: choose random action (explore)\n- With probability 1-ε: choose best action (exploit)\n- Ensures all actions tried infinitely often\n- ε decay: Start high (explore), decay to low (exploit)\n\nTrade-off: Exploration vs Exploitation\n- Too much exploration: Suboptimal actions slow learning\n- Too little exploration: Miss better actions, get stuck\n\n\nMC vs DP: KEY DIFFERENCES\n─────────────────────────\n| Aspect | Dynamic Programming | Monte Carlo |\n|--------|-------------------|-------------|\n| **Model** | Requires P and R | Model-free |\n| **Updates** | Bootstraps (uses V estimates) | Uses actual returns |\n| **Timing** | Can update any time | Wait for episode end |\n| **Episodes** | Not needed | MUST have episodes |\n| **Variance** | Low (expectations) | High (sampling) |\n| **Bias** | Biased by initialization | Unbiased (true G_t) |\n| **Computation** | Iterate over all states | Sample episodes |\n| **Task type** | Any MDP | EPISODIC only |\n\nWhen to use MC:\n✓ Don't know environment model\n✓ Have episodic tasks (games, simulations)\n✓ Can tolerate high variance\n✓ Want unbiased estimates\n\nWhen to use DP:\n✓ Know complete MDP model\n✓ Need low variance\n✓ Have continuing tasks\n✓ Want computational efficiency\n\n\nCONVERGENCE PROPERTIES\n──────────────────────\nFirst-visit MC converges to V^π(s) as episodes → ∞\n- Law of Large Numbers: sample mean → true mean\n- Guaranteed for episodic tasks with bounded returns\n\nMC Control converges to Q* if:\n1. All (s,a) pairs visited infinitely often (exploration)\n2. Policy improves greedily w.r.t. Q\n3. Episodes are finite and have bounded returns\n\nPractical convergence:\n- Needs MANY episodes (10,000s to 100,000s)\n- ε-greedy with decay balances exploration/exploitation\n- Variance decreases as 1/√N (slow!)\n\n\nIMPLEMENTATION INSIGHTS\n───────────────────────\nData structure: Track returns_sum[s,a] and returns_count[s,a]\nUpdate: Q[s,a] = returns_sum[s,a] / returns_count[s,a]\n\nFirst-visit check: Use set() to track visited (s,a) pairs\nBackward iteration: Process episode in reverse for efficiency\n  for t in reversed(range(len(episode))):\n      G = γ*G + reward[t]\n\nε-decay schedule: \n- Start: ε = 1.0 (full exploration)\n- Decay: ε = max(min_ε, ε * decay_rate)\n- End: ε ≈ 0.01 (mostly exploitation)\n\nEpisode generation:\n- Follow current policy (ε-greedy)\n- Store (state, action, reward) tuples\n- Continue until terminal state reached\n```\n\n## Key Takeaways\n\n1. **Model-free learning**: MC learns from sampled experience, no P or R needed\n2. **Episode-based**: Must wait for complete episodes before updating values\n3. **Unbiased but high variance**: Uses actual returns (unbiased) but varies across episodes\n4. **Exploration is crucial**: ε-greedy or exploring starts ensure all actions are tried\n5. **Q-learning not V-learning**: Need Q(s,a) for model-free control\n6. **Episodic tasks only**: MC requires episodes to terminate (have finite length)\n\n## Limitations of Monte Carlo\n\n- **Episodic tasks only**: Cannot handle continuing tasks (infinite horizon)\n- **Episode completion required**: Must wait until end to update (slow for long episodes)\n- **High variance**: Sample returns can vary significantly between episodes\n- **Sample inefficiency**: Each return updates only one (s,a) pair per episode\n- **Slow convergence**: Needs many episodes to average out variance\n\n## What's Next?\n\nMonte Carlo was a huge step forward (model-free!), but has limitations:\n- Only works for episodic tasks\n- Must wait for episode to end\n- High variance slows learning\n\nIn the next notebook (**05_temporal_difference.ipynb**), we'll learn **Temporal Difference (TD) methods**:\n- Update **every step** (not just at episode end)\n- Work for **continuing** tasks (non-episodic)\n- **Lower variance** than MC (bootstrap from estimates)\n- Include famous algorithms: **SARSA** and **Q-Learning**\n\nTD methods combine the best of both worlds:\n- Model-free like MC\n- Bootstrap like DP\n- Online learning capability"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n# Your Turn\n\nNow it's time to test your understanding with some hands-on exercises!\n\n## Exercise 1: Implement First-Visit MC Prediction for Custom Policy Evaluation\n\nIn the notebook, we evaluated a uniform random policy. Now implement first-visit MC prediction to evaluate a different policy.\n\n**Task**: Complete the code below to evaluate a \"cautious\" policy that prefers RIGHT and DOWN over LEFT and UP.\n\n```python\n# YOUR CODE HERE\n# Define a cautious policy: P(RIGHT) = 0.4, P(DOWN) = 0.4, P(LEFT) = 0.1, P(UP) = 0.1\n\ndef create_cautious_policy(n_states, n_actions):\n    \"\"\"\n    Create a cautious policy that prefers RIGHT and DOWN.\n    \n    Returns: policy array of shape (n_states, n_actions)\n    \"\"\"\n    policy = np.zeros((n_states, n_actions))\n    \n    # TODO: Fill in the policy probabilities\n    # Hint: For all states, set probabilities as [0.1, 0.4, 0.4, 0.1] for [LEFT, DOWN, RIGHT, UP]\n    \n    pass  # Replace with your implementation\n\n# TODO: Run MC prediction with this cautious policy for 50,000 episodes\n# Hint: Use mc_prediction_first_visit function from the notebook\n\n# TODO: Compare V(start_state) for cautious vs random policy\n# Which one has higher value at the start state?\n```\n\n<details>\n<summary>Click to see hint</summary>\n\nThe cautious policy should:\n- Have same probabilities for all states: [0.1, 0.4, 0.4, 0.1]\n- Use the `mc_prediction_first_visit` function with gamma=0.99\n- Compare the value at state 0 (start state)\n\nRemember that RIGHT generally moves toward the goal in FrozenLake, so this policy should perform better than random!\n\n</details>\n\n<details>\n<summary>Click to see solution</summary>\n\n```python\ndef create_cautious_policy(n_states, n_actions):\n    \"\"\"Create cautious policy that prefers RIGHT and DOWN.\"\"\"\n    policy = np.zeros((n_states, n_actions))\n    # For all states: [LEFT, DOWN, RIGHT, UP] = [0.1, 0.4, 0.4, 0.1]\n    for s in range(n_states):\n        policy[s] = [0.1, 0.4, 0.4, 0.1]\n    return policy\n\n# Create and evaluate cautious policy\ncautious_policy = create_cautious_policy(n_states, n_actions)\nV_cautious, _ = mc_prediction_first_visit(env, cautious_policy, gamma=0.99, \n                                          n_episodes=50000, verbose=True)\n\nprint(\"\\nComparison:\")\nprint(f\"Cautious policy V(0): {V_cautious[0]:.4f}\")\nprint(f\"Random policy V(0):   {V_random[0]:.4f}\")\nprint(f\"Improvement: {V_cautious[0] - V_random[0]:.4f}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nplot_value_function(V_cautious, title=\"Cautious Policy V^π\", ax=axes[0])\nplot_value_function(V_random, title=\"Random Policy V^π\", ax=axes[1])\nplt.tight_layout()\nplt.show()\n\n# The cautious policy should have higher value at the start!\n# Why? Because it moves toward the goal (right and down) more often.\n```\n\n**Key Insight**: Even without knowing the optimal policy, choosing actions that seem reasonable (moving toward goal) improves performance. This is why policy design matters in RL!\n\n</details>\n\n## Exercise 2: Experiment with ε-Decay Schedules\n\nThe MC control algorithm uses ε-greedy exploration with decay. The decay schedule significantly affects learning performance.\n\n**Task**: Experiment with different ε-decay schedules and compare their convergence.\n\n```python\n# YOUR CODE HERE\n# Test three decay schedules:\n# 1. Fast decay: epsilon_decay = 0.999\n# 2. Medium decay: epsilon_decay = 0.9999 (what we used)\n# 3. Slow decay: epsilon_decay = 0.99999\n\ndecay_rates = [0.999, 0.9999, 0.99999]\nresults_comparison = []\n\nfor decay in decay_rates:\n    # TODO: Run mc_control_epsilon_greedy with different decay rates\n    # Use n_episodes = 50000 for fair comparison\n    # Record: final success rate, final epsilon, learning curve\n    pass\n\n# TODO: Plot comparison of learning curves\n# TODO: Which decay schedule learns fastest? Which achieves best final performance?\n```\n\n<details>\n<summary>Click to see hint</summary>\n\nFor each decay rate:\n1. Run `mc_control_epsilon_greedy` with the decay rate\n2. Evaluate the learned policy using `evaluate_policy`\n3. Plot the smoothed rewards over episodes\n4. Compare final epsilon values\n\nThink about the trade-off:\n- Fast decay: Quick convergence but might miss optimal actions\n- Slow decay: Better exploration but slower convergence\n\n</details>\n\n<details>\n<summary>Click to see solution</summary>\n\n```python\ndecay_rates = [0.999, 0.9999, 0.99999]\ndecay_names = ['Fast (0.999)', 'Medium (0.9999)', 'Slow (0.99999)']\nresults_comparison = []\n\nprint(\"Testing Different ε-Decay Schedules\")\nprint(\"=\" * 70)\n\nfor decay, name in zip(decay_rates, decay_names):\n    print(f\"\\nTesting {name}...\")\n    Q, policy, stats = mc_control_epsilon_greedy(\n        env, gamma=0.99, n_episodes=50000,\n        epsilon=1.0, epsilon_decay=decay, min_epsilon=0.01\n    )\n    \n    # Evaluate final policy\n    rewards_eval = evaluate_policy(env, Q, n_episodes=10000)\n    success_rate = np.mean(rewards_eval) * 100\n    final_epsilon = stats['epsilons'][-1]\n    \n    results_comparison.append({\n        'name': name,\n        'decay': decay,\n        'Q': Q,\n        'stats': stats,\n        'success_rate': success_rate,\n        'final_epsilon': final_epsilon\n    })\n    \n    print(f\"  Final success rate: {success_rate:.2f}%\")\n    print(f\"  Final epsilon: {final_epsilon:.4f}\")\n\n# Plot learning curves\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Learning curves\nwindow = 1000\nfor r in results_comparison:\n    rewards_smooth = np.convolve(r['stats']['episode_rewards'], \n                                  np.ones(window)/window, mode='valid')\n    axes[0].plot(rewards_smooth, label=r['name'], linewidth=2)\n\naxes[0].set_xlabel('Episode')\naxes[0].set_ylabel('Reward (moving avg)')\naxes[0].set_title('Learning Curves Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Epsilon decay curves\nfor r in results_comparison:\n    axes[1].plot(r['stats']['epsilons'], label=r['name'], linewidth=2)\n\naxes[1].set_xlabel('Episode')\naxes[1].set_ylabel('Epsilon')\naxes[1].set_title('Exploration Rate Decay')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Final performance comparison\nnames = [r['name'] for r in results_comparison]\nsuccess = [r['success_rate'] for r in results_comparison]\ncolors = ['red', 'green', 'blue']\n\nbars = axes[2].bar(range(len(names)), success, color=colors, edgecolor='black')\naxes[2].set_xticks(range(len(names)))\naxes[2].set_xticklabels(names, rotation=15, ha='right')\naxes[2].set_ylabel('Success Rate (%)')\naxes[2].set_title('Final Performance')\naxes[2].set_ylim(0, 100)\n\nfor bar, rate in zip(bars, success):\n    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                f'{rate:.1f}%', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANALYSIS:\")\nprint(\"- Fast decay (0.999): Converges quickly but may get stuck in local optimum\")\nprint(\"- Medium decay (0.9999): Good balance of exploration and exploitation\")\nprint(\"- Slow decay (0.99999): More exploration, slower convergence but better final policy\")\nprint(\"\\nKey insight: ε-decay schedule is crucial for balancing exploration vs exploitation!\")\n```\n\n**Key Takeaways**:\n- **Fast decay**: Learns quickly early on but stops exploring too soon\n- **Slow decay**: Explores more thoroughly, often finds better final policy\n- **Trade-off**: Convergence speed vs final performance quality\n- In practice: Start with medium decay, tune based on task complexity\n\n</details>\n\n## Exercise 3: Conceptual Question - MC Variance vs DP Bias\n\n**Question**: We learned that Monte Carlo methods have high variance but are unbiased, while Dynamic Programming has low variance but can be biased by initialization. Explain:\n\n1. Why does MC have high variance? Give a concrete example from FrozenLake.\n2. What does \"unbiased\" mean in the context of MC estimation?\n3. In what scenarios would you prefer MC's high-variance, unbiased estimates over DP's low-variance, potentially biased estimates?\n\n<details>\n<summary>Click to see hint</summary>\n\nThink about:\n1. **Variance**: Why do different episodes from the same state give different returns?\n2. **Bias**: What does it mean that MC converges to the \"true\" value?\n3. **Trade-offs**: When is it better to have noisy but correct estimates vs smooth but potentially wrong estimates?\n\nConsider:\n- Episode 1: Start → Hole (return = 0)\n- Episode 2: Start → Goal (return = 1.0)\n- Both from same state with same policy!\n\n</details>\n\n<details>\n<summary>Click to see detailed answer</summary>\n\n### 1. Why MC has high variance\n\n**High variance** means the estimated value can vary significantly between episodes, even from the same state.\n\n**FrozenLake example**:\n```\nState 0 (start) with random policy:\n\nEpisode 1: S0 → S4 → H (Hole at step 2)\n  Return: G = 0\n  \nEpisode 2: S0 → S1 → S2 → S6 → S7 → G (Goal at step 5)\n  Return: G = γ^4 * 1.0 = 0.99^4 ≈ 0.96\n  \nEpisode 3: S0 → S0 → S4 → S5 → H (Hole at step 4)\n  Return: G = 0\n\nV(S0) after 3 episodes: (0 + 0.96 + 0) / 3 = 0.32\n```\n\nThe same state gives returns ranging from 0 to 0.96! This is **high variance**.\n\n**Why this happens**:\n- Environment stochasticity (slippery ice)\n- Policy stochasticity (random actions)\n- Different episode trajectories from same state\n- Variance decreases only as 1/√N (slow!)\n\n### 2. What \"unbiased\" means for MC\n\n**Unbiased** means the *expected value* of the estimate equals the true value:\n```\nE[V_MC(s)] = V^π(s)\n```\n\nEven though individual returns vary (high variance), their **average converges to the true value**.\n\n**Why MC is unbiased**:\n- Uses actual returns G_t (not estimates)\n- Law of Large Numbers: sample mean → population mean\n- No bootstrapping (doesn't depend on other estimates)\n\n**DP can be biased**:\n```\nV_{k+1}(s) = Σ π(a|s) [R + γ Σ P * V_k(s')]\n                                    ^^^^\n                              Uses estimated values!\n```\nIf V_k is wrong (from initialization), it propagates. Only asymptotically unbiased.\n\n### 3. When to prefer MC over DP\n\n**Prefer MC when**:\n\n✓ **No model available**\n- Most important reason! DP requires knowing P and R\n- Example: Learning to play a video game (don't know transition dynamics)\n\n✓ **Subset of states matters**\n- MC only updates visited states (efficient for large state spaces)\n- DP must update all states each iteration\n- Example: Chess - only care about good positions, not all 10^47 states\n\n✓ **Want unbiased estimates**\n- Critical for statistical guarantees\n- Example: A/B testing policies - need true value differences\n\n✓ **Can simulate episodes cheaply**\n- If simulation is fast, variance is manageable\n- Example: Game emulators, physics simulators\n\n**Prefer DP when**:\n\n✓ **Have complete model** (P, R known)\n- Can leverage model for efficiency\n- Example: Gridworld puzzles, tic-tac-toe\n\n✓ **Need low variance**\n- Noisy estimates are problematic\n- Example: Safety-critical applications (robotics)\n\n✓ **Continuing tasks**\n- MC requires episodes to end\n- Example: Stock trading (market never \"terminates\")\n\n✓ **Computational efficiency**\n- DP can converge in fewer iterations\n- Example: Small state spaces where sweeps are cheap\n\n### Practical Reality\n\nIn modern RL:\n- **Model-free is standard** (we rarely have perfect models)\n- **Temporal Difference** methods (next notebook) get best of both worlds:\n  - Model-free like MC\n  - Low variance like DP (through bootstrapping)\n  - Can handle continuing tasks\n- **Deep RL** uses TD methods (DQN, A3C, PPO) for complex tasks\n\n**The big insight**: MC taught us we can learn from experience alone. This opened the door to applying RL to real-world problems without needing perfect models!\n\n</details>\n\n---\n\n## Challenge Exercise (Advanced)\n\n**Implement Every-Visit MC Prediction** and compare it with First-Visit MC.\n\n**Question**: How does Every-Visit MC differ from First-Visit in terms of:\n1. Number of samples collected per episode\n2. Bias and variance properties\n3. Convergence speed\n\nImplement it and run experiments to verify your hypothesis!\n\n<details>\n<summary>Click to see solution sketch</summary>\n\n```python\ndef mc_prediction_every_visit(env, policy, gamma, n_episodes):\n    \"\"\"Every-Visit Monte Carlo - count ALL visits to each state.\"\"\"\n    n_states = env.observation_space.n\n    returns_sum = np.zeros(n_states)\n    returns_count = np.zeros(n_states)\n    V = np.zeros(n_states)\n    \n    for _ in range(n_episodes):\n        episode = generate_episode(env, policy)\n        G = 0\n        \n        # No visited set - count EVERY occurrence\n        for t in reversed(range(len(episode))):\n            state, action, reward = episode[t]\n            G = gamma * G + reward\n            \n            # Update for every visit (no first-visit check!)\n            returns_sum[state] += G\n            returns_count[state] += 1\n        \n        for s in range(n_states):\n            if returns_count[s] > 0:\n                V[s] = returns_sum[s] / returns_count[s]\n    \n    return V\n\n# Both should converge to same V^π, but every-visit may be faster!\n```\n\n</details>\n\n---\n\nprint(\"Congratulations! You've completed Part 4 of the RL Tutorial!\")\nprint(\"\\nKey takeaways:\")\nprint(\"- Monte Carlo methods learn from complete episodes\")\nprint(\"- They don't need a model - completely model-free\")\nprint(\"- MC Prediction estimates V or Q by averaging returns\")\nprint(\"- MC Control uses ε-greedy or exploring starts for exploration\")\nprint(\"- MC has high variance but no bias from bootstrapping\")\nprint(\"\\nNext: 05_temporal_difference.ipynb\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}