{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Monte Carlo Methods\n",
    "\n",
    "In this notebook, we'll learn **Monte Carlo (MC) methods** - our first **model-free** algorithms that learn from experience without knowing the environment dynamics.\n",
    "\n",
    "## What You'll Learn\n",
    "- Why model-free methods are important\n",
    "- Monte Carlo prediction (policy evaluation)\n",
    "- First-visit vs Every-visit MC\n",
    "- Monte Carlo control (finding optimal policy)\n",
    "- Exploring starts and ε-soft policies\n",
    "\n",
    "## Prerequisites\n",
    "- Understanding of MDPs and value functions (Notebooks 01-02)\n",
    "- Dynamic Programming concepts (Notebook 03)\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and helper variables\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['←', '↓', '→', '↑']\n",
    "\n",
    "print(\"FrozenLake Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"States: {n_states}\")\n",
    "print(f\"Actions: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def plot_value_function(V, title=\"Value Function\", ax=None):\n",
    "    \"\"\"Plot value function as a heatmap.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    V_grid = V.reshape(nrow, ncol)\n",
    "    \n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=max(V.max(), 0.01))\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            color = 'white' if V_grid[i, j] < V.max() / 2 else 'black'\n",
    "            ax.text(j, i, f'{cell}\\n{V[state]:.3f}', ha='center', va='center',\n",
    "                   fontsize=9, color=color)\n",
    "    \n",
    "    ax.set_xticks(range(ncol))\n",
    "    ax.set_yticks(range(nrow))\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def plot_policy(Q, title=\"Policy\", ax=None):\n",
    "    \"\"\"Plot policy derived from Q-values.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            best_action = np.argmax(Q[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "print(\"Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Why Model-Free Methods?\n",
    "\n",
    "## Limitations of Dynamic Programming\n",
    "\n",
    "Dynamic Programming (Policy Iteration, Value Iteration) requires:\n",
    "- Complete knowledge of transition probabilities $P_{ss'}^a$\n",
    "- Complete knowledge of reward function $R_s^a$\n",
    "\n",
    "In many real-world problems:\n",
    "- The environment model is **unknown**\n",
    "- The model is too **complex** to use directly\n",
    "- Building the model is too **expensive**\n",
    "\n",
    "## Model-Free Learning\n",
    "\n",
    "**Model-free** methods learn directly from experience:\n",
    "- No need to know P or R\n",
    "- Learn from sampled episodes\n",
    "- Can handle unknown environments\n",
    "\n",
    "Monte Carlo methods are one class of model-free algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Monte Carlo Prediction\n",
    "\n",
    "**Problem**: Estimate $V^\\pi(s)$ or $Q^\\pi(s,a)$ for a given policy $\\pi$, without knowing the model.\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "The value function is the **expected return**:\n",
    "\n",
    "$$V^\\pi(s) = E_\\pi[G_t | S_t = s]$$\n",
    "\n",
    "Monte Carlo estimates this expectation by **averaging sample returns**:\n",
    "\n",
    "$$V^\\pi(s) \\approx \\frac{1}{N} \\sum_{i=1}^{N} G_t^{(i)}$$\n",
    "\n",
    "where $G_t^{(i)}$ is the return from state $s$ in the $i$-th episode.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Episodes must **terminate** (finite episodes)\n",
    "- We learn from **complete episodes** (unlike TD methods)\n",
    "- Only updates happen at the **end of episodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-Visit vs Every-Visit MC\n",
    "\n",
    "When a state $s$ is visited multiple times in an episode:\n",
    "\n",
    "**First-Visit MC**: Only use the return from the **first** time $s$ is visited\n",
    "\n",
    "**Every-Visit MC**: Use returns from **every** time $s$ is visited\n",
    "\n",
    "Both converge to $V^\\pi(s)$ as the number of episodes → ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode following the given policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy array π[s,a] with probabilities or π[s] with action\n",
    "    \n",
    "    Returns:\n",
    "        episode: List of (state, action, reward) tuples\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from policy\n",
    "        if len(policy.shape) == 1:\n",
    "            action = int(policy[state])\n",
    "        else:\n",
    "            action = np.random.choice(len(policy[state]), p=policy[state])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate episode generation\n",
    "uniform_policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "print(\"Example Episodes with Random Policy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    episode = generate_episode(env, uniform_policy)\n",
    "    states = [s for s, a, r in episode]\n",
    "    actions = [action_arrows[a] for s, a, r in episode]\n",
    "    rewards = [r for s, a, r in episode]\n",
    "    total_reward = sum(rewards)\n",
    "    \n",
    "    print(f\"\\nEpisode {i+1} (length={len(episode)}, reward={total_reward}):\")\n",
    "    print(f\"  States:  {states}\")\n",
    "    print(f\"  Actions: {actions}\")\n",
    "    print(f\"  Rewards: {rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_first_visit(env, policy, gamma, n_episodes, verbose=False):\n",
    "    \"\"\"\n",
    "    First-Visit Monte Carlo Prediction for estimating V^π.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy to evaluate\n",
    "        gamma: Discount factor\n",
    "        n_episodes: Number of episodes to sample\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        V: Estimated state value function\n",
    "        V_history: Value function at intervals for visualization\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    \n",
    "    # Store returns for each state\n",
    "    returns_sum = np.zeros(n_states)\n",
    "    returns_count = np.zeros(n_states)\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    V_history = [V.copy()]\n",
    "    \n",
    "    for episode_num in range(n_episodes):\n",
    "        # Generate an episode\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Calculate returns for each state visited\n",
    "        states_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        # Go backwards through the episode\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            # First-visit: only count if this is the first occurrence of state in episode\n",
    "            if state not in states_visited:\n",
    "                states_visited.add(state)\n",
    "                returns_sum[state] += G\n",
    "                returns_count[state] += 1\n",
    "        \n",
    "        # Update V\n",
    "        for s in range(n_states):\n",
    "            if returns_count[s] > 0:\n",
    "                V[s] = returns_sum[s] / returns_count[s]\n",
    "        \n",
    "        # Save history at intervals\n",
    "        if (episode_num + 1) % (n_episodes // 10) == 0:\n",
    "            V_history.append(V.copy())\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode_num + 1}/{n_episodes}\")\n",
    "    \n",
    "    return V, V_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MC prediction for random policy\n",
    "print(\"Monte Carlo Prediction (First-Visit)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Evaluating uniform random policy...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "V_mc, V_history = mc_prediction_first_visit(env, uniform_policy, gamma=0.99, \n",
    "                                             n_episodes=50000, verbose=True)\n",
    "mc_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime taken: {mc_time:.2f} seconds\")\n",
    "print(f\"\\nEstimated V^π (random policy):\")\n",
    "print(V_mc.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with DP solution (if we had the model)\n",
    "# First, get the \"true\" values using DP\n",
    "def extract_mdp(env):\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    return P, R\n",
    "\n",
    "def policy_evaluation_dp(P, R, policy, gamma, theta=1e-8):\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                V_new[s] += policy[s, a] * (R[s, a] + gamma * np.sum(P[s, a] * V))\n",
    "        \n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    return V\n",
    "\n",
    "P, R = extract_mdp(env)\n",
    "V_true = policy_evaluation_dp(P, R, uniform_policy, gamma=0.99)\n",
    "\n",
    "print(\"Comparison: MC Prediction vs DP (True Values)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'State':<8} {'MC Estimate':>15} {'True (DP)':>15} {'Error':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for s in range(n_states):\n",
    "    error = abs(V_mc[s] - V_true[s])\n",
    "    print(f\"{s:<8} {V_mc[s]:>15.4f} {V_true[s]:>15.4f} {error:>15.4f}\")\n",
    "\n",
    "print(f\"\\nMean Absolute Error: {np.mean(np.abs(V_mc - V_true)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MC convergence\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot value function evolution\n",
    "episodes_at = [0, 5000, 10000, 25000, 50000]\n",
    "for idx, (ax, ep) in enumerate(zip(axes.flat[:-1], episodes_at)):\n",
    "    hist_idx = min(idx, len(V_history)-1)\n",
    "    plot_value_function(V_history[hist_idx], title=f\"After {ep} episodes\", ax=ax)\n",
    "\n",
    "# Plot comparison with true values\n",
    "ax = axes.flat[-1]\n",
    "x = np.arange(n_states)\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, V_mc, width, label='MC Estimate', color='steelblue')\n",
    "ax.bar(x + width/2, V_true, width, label='True (DP)', color='orange')\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('MC vs True Values')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Monte Carlo Prediction Convergence (50,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Monte Carlo Estimation of Q-values\n",
    "\n",
    "For **control** (finding the optimal policy), we need to estimate **action-values** $Q^\\pi(s,a)$, not just state-values.\n",
    "\n",
    "Why? Because to improve a policy, we need to know how good each action is:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a Q^\\pi(s, a)$$\n",
    "\n",
    "With just $V(s)$, we would need the model to compute:\n",
    "$$Q(s,a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V(s')$$\n",
    "\n",
    "But we're model-free! So we estimate Q directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_Q(env, policy, gamma, n_episodes):\n",
    "    \"\"\"\n",
    "    First-Visit Monte Carlo Prediction for Q-values.\n",
    "    \n",
    "    Returns:\n",
    "        Q: Estimated action-value function Q[s, a]\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        episode = generate_episode(env, policy)\n",
    "        \n",
    "        # Track visited (state, action) pairs\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            # First-visit check\n",
    "            if (state, action) not in sa_visited:\n",
    "                sa_visited.add((state, action))\n",
    "                returns_sum[state, action] += G\n",
    "                returns_count[state, action] += 1\n",
    "        \n",
    "        # Update Q\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                if returns_count[s, a] > 0:\n",
    "                    Q[s, a] = returns_sum[s, a] / returns_count[s, a]\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Estimate Q for random policy\n",
    "print(\"Estimating Q^π for random policy...\")\n",
    "Q_random = mc_prediction_Q(env, uniform_policy, gamma=0.99, n_episodes=50000)\n",
    "\n",
    "print(\"\\nQ-values for state 0 (start):\")\n",
    "for a in range(n_actions):\n",
    "    print(f\"  Q(0, {action_names[a]}) = {Q_random[0, a]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Monte Carlo Control\n",
    "\n",
    "**Goal**: Find the optimal policy $\\pi^*$ using only experience.\n",
    "\n",
    "## Approach: Generalized Policy Iteration\n",
    "\n",
    "Like Policy Iteration, we alternate between:\n",
    "1. **Policy Evaluation**: Estimate $Q^\\pi$ using MC\n",
    "2. **Policy Improvement**: Make policy greedy with respect to Q\n",
    "\n",
    "## The Exploration Problem\n",
    "\n",
    "If our policy is deterministic, we might never visit some (state, action) pairs!\n",
    "\n",
    "**Solution 1: Exploring Starts**\n",
    "- Start each episode from a random (state, action) pair\n",
    "- Ensures all pairs have chance to be visited\n",
    "\n",
    "**Solution 2: ε-soft Policies**\n",
    "- Always have non-zero probability of selecting any action\n",
    "- E.g., ε-greedy: with prob ε choose random, else choose best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, gamma, n_episodes, epsilon=0.1, \n",
    "                               epsilon_decay=0.9999, min_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control with ε-greedy exploration.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        gamma: Discount factor\n",
    "        n_episodes: Number of episodes\n",
    "        epsilon: Initial exploration rate\n",
    "        epsilon_decay: Decay rate for epsilon\n",
    "        min_epsilon: Minimum epsilon value\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned action-value function\n",
    "        policy: Learned policy\n",
    "        stats: Training statistics\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize Q arbitrarily and returns tracking\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    # Statistics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for episode_num in range(n_episodes):\n",
    "        # Generate episode using ε-greedy policy derived from Q\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # ε-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Record stats\n",
    "        episode_rewards.append(sum(r for _, _, r in episode))\n",
    "        episode_lengths.append(len(episode))\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # Update Q using first-visit MC\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            if (state, action) not in sa_visited:\n",
    "                sa_visited.add((state, action))\n",
    "                returns_sum[state, action] += G\n",
    "                returns_count[state, action] += 1\n",
    "                Q[state, action] = returns_sum[state, action] / returns_count[state, action]\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Extract greedy policy\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        policy[s, np.argmax(Q[s])] = 1.0\n",
    "    \n",
    "    stats = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'epsilons': epsilons\n",
    "    }\n",
    "    \n",
    "    return Q, policy, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MC Control\n",
    "print(\"Monte Carlo Control with ε-greedy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "Q_mc, policy_mc, stats = mc_control_epsilon_greedy(\n",
    "    env, gamma=0.99, n_episodes=100000, \n",
    "    epsilon=1.0, epsilon_decay=0.99995, min_epsilon=0.01\n",
    ")\n",
    "mc_control_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {mc_control_time:.2f} seconds\")\n",
    "print(f\"Final epsilon: {stats['epsilons'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Smooth rewards with moving average\n",
    "window = 1000\n",
    "rewards_smooth = np.convolve(stats['episode_rewards'], \n",
    "                              np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(rewards_smooth)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward (moving avg)')\n",
    "axes[0, 0].set_title(f'Learning Curve (window={window})')\n",
    "\n",
    "# Epsilon decay\n",
    "axes[0, 1].plot(stats['epsilons'])\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].set_title('Exploration Rate Decay')\n",
    "\n",
    "# Final Q-values heatmap\n",
    "V_from_Q = np.max(Q_mc, axis=1)\n",
    "plot_value_function(V_from_Q, title=\"Learned V* = max_a Q(s,a)\", ax=axes[1, 0])\n",
    "\n",
    "# Learned policy\n",
    "plot_policy(Q_mc, title=\"Learned Policy\", ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle(\"Monte Carlo Control Results (100,000 episodes)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the learned policy\n",
    "def evaluate_policy(env, Q, n_episodes=10000):\n",
    "    \"\"\"Evaluate a greedy policy derived from Q.\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return np.array(rewards)\n",
    "\n",
    "# Compare learned policy with optimal (from DP) and random\n",
    "print(\"Policy Evaluation Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# MC learned policy\n",
    "rewards_mc = evaluate_policy(env, Q_mc, n_episodes=10000)\n",
    "print(f\"MC Policy: Success rate = {np.mean(rewards_mc)*100:.2f}%\")\n",
    "\n",
    "# Random policy\n",
    "Q_random = np.random.rand(n_states, n_actions)\n",
    "rewards_random = evaluate_policy(env, Q_random, n_episodes=10000)\n",
    "print(f\"Random Policy: Success rate = {np.mean(rewards_random)*100:.2f}%\")\n",
    "\n",
    "# Optimal policy (from Value Iteration)\n",
    "def value_iteration(P, R, gamma, theta=1e-8):\n",
    "    n_states, n_actions = R.shape\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = np.max([R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)])\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    # Extract Q\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    return Q\n",
    "\n",
    "Q_optimal = value_iteration(P, R, gamma=0.99)\n",
    "rewards_optimal = evaluate_policy(env, Q_optimal, n_episodes=10000)\n",
    "print(f\"Optimal Policy (DP): Success rate = {np.mean(rewards_optimal)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "policies = ['Random', 'MC Learned', 'Optimal (DP)']\n",
    "success_rates = [np.mean(rewards_random)*100, np.mean(rewards_mc)*100, np.mean(rewards_optimal)*100]\n",
    "colors = ['gray', 'steelblue', 'green']\n",
    "\n",
    "# Bar chart\n",
    "bars = axes[0].bar(policies, success_rates, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Success Rate (%)')\n",
    "axes[0].set_title('Policy Performance Comparison')\n",
    "axes[0].set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{rate:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# MC learned policy\n",
    "plot_policy(Q_mc, title=\"MC Learned Policy\", ax=axes[1])\n",
    "\n",
    "# Optimal policy\n",
    "plot_policy(Q_optimal, title=\"Optimal Policy (DP)\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Exploring Starts MC Control\n",
    "\n",
    "An alternative to ε-greedy is **Exploring Starts**: every (state, action) pair has non-zero probability of being the starting point of an episode.\n",
    "\n",
    "This guarantees all pairs are visited infinitely often, allowing convergence to the true optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_exploring_starts(env, gamma, n_episodes):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control with Exploring Starts.\n",
    "    \n",
    "    Note: This requires ability to set the initial state,\n",
    "    which isn't always possible in real environments.\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        # Exploring start: random initial state and action\n",
    "        # Note: FrozenLake doesn't support setting initial state,\n",
    "        # so we'll simulate by using the normal start but random first action\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        # Random first action (exploring start)\n",
    "        first_action = np.random.randint(n_actions)\n",
    "        \n",
    "        # Generate episode\n",
    "        episode = []\n",
    "        action = first_action\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if not done:\n",
    "                # Follow greedy policy after first action\n",
    "                action = np.argmax(Q[state])\n",
    "        \n",
    "        episode_rewards.append(sum(r for _, _, r in episode))\n",
    "        \n",
    "        # Update Q\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            \n",
    "            if (s, a) not in sa_visited:\n",
    "                sa_visited.add((s, a))\n",
    "                returns_sum[s, a] += G\n",
    "                returns_count[s, a] += 1\n",
    "                Q[s, a] = returns_sum[s, a] / returns_count[s, a]\n",
    "    \n",
    "    return Q, episode_rewards\n",
    "\n",
    "# Run Exploring Starts MC\n",
    "print(\"Monte Carlo Control with Exploring Starts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "Q_es, rewards_es = mc_control_exploring_starts(env, gamma=0.99, n_episodes=100000)\n",
    "\n",
    "# Evaluate\n",
    "rewards_es_eval = evaluate_policy(env, Q_es, n_episodes=10000)\n",
    "print(f\"Exploring Starts Policy: Success rate = {np.mean(rewards_es_eval)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. MC vs DP: Key Differences\n",
    "\n",
    "| Aspect | Dynamic Programming | Monte Carlo |\n",
    "|--------|--------------------|--------------|\n",
    "| **Model** | Requires complete model | Model-free |\n",
    "| **Updates** | Bootstraps (uses estimated values) | Uses actual returns |\n",
    "| **Episodes** | Can update mid-episode | Must wait for episode end |\n",
    "| **Variance** | Lower (uses expectations) | Higher (uses samples) |\n",
    "| **Bias** | Biased by initialization | Unbiased (true returns) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare convergence characteristics\n",
    "print(\"MC Characteristics Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run MC with different numbers of episodes\n",
    "episode_counts = [1000, 5000, 10000, 50000, 100000]\n",
    "mc_errors = []\n",
    "\n",
    "for n_ep in episode_counts:\n",
    "    Q_temp, _, _ = mc_control_epsilon_greedy(env, gamma=0.99, n_episodes=n_ep,\n",
    "                                             epsilon=1.0, epsilon_decay=0.9999, min_epsilon=0.01)\n",
    "    # Compare with optimal Q\n",
    "    error = np.mean(np.abs(Q_temp - Q_optimal))\n",
    "    mc_errors.append(error)\n",
    "    print(f\"{n_ep:>7} episodes: Mean |Q - Q*| = {error:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_counts, mc_errors, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Episodes')\n",
    "plt.ylabel('Mean Absolute Error in Q')\n",
    "plt.title('MC Control Convergence to Optimal Q')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Monte Carlo Methods\n",
    "\n",
    "| Method | Purpose | Key Feature |\n",
    "|--------|---------|-------------|\n",
    "| **MC Prediction** | Estimate $V^\\pi$ or $Q^\\pi$ | Average sample returns |\n",
    "| **MC Control** | Find $\\pi^*$ | Generalized Policy Iteration |\n",
    "| **First-Visit MC** | Each state counted once per episode | Simple, unbiased |\n",
    "| **Every-Visit MC** | All visits counted | More data, slightly biased |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Model-free**: MC learns from experience, no need for P and R\n",
    "2. **Uses complete episodes**: Must wait until episode ends to update\n",
    "3. **Unbiased estimates**: Uses actual returns, not bootstrapped values\n",
    "4. **High variance**: Sample returns can vary a lot\n",
    "5. **Exploration is crucial**: Need mechanisms like ε-greedy or exploring starts\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Only works for **episodic** tasks (must terminate)\n",
    "- Updates only at **episode end** (slow for long episodes)\n",
    "- **High variance** can slow convergence\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook (**05_temporal_difference.ipynb**), we'll learn **TD methods** that:\n",
    "- Update every step (not just at episode end)\n",
    "- Work for continuing (non-episodic) tasks\n",
    "- Include SARSA and Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congratulations! You've completed Part 4 of the RL Tutorial!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- Monte Carlo methods learn from complete episodes\")\n",
    "print(\"- They don't need a model - completely model-free\")\n",
    "print(\"- MC Prediction estimates V or Q by averaging returns\")\n",
    "print(\"- MC Control uses ε-greedy or exploring starts for exploration\")\n",
    "print(\"- MC has high variance but no bias from bootstrapping\")\n",
    "print(\"\\nNext: 05_temporal_difference.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
