{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Dynamic Programming\n",
    "\n",
    "In this notebook, we'll learn **Dynamic Programming (DP)** methods for solving MDPs. These are **model-based** methods that require complete knowledge of the environment.\n",
    "\n",
    "## What You'll Learn\n",
    "- Policy Evaluation (computing $V^\\pi$)\n",
    "- Policy Improvement (making policy better)\n",
    "- Policy Iteration (evaluation + improvement)\n",
    "- Value Iteration (finding $V^*$ directly)\n",
    "- Comparison of methods\n",
    "\n",
    "## Prerequisites\n",
    "- Understanding of MDPs and Bellman equations (Notebook 02)\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import time\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment and extract MDP components\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['←', '↓', '→', '↑']\n",
    "\n",
    "# Extract transition and reward matrices\n",
    "def extract_mdp(env):\n",
    "    \"\"\"Extract P[s,a,s'] and R[s,a] from environment.\"\"\"\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    \n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    \n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    \n",
    "    return P, R\n",
    "\n",
    "P, R = extract_mdp(env)\n",
    "\n",
    "print(\"FrozenLake MDP Loaded\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"States: {n_states}\")\n",
    "print(f\"Actions: {n_actions}\")\n",
    "print(f\"Transition matrix shape: {P.shape}\")\n",
    "print(f\"Reward matrix shape: {R.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def plot_value_function(V, title=\"Value Function\", ax=None):\n",
    "    \"\"\"Plot value function as a heatmap.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    V_grid = V.reshape(nrow, ncol)\n",
    "    \n",
    "    im = ax.imshow(V_grid, cmap='RdYlGn', vmin=0, vmax=max(V.max(), 0.01))\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            color = 'white' if V_grid[i, j] < V.max() / 2 else 'black'\n",
    "            ax.text(j, i, f'{cell}\\n{V[state]:.3f}', ha='center', va='center',\n",
    "                   fontsize=9, color=color)\n",
    "    \n",
    "    ax.set_xticks(range(ncol))\n",
    "    ax.set_yticks(range(nrow))\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def plot_policy(policy, title=\"Policy\", ax=None):\n",
    "    \"\"\"Plot policy showing best action in each state.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Get best action (handle both deterministic and stochastic policies)\n",
    "            if len(policy.shape) == 1:\n",
    "                best_action = int(policy[state])\n",
    "            else:\n",
    "                best_action = np.argmax(policy[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "print(\"Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. What is Dynamic Programming?\n",
    "\n",
    "**Dynamic Programming** (DP) is a method for solving complex problems by:\n",
    "1. Breaking them into simpler subproblems\n",
    "2. Solving the subproblems\n",
    "3. Combining solutions to solve the original problem\n",
    "\n",
    "## Requirements for DP\n",
    "\n",
    "DP can be applied when the problem has:\n",
    "1. **Optimal substructure**: Optimal solution can be decomposed into subproblems\n",
    "2. **Overlapping subproblems**: Subproblems recur many times (solutions can be cached)\n",
    "\n",
    "MDPs satisfy both properties:\n",
    "- Bellman equation gives recursive decomposition (optimal substructure)\n",
    "- Value function stores solutions (overlapping subproblems)\n",
    "\n",
    "## DP in Reinforcement Learning\n",
    "\n",
    "DP assumes **full knowledge of the MDP** (model-based):\n",
    "- We know the transition probabilities $P_{ss'}^a$\n",
    "- We know the reward function $R_s^a$\n",
    "\n",
    "This is used for **planning** in a known environment, not learning from experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Policy Evaluation (Prediction)\n",
    "\n",
    "**Problem**: Given a policy $\\pi$, compute the state-value function $V^\\pi$.\n",
    "\n",
    "## Approach: Iterative Policy Evaluation\n",
    "\n",
    "Use the Bellman expectation equation as an update rule:\n",
    "\n",
    "$$V_{k+1}(s) = \\sum_a \\pi(a|s) \\left[ R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V_k(s') \\right]$$\n",
    "\n",
    "Start with arbitrary $V_0$ and iterate until convergence.\n",
    "\n",
    "**Convergence**: $V_k \\to V^\\pi$ as $k \\to \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, R, policy, gamma, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Iterative Policy Evaluation.\n",
    "    \n",
    "    Args:\n",
    "        P: Transition matrix P[s,a,s']\n",
    "        R: Reward matrix R[s,a]\n",
    "        policy: Policy matrix π[s,a] (probabilities)\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        V: State value function\n",
    "        history: List of V at each iteration (for visualization)\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    # Initialize V arbitrarily (zeros is fine)\n",
    "    V = np.zeros(n_states)\n",
    "    history = [V.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        for s in range(n_states):\n",
    "            # V(s) = Σ_a π(a|s) * [R(s,a) + γ * Σ_s' P(s'|s,a) * V(s')]\n",
    "            for a in range(n_actions):\n",
    "                # Expected value of next state\n",
    "                expected_next_V = np.sum(P[s, a] * V)\n",
    "                # Add contribution from this action\n",
    "                V_new[s] += policy[s, a] * (R[s, a] + gamma * expected_next_V)\n",
    "        \n",
    "        # Check convergence\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Policy Evaluation converged in {iteration + 1} iterations (delta={delta:.2e})\")\n",
    "            break\n",
    "    \n",
    "    return V, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a uniform random policy\n",
    "uniform_policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "print(\"Evaluating Uniform Random Policy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "V_random, history_random = policy_evaluation(P, R, uniform_policy, gamma=0.99)\n",
    "\n",
    "print(\"\\nValue function for random policy:\")\n",
    "print(V_random.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the convergence of policy evaluation\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "iterations_to_show = [0, 1, 2, 5, 10, 20, 50, len(history_random)-1]\n",
    "iterations_to_show = [min(i, len(history_random)-1) for i in iterations_to_show]\n",
    "\n",
    "for idx, (ax, it) in enumerate(zip(axes.flat, iterations_to_show)):\n",
    "    V_it = history_random[it]\n",
    "    plot_value_function(V_it, title=f\"Iteration {it}\", ax=ax)\n",
    "\n",
    "plt.suptitle(\"Policy Evaluation Convergence (Random Policy, γ=0.99)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Max change per iteration\n",
    "deltas = [np.max(np.abs(history_random[i+1] - history_random[i])) \n",
    "          for i in range(len(history_random)-1)]\n",
    "axes[0].semilogy(deltas)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Max |V_new - V_old| (log scale)')\n",
    "axes[0].set_title('Convergence of Policy Evaluation')\n",
    "axes[0].axhline(y=1e-8, color='r', linestyle='--', label='Threshold (1e-8)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Value of specific states over iterations\n",
    "states_to_track = [0, 1, 6, 14]  # Start, near start, middle, near goal\n",
    "for s in states_to_track:\n",
    "    values = [history_random[i][s] for i in range(len(history_random))]\n",
    "    axes[1].plot(values, label=f'State {s}')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('V(s)')\n",
    "axes[1].set_title('Value Evolution for Selected States')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Policy Improvement\n",
    "\n",
    "**Problem**: Given a value function $V^\\pi$, find a better policy $\\pi'$.\n",
    "\n",
    "## Approach: Greedy Policy\n",
    "\n",
    "For each state, choose the action that maximizes expected return:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a \\left[ R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s') \\right]$$\n",
    "\n",
    "## Policy Improvement Theorem\n",
    "\n",
    "If $\\pi'$ is the greedy policy with respect to $V^\\pi$, then:\n",
    "\n",
    "$$V^{\\pi'}(s) \\geq V^\\pi(s) \\text{ for all } s$$\n",
    "\n",
    "The new policy is at least as good as the old one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, R, V, gamma):\n",
    "    \"\"\"\n",
    "    Compute greedy policy with respect to value function V.\n",
    "    \n",
    "    Args:\n",
    "        P: Transition matrix\n",
    "        R: Reward matrix\n",
    "        V: Current value function\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        policy: New deterministic policy (one-hot encoded)\n",
    "        Q: Action-value function\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    # Compute Q-values\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    \n",
    "    # Create greedy policy\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        best_action = np.argmax(Q[s])\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the random policy\n",
    "print(\"Policy Improvement\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "improved_policy, Q = policy_improvement(P, R, V_random, gamma=0.99)\n",
    "\n",
    "# Show the improved policy\n",
    "print(\"\\nImproved Policy (best action for each state):\")\n",
    "best_actions = np.argmax(improved_policy, axis=1).reshape(4, 4)\n",
    "for i in range(4):\n",
    "    row = [action_arrows[a] for a in best_actions[i]]\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random and improved policies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_policy(uniform_policy, title=\"Original: Uniform Random Policy\", ax=axes[0])\n",
    "plot_policy(improved_policy, title=\"Improved: Greedy w.r.t. V^π_random\", ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the improved policy to verify it's better\n",
    "V_improved, _ = policy_evaluation(P, R, improved_policy, gamma=0.99)\n",
    "\n",
    "print(\"\\nComparison of Value Functions\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'State':<8} {'V_random':>12} {'V_improved':>12} {'Improvement':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for s in range(n_states):\n",
    "    diff = V_improved[s] - V_random[s]\n",
    "    print(f\"{s:<8} {V_random[s]:>12.4f} {V_improved[s]:>12.4f} {diff:>12.4f}\")\n",
    "\n",
    "print(f\"\\nTotal improvement: {np.sum(V_improved - V_random):.4f}\")\n",
    "print(f\"All states improved or same: {np.all(V_improved >= V_random - 1e-10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Policy Iteration\n",
    "\n",
    "**Policy Iteration** alternates between:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for current policy\n",
    "2. **Policy Improvement**: Make policy greedy with respect to $V^\\pi$\n",
    "\n",
    "Repeat until the policy no longer changes (converges to optimal policy $\\pi^*$).\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "```\n",
    "1. Initialize policy π arbitrarily\n",
    "2. Repeat:\n",
    "   a. Policy Evaluation: Compute V^π\n",
    "   b. Policy Improvement: π' = greedy(V^π)\n",
    "   c. If π' = π, stop (converged)\n",
    "   d. π = π'\n",
    "3. Return π* and V*\n",
    "```\n",
    "\n",
    "## Convergence\n",
    "\n",
    "Policy iteration is guaranteed to converge to the optimal policy in a finite number of iterations (since there are finite deterministic policies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, R, gamma, theta=1e-8, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm.\n",
    "    \n",
    "    Args:\n",
    "        P: Transition matrix\n",
    "        R: Reward matrix\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold for policy evaluation\n",
    "        max_iterations: Maximum policy improvement iterations\n",
    "    \n",
    "    Returns:\n",
    "        policy: Optimal policy\n",
    "        V: Optimal value function\n",
    "        policy_history: List of policies at each iteration\n",
    "        V_history: List of value functions at each iteration\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    # Initialize with random policy\n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "    \n",
    "    policy_history = [policy.copy()]\n",
    "    V_history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        V, _ = policy_evaluation(P, R, policy, gamma, theta=theta)\n",
    "        V_history.append(V.copy())\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy, Q = policy_improvement(P, R, V, gamma)\n",
    "        \n",
    "        # Check if policy changed\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            print(f\"Policy Iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "        policy_history.append(policy.copy())\n",
    "    \n",
    "    return policy, V, policy_history, V_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Policy Iteration\n",
    "print(\"Running Policy Iteration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "optimal_policy_pi, V_star_pi, policy_history, V_history = policy_iteration(P, R, gamma=0.99)\n",
    "pi_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime taken: {pi_time:.4f} seconds\")\n",
    "print(f\"\\nOptimal Value Function V*:\")\n",
    "print(V_star_pi.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy evolution\n",
    "n_policies = len(policy_history)\n",
    "fig, axes = plt.subplots(1, min(n_policies, 5), figsize=(4*min(n_policies, 5), 4))\n",
    "\n",
    "if n_policies == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < n_policies:\n",
    "        plot_policy(policy_history[i], title=f\"Iteration {i}\", ax=ax)\n",
    "\n",
    "plt.suptitle(\"Policy Evolution in Policy Iteration\", fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final optimal policy and value function\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_value_function(V_star_pi, title=\"Optimal Value Function V*\", ax=axes[0])\n",
    "plot_policy(optimal_policy_pi, title=\"Optimal Policy π*\", ax=axes[1])\n",
    "\n",
    "plt.suptitle(\"Policy Iteration Result (γ=0.99)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Value Iteration\n",
    "\n",
    "**Value Iteration** combines policy evaluation and improvement into a single update:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\left[ R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V_k(s') \\right]$$\n",
    "\n",
    "This is applying the **Bellman Optimality Equation** as an update rule.\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "- Policy Iteration does full policy evaluation (many iterations) before each improvement\n",
    "- Value Iteration does only **one sweep** of evaluation, then immediately improves\n",
    "- Value Iteration is like \"truncated\" policy iteration with k=1\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "```\n",
    "1. Initialize V arbitrarily\n",
    "2. Repeat:\n",
    "   For each state s:\n",
    "     V(s) = max_a [R(s,a) + γ * Σ P(s'|s,a) * V(s')]\n",
    "   Until V converges\n",
    "3. Extract policy: π(s) = argmax_a [R(s,a) + γ * Σ P(s'|s,a) * V(s')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, R, gamma, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm.\n",
    "    \n",
    "    Args:\n",
    "        P: Transition matrix\n",
    "        R: Reward matrix\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy\n",
    "        history: Value function at each iteration\n",
    "    \"\"\"\n",
    "    n_states = P.shape[0]\n",
    "    n_actions = P.shape[1]\n",
    "    \n",
    "    # Initialize V arbitrarily\n",
    "    V = np.zeros(n_states)\n",
    "    history = [V.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        for s in range(n_states):\n",
    "            # Compute Q-values for all actions\n",
    "            Q_s = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                Q_s[a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "            \n",
    "            # Take maximum\n",
    "            V_new[s] = np.max(Q_s)\n",
    "        \n",
    "        # Check convergence\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value Iteration converged in {iteration + 1} iterations (delta={delta:.2e})\")\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        Q_s = np.zeros(n_actions)\n",
    "        for a in range(n_actions):\n",
    "            Q_s[a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "        policy[s, np.argmax(Q_s)] = 1.0\n",
    "    \n",
    "    return V, policy, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Value Iteration\n",
    "print(\"Running Value Iteration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "V_star_vi, optimal_policy_vi, vi_history = value_iteration(P, R, gamma=0.99)\n",
    "vi_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime taken: {vi_time:.4f} seconds\")\n",
    "print(f\"\\nOptimal Value Function V*:\")\n",
    "print(V_star_vi.reshape(4, 4).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Value Iteration convergence\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "iterations_to_show = [0, 1, 5, 10, 20, 50, 100, len(vi_history)-1]\n",
    "iterations_to_show = [min(i, len(vi_history)-1) for i in iterations_to_show]\n",
    "\n",
    "for idx, (ax, it) in enumerate(zip(axes.flat, iterations_to_show)):\n",
    "    V_it = vi_history[it]\n",
    "    plot_value_function(V_it, title=f\"Iteration {it}\", ax=ax)\n",
    "\n",
    "plt.suptitle(\"Value Iteration Convergence (γ=0.99)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Value Iteration convergence\n",
    "vi_deltas = [np.max(np.abs(vi_history[i+1] - vi_history[i])) \n",
    "             for i in range(len(vi_history)-1)]\n",
    "ax.semilogy(vi_deltas, label='Value Iteration', linewidth=2)\n",
    "\n",
    "ax.axhline(y=1e-8, color='r', linestyle='--', label='Threshold')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max |V_new - V_old| (log scale)')\n",
    "ax.set_title('Value Iteration Convergence')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Comparison: Policy Iteration vs Value Iteration\n",
    "\n",
    "Let's compare the two methods in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final results\n",
    "print(\"Comparison of Policy Iteration vs Value Iteration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if they found the same solution\n",
    "V_diff = np.max(np.abs(V_star_pi - V_star_vi))\n",
    "policy_same = np.array_equal(np.argmax(optimal_policy_pi, axis=1), \n",
    "                              np.argmax(optimal_policy_vi, axis=1))\n",
    "\n",
    "print(f\"\\nMax difference in V*: {V_diff:.2e}\")\n",
    "print(f\"Same optimal policy: {policy_same}\")\n",
    "print(f\"\\nPolicy Iteration time: {pi_time:.4f}s\")\n",
    "print(f\"Value Iteration time: {vi_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both results side by side\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "plot_value_function(V_star_pi, title=\"V* (Policy Iteration)\", ax=axes[0, 0])\n",
    "plot_value_function(V_star_vi, title=\"V* (Value Iteration)\", ax=axes[0, 1])\n",
    "plot_policy(optimal_policy_pi, title=\"π* (Policy Iteration)\", ax=axes[1, 0])\n",
    "plot_policy(optimal_policy_vi, title=\"π* (Value Iteration)\", ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with different discount factors\n",
    "gammas = [0.5, 0.9, 0.95, 0.99, 0.999]\n",
    "results = []\n",
    "\n",
    "print(\"Benchmarking with Different Discount Factors\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'γ':>8} {'PI iters':>12} {'PI time':>12} {'VI iters':>12} {'VI time':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for gamma in gammas:\n",
    "    # Policy Iteration\n",
    "    start = time.time()\n",
    "    _, V_pi, policy_hist, _ = policy_iteration(P, R, gamma, theta=1e-8)\n",
    "    pi_time_g = time.time() - start\n",
    "    pi_iters = len(policy_hist)\n",
    "    \n",
    "    # Value Iteration\n",
    "    start = time.time()\n",
    "    V_vi, _, vi_hist = value_iteration(P, R, gamma, theta=1e-8)\n",
    "    vi_time_g = time.time() - start\n",
    "    vi_iters = len(vi_hist) - 1\n",
    "    \n",
    "    print(f\"{gamma:>8.3f} {pi_iters:>12} {pi_time_g:>12.4f}s {vi_iters:>12} {vi_time_g:>12.4f}s\")\n",
    "    results.append({'gamma': gamma, 'pi_iters': pi_iters, 'pi_time': pi_time_g,\n",
    "                    'vi_iters': vi_iters, 'vi_time': vi_time_g})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot iterations\n",
    "gammas_plot = [r['gamma'] for r in results]\n",
    "pi_iters_plot = [r['pi_iters'] for r in results]\n",
    "vi_iters_plot = [r['vi_iters'] for r in results]\n",
    "\n",
    "x = np.arange(len(gammas_plot))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, pi_iters_plot, width, label='Policy Iteration', color='steelblue')\n",
    "axes[0].bar(x + width/2, vi_iters_plot, width, label='Value Iteration', color='orange')\n",
    "axes[0].set_xlabel('Discount Factor γ')\n",
    "axes[0].set_ylabel('Iterations')\n",
    "axes[0].set_title('Iterations to Convergence')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([f'{g}' for g in gammas_plot])\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot time\n",
    "pi_times_plot = [r['pi_time'] for r in results]\n",
    "vi_times_plot = [r['vi_time'] for r in results]\n",
    "\n",
    "axes[1].bar(x - width/2, pi_times_plot, width, label='Policy Iteration', color='steelblue')\n",
    "axes[1].bar(x + width/2, vi_times_plot, width, label='Value Iteration', color='orange')\n",
    "axes[1].set_xlabel('Discount Factor γ')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_title('Computation Time')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'{g}' for g in gammas_plot])\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Evaluating the Optimal Policy\n",
    "\n",
    "Let's test our learned optimal policy on the actual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_empirically(env, policy, n_episodes=10000):\n",
    "    \"\"\"Evaluate a policy by running episodes.\"\"\"\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from policy\n",
    "            if len(policy.shape) == 1:\n",
    "                action = int(policy[obs])\n",
    "            else:\n",
    "                action = np.random.choice(n_actions, p=policy[obs])\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        steps_list.append(steps)\n",
    "    \n",
    "    return np.array(rewards), np.array(steps_list)\n",
    "\n",
    "# Evaluate different policies\n",
    "env_eval = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "n_eval_episodes = 10000\n",
    "\n",
    "print(f\"Evaluating Policies ({n_eval_episodes} episodes each)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Random policy\n",
    "uniform_policy = np.ones((n_states, n_actions)) / n_actions\n",
    "rewards_random, steps_random = evaluate_policy_empirically(env_eval, uniform_policy, n_eval_episodes)\n",
    "print(f\"Random Policy: Success rate = {np.mean(rewards_random)*100:.2f}%, Avg steps = {np.mean(steps_random):.1f}\")\n",
    "\n",
    "# Optimal policy from Policy Iteration\n",
    "rewards_pi, steps_pi = evaluate_policy_empirically(env_eval, optimal_policy_pi, n_eval_episodes)\n",
    "print(f\"Policy Iteration: Success rate = {np.mean(rewards_pi)*100:.2f}%, Avg steps = {np.mean(steps_pi):.1f}\")\n",
    "\n",
    "# Optimal policy from Value Iteration\n",
    "rewards_vi, steps_vi = evaluate_policy_empirically(env_eval, optimal_policy_vi, n_eval_episodes)\n",
    "print(f\"Value Iteration: Success rate = {np.mean(rewards_vi)*100:.2f}%, Avg steps = {np.mean(steps_vi):.1f}\")\n",
    "\n",
    "env_eval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Success rates\n",
    "policies_names = ['Random', 'Policy Iteration', 'Value Iteration']\n",
    "success_rates = [np.mean(rewards_random)*100, np.mean(rewards_pi)*100, np.mean(rewards_vi)*100]\n",
    "colors = ['gray', 'steelblue', 'orange']\n",
    "\n",
    "bars = axes[0].bar(policies_names, success_rates, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Success Rate (%)')\n",
    "axes[0].set_title('Policy Comparison: Success Rate')\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{rate:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Average steps (only for successful episodes)\n",
    "avg_steps = [np.mean(steps_random), np.mean(steps_pi), np.mean(steps_vi)]\n",
    "\n",
    "bars = axes[1].bar(policies_names, avg_steps, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Average Steps')\n",
    "axes[1].set_title('Policy Comparison: Average Episode Length')\n",
    "\n",
    "for bar, steps in zip(bars, avg_steps):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{steps:.1f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Effect of Discount Factor on Optimal Policy\n",
    "\n",
    "Let's see how different discount factors affect the learned optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal policies for different discount factors\n",
    "gammas_to_compare = [0.1, 0.5, 0.9, 0.99]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for idx, gamma in enumerate(gammas_to_compare):\n",
    "    V, policy, _ = value_iteration(P, R, gamma)\n",
    "    \n",
    "    plot_value_function(V, title=f\"V* (γ={gamma})\", ax=axes[0, idx])\n",
    "    plot_policy(policy, title=f\"π* (γ={gamma})\", ax=axes[1, idx])\n",
    "\n",
    "plt.suptitle(\"Effect of Discount Factor on Optimal Policy\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Lower γ: Agent is 'shortsighted', values states closer to goal more\")\n",
    "print(\"- Higher γ: Agent is 'farsighted', considers long-term consequences\")\n",
    "print(\"- The optimal policy may change based on how much we value future rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Dynamic Programming Methods\n",
    "\n",
    "| Method | What it does | Key equation | Requirements |\n",
    "|--------|-------------|--------------|---------------|\n",
    "| **Policy Evaluation** | Computes $V^\\pi$ | Bellman Expectation | Policy $\\pi$, full MDP |\n",
    "| **Policy Improvement** | Gets better policy | Greedy w.r.t. V | Value function V |\n",
    "| **Policy Iteration** | Finds $\\pi^*$, $V^*$ | Eval + Improve | Full MDP |\n",
    "| **Value Iteration** | Finds $V^*$, $\\pi^*$ | Bellman Optimality | Full MDP |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **DP requires full model knowledge** - we need P and R\n",
    "2. **Policy Iteration**: Alternates evaluation and improvement, fewer iterations but each is expensive\n",
    "3. **Value Iteration**: One-step evaluation + improvement, more iterations but each is cheap\n",
    "4. **Both converge** to the optimal policy and value function\n",
    "5. **Discount factor** affects what the optimal policy looks like\n",
    "\n",
    "## Limitations of DP\n",
    "\n",
    "- Requires **complete model** of the environment (P, R)\n",
    "- Computationally expensive for large state spaces (curse of dimensionality)\n",
    "- Cannot be used when the environment is unknown\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook (**04_monte_carlo.ipynb**), we'll learn **model-free** methods that can learn from experience without knowing the transition probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congratulations! You've completed Part 3 of the RL Tutorial!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- Policy Evaluation: Iteratively compute V^π using Bellman expectation\")\n",
    "print(\"- Policy Improvement: Make policy greedy w.r.t. current V\")\n",
    "print(\"- Policy Iteration: Alternate eval and improvement until convergence\")\n",
    "print(\"- Value Iteration: Apply Bellman optimality directly\")\n",
    "print(\"- Both methods find the optimal policy when given the full MDP\")\n",
    "print(\"\\nNext: 04_monte_carlo.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
