{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 1.1: Foundations of Reinforcement Learning\n\nWelcome to the first notebook in our Reinforcement Learning tutorial series! This notebook introduces the fundamental concepts that make RL unique.\n\n## What This Notebook Covers\n- What makes Reinforcement Learning different from other ML paradigms\n- The reward hypothesis and sequential decision making\n- The agent-environment interaction loop\n- States and the Markov property\n- Fully observable (MDP) vs partially observable (POMDP) environments\n\n## What This Notebook Does NOT Cover\n\n| Topic | Why Not Here | How It Differs From What We Cover |\n|-------|--------------|-----------------------------------|\n| **Specific RL algorithms** | This notebook focuses on concepts, not methods. Algorithms are covered in later notebooks once you understand the foundations. | We explain *what* RL is trying to achieve (maximize reward, find good policies). Algorithms like Q-learning, SARSA, and Policy Gradient are *how* to achieve it — they require understanding value functions and policies first. |\n| **Deep reinforcement learning** | Deep RL combines RL with neural networks, adding significant complexity. We start with tabular methods where states are discrete and enumerable. | In this notebook, states are simple integers (0-15 in FrozenLake). Deep RL handles high-dimensional states like images or continuous values, using neural networks to approximate value functions — a topic that builds on everything here. |\n| **Multi-agent RL** | Multi-agent settings introduce game theory, coordination, and competition. We focus on single-agent foundations first. | Here, one agent interacts with a passive environment. In multi-agent RL, multiple agents interact simultaneously, and one agent's optimal action depends on what others do — fundamentally changing the problem structure. |\n\n## Prerequisites\n- Basic Python programming\n- Familiarity with NumPy and Matplotlib\n- Basic probability concepts (expected value, probability distributions)\n\n## How to Read This Notebook\n1. **Theory first**: Each section starts with conceptual explanations\n2. **Intuition building**: We use the FrozenLake environment as a concrete example throughout\n3. **Code demonstrations**: Run the code cells to see concepts in action\n4. **Checkpoints**: Use these to verify your understanding before moving on\n\nLet's begin!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment.\n",
    "\n",
    "> **Note:** If you're running this in a fresh environment (like Google Colab or a new virtualenv), uncomment and run the installation cell below first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (uncomment if needed)\n",
    "# !pip install gymnasium[toy-text] numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**. Unlike supervised learning (where we have labeled data) or unsupervised learning (where we find patterns in data), RL learns from **trial and error**.\n",
    "\n",
    "## Key Characteristics of RL\n",
    "\n",
    "RL has several unique characteristics that distinguish it from other machine learning paradigms:\n",
    "\n",
    "| Characteristic | Description | FrozenLake Example |\n",
    "|---------------|-------------|---------------------|\n",
    "| **No supervisor** | There's no teacher telling the agent what the correct action is. Instead, there's only a reward signal indicating how good or bad an action was. | The agent isn't told \"go RIGHT here\" or \"avoid this path.\" It only receives +1 for reaching the Goal and 0 otherwise. It must discover the correct path through trial and error. |\n",
    "| **Delayed feedback** | The reward may come much later after the action was taken. A chess move might only prove good or bad many moves later. | The agent receives reward only at the end of an episode (reaching Goal or falling into a Hole). All intermediate moves on frozen tiles give 0 reward, so the agent doesn't know if early moves were good until much later. |\n",
    "| **Sequential data** | Data is not i.i.d. (independent and identically distributed). Current observations depend on previous actions. | The state the agent sees depends entirely on its previous actions. Being in state 6 means the agent took a specific sequence of moves from the start. Each observation is a direct consequence of prior decisions. |\n",
    "| **Actions affect future** | The agent's actions influence the data it will receive in the future. This creates a feedback loop. | Moving DOWN from state 0 leads to states 4 or below, while moving RIGHT leads to states 1 or to the right. The agent's choice determines which parts of the grid it will experience and whether it encounters Holes or the Goal. |\n",
    "\n",
    "> **Checkpoint — You should now understand:**\n",
    "> - Why RL is different from supervised and unsupervised learning\n",
    "> - The four key characteristics that make RL unique\n",
    "> - How these characteristics apply to the FrozenLake environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. The Agent-Environment Interface\n",
    "\n",
    "In RL, we have two main entities:\n",
    "\n",
    "1. **Agent**: The learner and decision maker\n",
    "2. **Environment**: Everything outside the agent that it interacts with\n",
    "\n",
    "## The Interaction Loop\n",
    "\n",
    "At each time step $t$:\n",
    "\n",
    "**Agent:**\n",
    "- Receives observation $O_t$\n",
    "- Receives reward $R_t$\n",
    "- Executes action $A_t$\n",
    "\n",
    "**Environment:**\n",
    "- Receives action $A_t$\n",
    "- Emits observation $O_{t+1}$\n",
    "- Emits reward $R_{t+1}$\n",
    "\n",
    "```\n",
    "    ┌────────────────────────────────────────┐\n",
    "    │                                        │\n",
    "    │    ┌───────┐         ┌─────────────┐   │\n",
    "    │    │       │  Action │             │   │\n",
    "    │    │ Agent │────────►│ Environment │   │\n",
    "    │    │       │         │             │   │\n",
    "    │    └───────┘◄────────└─────────────┘   │\n",
    "    │        ▲    Observation, Reward        │\n",
    "    │        │                               │\n",
    "    └────────┼───────────────────────────────┘\n",
    "             │\n",
    "         Time step t\n",
    "```\n",
    "\n",
    "## The History\n",
    "\n",
    "The **history** $H_t$ is the sequence of all observations, actions, and rewards up to time $t$:\n",
    "\n",
    "$$H_t = O_1, R_1, A_1, O_2, R_2, A_2, \\ldots, A_{t-1}, O_t, R_t$$\n",
    "\n",
    "This is everything that has happened. The next action (by the agent) and the next observation/reward (by the environment) depend on this history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's See This in Action: FrozenLake Environment\n",
    "\n",
    "We'll use the **FrozenLake** environment from Gymnasium. This is a simple grid world where:\n",
    "\n",
    "- **S** (Start): Starting position\n",
    "- **F** (Frozen): Safe frozen surface (you can walk on it)\n",
    "- **H** (Hole): Hole in the ice (falling in ends the episode with 0 reward)\n",
    "- **G** (Goal): The goal (reaching it gives reward of 1)\n",
    "\n",
    "The agent can take 4 actions:\n",
    "- 0: LEFT\n",
    "- 1: DOWN\n",
    "- 2: RIGHT\n",
    "- 3: UP\n",
    "\n",
    "**Note**: The ice is slippery! When the agent chooses an action, it only moves in that direction 1/3 of the time. The other 2/3 of the time, it slips perpendicular to the intended direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=True)\n",
    "\n",
    "# Reset the environment to get the initial observation\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "print(\"FrozenLake Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\\nState space: {env.observation_space}\")\n",
    "print(f\"Number of states: {env.observation_space.n}\")\n",
    "print(f\"\\nAction space: {env.action_space}\")\n",
    "print(f\"Number of actions: {env.action_space.n}\")\n",
    "print(f\"\\nInitial observation (state): {observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function to visualize the FrozenLake grid\n",
    "def visualize_frozenlake(env, current_state=None, title=\"FrozenLake Environment\"):\n",
    "    \"\"\"Visualize the FrozenLake grid with the current state highlighted.\"\"\"\n",
    "    # Get the map\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Color mapping\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    # Draw the grid\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            cell = desc[i, j]\n",
    "            color = colors.get(cell, 'white')\n",
    "            \n",
    "            # Highlight current state\n",
    "            state_idx = i * ncol + j\n",
    "            if current_state is not None and state_idx == current_state:\n",
    "                rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True, \n",
    "                                     facecolor='yellow', edgecolor='black', linewidth=2)\n",
    "            else:\n",
    "                rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                     facecolor=color, edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text\n",
    "            ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                   ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightblue', label='S: Start'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='white', edgecolor='black', label='F: Frozen (safe)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightcoral', label='H: Hole (game over)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightgreen', label='G: Goal (reward!)'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='yellow', edgecolor='black', linewidth=2, label='Current position')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Visualize the environment\n",
    "visualize_frozenlake(env, current_state=observation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (2148721985.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mLet's see how the agent interacts with the environment step by step.\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "## The Agent-Environment Interaction in Code\n",
    "\n",
    "Let's see how the agent interacts with the environment step by step.\n",
    "\n",
    "> **Note on Episode Endings:** In Gymnasium, an episode can end in two ways:\n",
    "> - **Terminated**: The agent reached a terminal state (Goal or Hole). The task is \"naturally\" over.\n",
    "> - **Truncated**: The episode was cut short by a time limit (e.g., taking too many steps without reaching the goal). This prevents the agent from wandering forever.\n",
    ">\n",
    "> The old Gym API used a single `done` flag, but the new Gymnasium API separates these cases because they have different implications for learning (truncated episodes might still have future value, while terminated episodes don't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "print(\"Step-by-Step Agent-Environment Interaction\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nInitial state: {observation}\")\n",
    "\n",
    "# Action names for better readability\n",
    "action_names = {0: 'LEFT', 1: 'DOWN', 2: 'RIGHT', 3: 'UP'}\n",
    "\n",
    "# Helper function to compute expected next state (for slip detection)\n",
    "def get_expected_state(state, action, nrow=4, ncol=4):\n",
    "    \"\"\"Calculate where the agent would go if no slip occurred.\"\"\"\n",
    "    row, col = state // ncol, state % ncol\n",
    "    if action == 0:  # LEFT\n",
    "        col = max(col - 1, 0)\n",
    "    elif action == 1:  # DOWN\n",
    "        row = min(row + 1, nrow - 1)\n",
    "    elif action == 2:  # RIGHT\n",
    "        col = min(col + 1, ncol - 1)\n",
    "    elif action == 3:  # UP\n",
    "        row = max(row - 1, 0)\n",
    "    return row * ncol + col\n",
    "\n",
    "# Let's take a few actions and observe the interaction\n",
    "history = [(observation, None, None)]  # (state, action, reward)\n",
    "\n",
    "for step in range(5):\n",
    "    # Agent chooses a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Environment responds with new observation, reward, and other info\n",
    "    new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    history.append((new_observation, action, reward))\n",
    "    \n",
    "    # Check if the agent slipped\n",
    "    expected_state = get_expected_state(observation, action)\n",
    "    slipped = (new_observation != expected_state)\n",
    "    slip_msg = \" (Slipped!)\" if slipped else \"\"\n",
    "    \n",
    "    print(f\"\\n--- Step {step + 1} ---\")\n",
    "    print(f\"  State: {observation}\")\n",
    "    print(f\"  Action taken: {action} ({action_names[action]})\")\n",
    "    print(f\"  Expected state: {expected_state}\")\n",
    "    print(f\"  New state: {new_observation}{slip_msg}\")\n",
    "    print(f\"  Reward received: {reward}\")\n",
    "    print(f\"  Episode over: {terminated or truncated}\")\n",
    "    \n",
    "    observation = new_observation\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(\"\\n*** Episode ended! ***\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the history\n",
    "print(\"\\nHistory of the episode:\")\n",
    "print(\"H_t = \", end=\"\")\n",
    "for i, (state, action, reward) in enumerate(history):\n",
    "    if action is None:\n",
    "        print(f\"S{state}\", end=\"\")\n",
    "    else:\n",
    "        print(f\" → A{action}({action_names[action]}) → R{reward}, S{state}\", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> **Checkpoint — You should now understand:**\n",
    "> - The agent-environment interaction loop (observe → act → receive reward → repeat)\n",
    "> - How the history captures everything that has happened\n",
    "> - How to use the Gymnasium API: `reset()`, `step()`, and the returned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# 3. The Reward Signal\n",
    "\n",
    "The **reward** $R_t$ is a scalar feedback signal that indicates how well the agent is doing at time step $t$.\n",
    "\n",
    "The agent's goal is to **maximize the cumulative reward** over time, not just the immediate reward.\n",
    "\n",
    "$$\\text{Goal: Maximize } \\sum_{t=0}^{\\infty} R_t$$\n",
    "\n",
    "### The Reward Signal in FrozenLake\n",
    "\n",
    "In the FrozenLake environment, the reward structure is simple but instructive:\n",
    "\n",
    "| Event | Reward | Meaning |\n",
    "|-------|--------|--------|\n",
    "| Step on Frozen tile (F) | 0 | No immediate feedback - the agent doesn't know if it's on the right path |\n",
    "| Fall into Hole (H) | 0 | Episode ends with no reward - the agent failed |\n",
    "| Reach Goal (G) | +1 | Episode ends successfully - this is the only positive signal |\n",
    "\n",
    "This **sparse reward** structure makes learning challenging:\n",
    "- The agent receives the same reward (0) for safe moves and falling into holes\n",
    "- Only reaching the Goal provides a positive signal\n",
    "- The agent must learn to associate earlier actions with the eventual outcome\n",
    "\n",
    "### How Can the Agent Maximize Reward in FrozenLake?\n",
    "\n",
    "Since the only positive reward (+1) comes from reaching the Goal, the agent maximizes cumulative reward by:\n",
    "\n",
    "1. **Finding paths that lead to the Goal** - The agent must discover which sequences of actions successfully navigate from S to G\n",
    "\n",
    "2. **Avoiding Holes** - Falling into a Hole ends the episode with 0 total reward, so the agent must learn to steer away from H tiles\n",
    "\n",
    "3. **Handling the slippery ice** - Since the ice is slippery, the intended action only succeeds 1/3 of the time. The other 2/3 of the time, the agent slips perpendicular to the intended direction. The agent must learn policies that are robust to this stochasticity (e.g., staying away from Holes even when \"pushed\" by the slippery ice)\n",
    "\n",
    "4. **Reaching the Goal quickly (with discounting)** - When using a discount factor $\\gamma < 1$, reaching the Goal sooner yields a higher present value. A reward of +1 received after 5 steps is worth $\\gamma^5$, while +1 after 10 steps is worth only $\\gamma^{10}$\n",
    "\n",
    "### The Reward Hypothesis\n",
    "\n",
    "> **\"All goals can be described as the maximization of expected cumulative reward.\"**\n",
    "\n",
    "This is a foundational assumption in RL. While powerful, it's also debated:\n",
    "\n",
    "**Arguments for:**\n",
    "- Many complex behaviors can indeed be expressed as reward maximization\n",
    "- It provides a unified framework for decision-making\n",
    "\n",
    "**Arguments against:**\n",
    "- Some goals (curiosity, creativity, ethics) are hard to quantify as a single scalar\n",
    "- Designing the right reward function (reward shaping) is often very challenging\n",
    "\n",
    "> **Checkpoint — You should now understand:**\n",
    "> - What the reward signal is and how it guides learning\n",
    "> - Why FrozenLake's sparse reward structure makes learning challenging\n",
    "> - The reward hypothesis and its implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Sequential Decision Making\n",
    "\n",
    "The goal of sequential decision making is to select actions that **maximize total future reward**.\n",
    "\n",
    "Key insight: Actions may have **long-term consequences**, and rewards may be **delayed**. Sometimes it's better to sacrifice immediate reward for greater long-term gain.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "1. **Financial investment**: You invest money today (losing immediate spending power) for returns months or years later.\n",
    "\n",
    "2. **Helicopter refueling**: Refueling now (taking time away from the mission) prevents crashes hours later.\n",
    "\n",
    "3. **Chess**: Sacrificing a piece now might lead to checkmate many moves later.\n",
    "\n",
    "4. **Education**: Studying hard now (foregoing leisure) leads to better career opportunities later.\n",
    "\n",
    "### Sequential Decision Making in FrozenLake\n",
    "\n",
    "FrozenLake is a perfect example of sequential decision making. Let's break down why:\n",
    "\n",
    "#### Why is it Sequential?\n",
    "\n",
    "Each decision depends on where you are, and where you are depends on all previous decisions:\n",
    "\n",
    "```\n",
    "Step 1: At S (state 0) → Choose DOWN → Move to state 4\n",
    "Step 2: At state 4 → Choose DOWN → Move to state 8\n",
    "Step 3: At state 8 → Choose RIGHT → Move to state 9\n",
    "...and so on until reaching G or falling into H\n",
    "```\n",
    "\n",
    "The agent cannot jump directly to the Goal - it must navigate through a **sequence of states**, and each action opens or closes future possibilities.\n",
    "\n",
    "#### Trade-offs in FrozenLake\n",
    "\n",
    "| Sacrifice | Short-term Cost | Long-term Benefit |\n",
    "|-----------|-----------------|-------------------|\n",
    "| **Taking a longer path** | More steps, lower discounted reward | Avoids Holes, higher probability of reaching Goal |\n",
    "| **Moving away from Goal temporarily** | Increases distance to reward | May avoid a dangerous Hole in the direct path |\n",
    "| **Staying near edges** | Limits movement options | Reduces risk of slipping into Holes |\n",
    "\n",
    "#### A Concrete Example: The Greedy Trap\n",
    "\n",
    "```\n",
    "Grid layout (4x4):\n",
    "S  F  F  F      States: 0  1  2  3\n",
    "F  H  F  H              4  5  6  7\n",
    "F  F  F  H              8  9  10 11\n",
    "H  F  F  G              12 13 14 15\n",
    "```\n",
    "\n",
    "**Greedy approach** (always move toward Goal):\n",
    "- From state 0: Go RIGHT and DOWN as directly as possible toward state 15\n",
    "- Problem: This path goes through states 5, 7, or 11 - all Holes!\n",
    "\n",
    "**Sequential thinking approach**:\n",
    "- From state 0: Go DOWN first (states 0→4→8), then navigate around Holes\n",
    "- The path S→4→8→9→10→14→15 avoids all Holes\n",
    "- Even though going DOWN initially moves \"sideways\" relative to the Goal, it's the winning strategy\n",
    "\n",
    "#### The Key Insight\n",
    "\n",
    "In FrozenLake, the agent must learn that:\n",
    "- **Immediate progress toward the Goal can be fatal** (Holes block direct paths)\n",
    "- **Seemingly suboptimal moves may be necessary** (going around Holes)\n",
    "- **The slippery ice adds uncertainty** - even \"safe\" moves might slip into danger (recall that intended actions only succeed 1/3 of the time)\n",
    "- **Every decision shapes future options** - one wrong move can make the Goal unreachable\n",
    "\n",
    "This is the essence of sequential decision making: **optimize for the cumulative outcome, not each individual step**.\n",
    "\n",
    "> **Checkpoint — You should now understand:**\n",
    "> - Why actions have long-term consequences\n",
    "> - The difference between greedy and strategic decision making\n",
    "> - Why the \"obvious\" path in FrozenLake is often wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. State and the Markov Property\n",
    "\n",
    "## What is State?\n",
    "\n",
    "The **state** is the information used to determine what happens next. Formally, the state is a function of the history:\n",
    "\n",
    "$$S_t = f(H_t)$$\n",
    "\n",
    "The state is a **summary** of the history that captures all relevant information.\n",
    "\n",
    "## Types of State\n",
    "\n",
    "### 1. Environment State ($S_t^e$)\n",
    "- The environment's **internal representation**\n",
    "- Used by the environment to generate the next observation and reward\n",
    "- Usually **not visible** to the agent\n",
    "- May contain irrelevant information\n",
    "\n",
    "### 2. Agent State ($S_t^a$)\n",
    "- The agent's **internal representation**\n",
    "- Used by the agent to select the next action\n",
    "- Can be any function of the history: $S_t^a = f(H_t)$\n",
    "- This is what RL algorithms use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Markov Property\n",
    "\n",
    "A state $S_t$ is **Markov** if and only if:\n",
    "\n",
    "$$P[S_{t+1} | S_t] = P[S_{t+1} | S_1, S_2, \\ldots, S_t]$$\n",
    "\n",
    "In plain English: **\"The future is independent of the past, given the present.\"**\n",
    "\n",
    "If a state is Markov:\n",
    "- The state captures all relevant information from the history\n",
    "- Once we know the current state, the history can be thrown away\n",
    "- The state is a **sufficient statistic** of the future\n",
    "\n",
    "### Examples:\n",
    "\n",
    "**Markov:** In chess, the current board position is Markov. It doesn't matter how we got there - only the current position matters for determining the best move.\n",
    "\n",
    "**Not Markov:** In poker with hidden cards, just knowing the visible cards isn't Markov - the history of betting might give information about hidden cards.\n",
    "\n",
    "### Why is FrozenLake Markov?\n",
    "\n",
    "FrozenLake is a perfect example of a Markov environment. Here's why:\n",
    "\n",
    "#### The State Contains All Relevant Information\n",
    "\n",
    "In FrozenLake, the state is simply the agent's position (0-15). This single number tells us everything we need to know:\n",
    "\n",
    "| What We Know from State | Why It's Sufficient |\n",
    "|------------------------|---------------------|\n",
    "| Current position on grid | Determines available actions and their outcomes |\n",
    "| Distance to Goal | Implicit in position (state 15 is Goal) |\n",
    "| Nearby Holes | Grid layout is fixed, so position reveals dangers |\n",
    "| Possible next states | Transition probabilities depend only on current position |\n",
    "\n",
    "#### The History Doesn't Matter\n",
    "\n",
    "Consider two different histories that both end in **state 10**:\n",
    "\n",
    "```\n",
    "History A: 0 → 4 → 8 → 9 → 10  (came from the left)\n",
    "History B: 0 → 1 → 2 → 6 → 10  (came from above)\n",
    "```\n",
    "\n",
    "**Key insight:** Once the agent is in state 10, the future is identical regardless of how it got there:\n",
    "- Same actions available (LEFT, DOWN, RIGHT, UP)\n",
    "- Same transition probabilities (1/3 chance each direction due to slippery ice)\n",
    "- Same possible next states (6, 9, 11, 14)\n",
    "- Same reward structure (0 for moves, +1 only at Goal)\n",
    "\n",
    "The path taken to reach state 10 provides **zero additional information** about what will happen next.\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "Because FrozenLake is Markov:\n",
    "1. **Simple state representation** - We only need to track current position, not the entire history\n",
    "2. **Efficient learning** - The agent can learn values for 16 states instead of infinitely many histories\n",
    "3. **Optimal policies exist** - We can find a policy that maps states to actions without considering history\n",
    "4. **Dynamic programming works** - Bellman equations (covered later) rely on the Markov property\n",
    "\n",
    "> **Checkpoint — You should now understand:**\n",
    "> - What the Markov property means: \"the future is independent of the past, given the present\"\n",
    "> - Why FrozenLake satisfies the Markov property\n",
    "> - Why the Markov property makes RL problems tractable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate FrozenLake as a Fully Observable MDP\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=True)\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "print(\"FrozenLake as a Fully Observable MDP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nThe observation IS the state: {observation}\")\n",
    "print(f\"State type: {type(observation).__name__}\")\n",
    "print(f\"Total number of states: {env.observation_space.n}\")\n",
    "print(\"\\nIn FrozenLake:\")\n",
    "print(\"- The agent knows exactly which cell it's in\")\n",
    "print(\"- State = row * 4 + column (for 4x4 grid)\")\n",
    "print(f\"- Current state {observation} means row {observation // 4}, column {observation % 4}\")\n",
    "\n",
    "# Visualize\n",
    "visualize_frozenlake(env, current_state=observation, title=f\"State = {observation} (Row {observation // 4}, Col {observation % 4})\")\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Observable vs Partially Observable\n",
    "\n",
    "### Fully Observable Environment (MDP)\n",
    "\n",
    "When the agent **directly observes** the environment state:\n",
    "\n",
    "$$O_t = S_t^a = S_t^e$$\n",
    "\n",
    "- Agent state = Environment state = Information state\n",
    "- This is called a **Markov Decision Process (MDP)**\n",
    "- Example: FrozenLake - the agent knows exactly which cell it's in\n",
    "\n",
    "### Partially Observable Environment (POMDP)\n",
    "\n",
    "When the agent only **indirectly observes** the environment:\n",
    "\n",
    "$$O_t \\neq S_t^e$$\n",
    "\n",
    "- Agent state ≠ Environment state\n",
    "- This is called a **Partially Observable MDP (POMDP)**\n",
    "\n",
    "> **Optional — Advanced Topic: POMDPs**\n",
    ">\n",
    "> In POMDPs, the agent cannot directly see the true state. Examples:\n",
    "> - A robot with a camera doesn't know its exact position\n",
    "> - A trading agent only sees current prices, not market sentiment\n",
    "> - A poker player only sees public cards, not opponents' hands\n",
    ">\n",
    "> In POMDPs, the agent must build its own state representation, such as:\n",
    "> - Complete history: $S_t^a = H_t$\n",
    "> - Beliefs (probability distribution over possible states)\n",
    "> - Recurrent neural network: $S_t^a = \\sigma(S_{t-1}^a W_s + O_t W_o)$\n",
    ">\n",
    "> We won't cover POMDPs in detail in this tutorial series, but they're important for real-world applications.\n",
    "\n",
    "> **Checkpoint — You should now understand:**\n",
    "> - The difference between fully observable (MDP) and partially observable (POMDP) environments\n",
    "> - Why FrozenLake is an MDP (agent knows exact state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Summary and Concept Map\n",
    "\n",
    "In this notebook, we covered the foundational concepts of Reinforcement Learning:\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "```\n",
    "REINFORCEMENT LEARNING FOUNDATIONS\n",
    "====================================\n",
    "\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    THE BIG PICTURE                      │\n",
    "│                                                         │\n",
    "│   Agent ◄───────────────────────────► Environment       │\n",
    "│      │         actions                    │             │\n",
    "│      │                                    │             │\n",
    "│      └────── observations, rewards ◄──────┘             │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "\n",
    "KEY IDEAS:\n",
    "──────────\n",
    "1. RL = Learning from interaction (trial and error)\n",
    "   └── No supervisor, only reward signals\n",
    "   \n",
    "2. The Agent-Environment Interface\n",
    "   └── Agents observe, act, receive rewards\n",
    "   └── History captures all past interactions\n",
    "   \n",
    "3. Sequential decision making\n",
    "   └── Actions have long-term consequences\n",
    "   └── Optimize cumulative reward, not immediate reward\n",
    "   \n",
    "4. The Markov property\n",
    "   └── \"The future depends only on the present\"\n",
    "   └── Current state contains all relevant information\n",
    "   \n",
    "5. MDP vs POMDP\n",
    "   └── MDP: Agent sees true state (like FrozenLake)\n",
    "   └── POMDP: Agent sees partial observations\n",
    "```\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the next notebook (**01_2_policies_and_exploration.ipynb**), we'll learn about:\n",
    "- **Policies**: How agents decide which actions to take\n",
    "- **Exploration vs Exploitation**: The fundamental RL dilemma\n",
    "- **Prediction vs Control**: Two core problems in RL\n",
    "\n",
    "*Later, we will formalize the intuition of \"good states\" using value functions, which assign a single number to each state capturing its long-term desirability.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Your Turn\n",
    "\n",
    "Now it's time to test your understanding with some exercises!\n",
    "\n",
    "## Exercise 1: Conceptual Question\n",
    "\n",
    "Consider a modified FrozenLake environment where:\n",
    "- Each frozen tile (F) can only be stepped on 3 times before it breaks and becomes a hole\n",
    "- The environment tracks how many times each tile has been visited\n",
    "\n",
    "**Question:** Would this modified environment still satisfy the Markov property if we only observe the agent's position (0-15)? Why or why not?\n",
    "\n",
    "*Think about your answer before looking at the hint below.*\n",
    "\n",
    "<details>\n",
    "<summary>Click to see hint</summary>\n",
    "\n",
    "Think about what information would be missing if we only know the current position. Does the history of where we've been affect what happens next?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**No, it would NOT be Markov with just the position.** \n",
    "\n",
    "In the modified environment, knowing only the current position isn't enough - we also need to know how many times each tile has been visited. Two agents at the same position might face different outcomes: one might be standing on a tile that's about to break, while the other is on a fresh tile.\n",
    "\n",
    "To make it Markov, we'd need to expand the state to include visit counts for all tiles, e.g., `(position, visit_count_tile_0, visit_count_tile_1, ...)`.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Exercise 2: Code Task\n",
    "\n",
    "Modify the code below to run an episode and count:\n",
    "1. How many times the agent slipped (moved in an unintended direction)\n",
    "2. The total number of steps taken\n",
    "\n",
    "Then calculate the \"slip rate\" (slips / total steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Uncomment and complete the code below\n",
    "\n",
    "# env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "# observation, info = env.reset(seed=123)\n",
    "\n",
    "# action_names = {0: 'LEFT', 1: 'DOWN', 2: 'RIGHT', 3: 'UP'}\n",
    "# slip_count = 0\n",
    "# step_count = 0\n",
    "\n",
    "# for _ in range(100):  # Max 100 steps\n",
    "#     action = env.action_space.sample()\n",
    "#     \n",
    "#     # Calculate expected state (use the get_expected_state function from earlier)\n",
    "#     expected = get_expected_state(observation, action)\n",
    "#     \n",
    "#     new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     \n",
    "#     # TODO: Check if agent slipped and update slip_count\n",
    "#     # TODO: Update step_count\n",
    "#     \n",
    "#     observation = new_observation\n",
    "#     \n",
    "#     if terminated or truncated:\n",
    "#         break\n",
    "\n",
    "# # Calculate and print the slip rate\n",
    "# slip_rate = slip_count / step_count if step_count > 0 else 0\n",
    "# print(f\"Total steps: {step_count}\")\n",
    "# print(f\"Total slips: {slip_count}\")\n",
    "# print(f\"Slip rate: {slip_rate:.2%}\")\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why this matters:** Understanding slip rates helps you appreciate why FrozenLake is challenging. In the next notebook, we'll see how different policies handle this stochasticity, and later you'll learn algorithms that explicitly account for uncertain transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! You've completed Part 1.1 of the RL Tutorial!**\n",
    "\n",
    "Key takeaways:\n",
    "- RL agents learn from interaction with an environment\n",
    "- The reward signal guides learning toward good behavior\n",
    "- Sequential decisions require long-term thinking\n",
    "- The Markov property simplifies RL problems significantly\n",
    "\n",
    "Next: 01_2_policies_and_exploration.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}