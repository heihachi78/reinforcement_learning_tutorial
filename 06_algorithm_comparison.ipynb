{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Algorithm Comparison and Conclusion\n",
    "\n",
    "In this final notebook, we'll compare all the RL algorithms we've learned and provide a comprehensive summary of the tutorial.\n",
    "\n",
    "## What You'll Learn\n",
    "- Side-by-side comparison of all algorithms\n",
    "- Performance benchmarks\n",
    "- When to use which algorithm\n",
    "- Complete summary of RL concepts\n",
    "- Next steps for further learning\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']\n",
    "\n",
    "print(f\"FrozenLake: {n_states} states, {n_actions} actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MDP for DP methods\n",
    "def extract_mdp(env):\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    return P, R\n",
    "\n",
    "P, R = extract_mdp(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_policy(Q, title=\"Policy\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            best_action = np.argmax(Q[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Implement All Algorithms\n",
    "\n",
    "Let's implement all the algorithms we've learned in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# DYNAMIC PROGRAMMING\n",
    "# =====================\n",
    "\n",
    "def policy_iteration(P, R, gamma, theta=1e-8):\n",
    "    \"\"\"Policy Iteration (DP method, requires model).\"\"\"\n",
    "    n_states, n_actions = R.shape\n",
    "    \n",
    "    # Initialize random policy\n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "    \n",
    "    iterations = 0\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        V = np.zeros(n_states)\n",
    "        while True:\n",
    "            V_new = np.zeros(n_states)\n",
    "            for s in range(n_states):\n",
    "                for a in range(n_actions):\n",
    "                    V_new[s] += policy[s, a] * (R[s, a] + gamma * np.sum(P[s, a] * V))\n",
    "            if np.max(np.abs(V_new - V)) < theta:\n",
    "                break\n",
    "            V = V_new\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy = np.zeros((n_states, n_actions))\n",
    "        for s in range(n_states):\n",
    "            q_values = [R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)]\n",
    "            new_policy[s, np.argmax(q_values)] = 1.0\n",
    "        \n",
    "        iterations += 1\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            break\n",
    "        policy = new_policy\n",
    "    \n",
    "    # Compute Q from V\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    \n",
    "    return Q, iterations\n",
    "\n",
    "\n",
    "def value_iteration(P, R, gamma, theta=1e-8):\n",
    "    \"\"\"Value Iteration (DP method, requires model).\"\"\"\n",
    "    n_states, n_actions = R.shape\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    iterations = 0\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = np.max([R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)])\n",
    "        \n",
    "        iterations += 1\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    \n",
    "    # Extract Q\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    \n",
    "    return Q, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# MONTE CARLO\n",
    "# =====================\n",
    "\n",
    "def mc_control(env, gamma, n_episodes, epsilon=0.1, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "    \"\"\"Monte Carlo Control with Œµ-greedy (model-free).\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode_num in range(n_episodes):\n",
    "        # Generate episode\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        episode_rewards.append(sum(r for _, _, r in episode))\n",
    "        \n",
    "        # Update Q\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in sa_visited:\n",
    "                sa_visited.add((s, a))\n",
    "                returns_sum[s, a] += G\n",
    "                returns_count[s, a] += 1\n",
    "                Q[s, a] = returns_sum[s, a] / returns_count[s, a]\n",
    "        \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    return Q, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# TEMPORAL DIFFERENCE\n",
    "# =====================\n",
    "\n",
    "def sarsa(env, gamma, alpha, n_episodes, epsilon=0.1, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "    \"\"\"SARSA: On-policy TD Control (model-free).\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    episode_rewards = []\n",
    "    \n",
    "    def eps_greedy(state, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(n_actions)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = eps_greedy(state, epsilon)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_action = eps_greedy(next_state, epsilon)\n",
    "            \n",
    "            # SARSA update\n",
    "            td_target = reward + gamma * Q[next_state, next_action] * (not done)\n",
    "            Q[state, action] += alpha * (td_target - Q[state, action])\n",
    "            \n",
    "            state, action = next_state, next_action\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    return Q, episode_rewards\n",
    "\n",
    "\n",
    "def q_learning(env, gamma, alpha, n_episodes, epsilon=0.1, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "    \"\"\"Q-Learning: Off-policy TD Control (model-free).\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Œµ-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-Learning update\n",
    "            td_target = reward + gamma * np.max(Q[next_state]) * (not done)\n",
    "            Q[state, action] += alpha * (td_target - Q[state, action])\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    return Q, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Run All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "n_episodes_mf = 100000  # For model-free methods\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Running All Algorithms\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Policy Iteration\n",
    "print(\"\\n1. Policy Iteration (DP)...\")\n",
    "start = time.time()\n",
    "Q_pi, iters_pi = policy_iteration(P, R, gamma)\n",
    "time_pi = time.time() - start\n",
    "results['Policy Iteration'] = {'Q': Q_pi, 'time': time_pi, 'iterations': iters_pi}\n",
    "print(f\"   Done in {time_pi:.4f}s, {iters_pi} iterations\")\n",
    "\n",
    "# Value Iteration\n",
    "print(\"\\n2. Value Iteration (DP)...\")\n",
    "start = time.time()\n",
    "Q_vi, iters_vi = value_iteration(P, R, gamma)\n",
    "time_vi = time.time() - start\n",
    "results['Value Iteration'] = {'Q': Q_vi, 'time': time_vi, 'iterations': iters_vi}\n",
    "print(f\"   Done in {time_vi:.4f}s, {iters_vi} iterations\")\n",
    "\n",
    "# Monte Carlo\n",
    "print(f\"\\n3. Monte Carlo ({n_episodes_mf} episodes)...\")\n",
    "start = time.time()\n",
    "Q_mc, rewards_mc = mc_control(env, gamma, n_episodes_mf, epsilon=1.0, epsilon_decay=0.99995)\n",
    "time_mc = time.time() - start\n",
    "results['Monte Carlo'] = {'Q': Q_mc, 'time': time_mc, 'rewards': rewards_mc}\n",
    "print(f\"   Done in {time_mc:.2f}s\")\n",
    "\n",
    "# SARSA\n",
    "print(f\"\\n4. SARSA ({n_episodes_mf} episodes)...\")\n",
    "start = time.time()\n",
    "Q_sarsa, rewards_sarsa = sarsa(env, gamma, alpha=0.1, n_episodes=n_episodes_mf, \n",
    "                                epsilon=1.0, epsilon_decay=0.99995)\n",
    "time_sarsa = time.time() - start\n",
    "results['SARSA'] = {'Q': Q_sarsa, 'time': time_sarsa, 'rewards': rewards_sarsa}\n",
    "print(f\"   Done in {time_sarsa:.2f}s\")\n",
    "\n",
    "# Q-Learning\n",
    "print(f\"\\n5. Q-Learning ({n_episodes_mf} episodes)...\")\n",
    "start = time.time()\n",
    "Q_qlearn, rewards_qlearn = q_learning(env, gamma, alpha=0.1, n_episodes=n_episodes_mf,\n",
    "                                       epsilon=1.0, epsilon_decay=0.99995)\n",
    "time_qlearn = time.time() - start\n",
    "results['Q-Learning'] = {'Q': Q_qlearn, 'time': time_qlearn, 'rewards': rewards_qlearn}\n",
    "print(f\"   Done in {time_qlearn:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All algorithms complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Evaluate All Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, n_episodes=10000):\n",
    "    \"\"\"Evaluate a greedy policy derived from Q.\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        rewards.append(total_reward)\n",
    "    return np.array(rewards)\n",
    "\n",
    "print(\"Evaluating All Policies (10,000 episodes each)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name in results:\n",
    "    rewards = evaluate_policy(env, results[name]['Q'])\n",
    "    results[name]['success_rate'] = np.mean(rewards) * 100\n",
    "    results[name]['eval_rewards'] = rewards\n",
    "    print(f\"{name}: Success rate = {results[name]['success_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Comprehensive Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Q-values with optimal (Policy Iteration is our reference)\n",
    "Q_optimal = results['Policy Iteration']['Q']\n",
    "\n",
    "print(\"Q-value Accuracy (Mean Absolute Error vs Optimal)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name in results:\n",
    "    mae = np.mean(np.abs(results[name]['Q'] - Q_optimal))\n",
    "    results[name]['q_mae'] = mae\n",
    "    print(f\"{name}: MAE = {mae:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Success rates bar chart\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "names = list(results.keys())\n",
    "success_rates = [results[n]['success_rate'] for n in names]\n",
    "colors = ['#2ecc71', '#27ae60', '#3498db', '#e74c3c', '#9b59b6']\n",
    "bars = ax1.bar(range(len(names)), success_rates, color=colors, edgecolor='black')\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels([n.replace(' ', '\\n') for n in names], fontsize=9)\n",
    "ax1.set_ylabel('Success Rate (%)')\n",
    "ax1.set_title('Policy Performance')\n",
    "ax1.set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 2. Training time bar chart\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "times = [results[n]['time'] for n in names]\n",
    "bars = ax2.bar(range(len(names)), times, color=colors, edgecolor='black')\n",
    "ax2.set_xticks(range(len(names)))\n",
    "ax2.set_xticklabels([n.replace(' ', '\\n') for n in names], fontsize=9)\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Training Time')\n",
    "for bar, t in zip(bars, times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.02,\n",
    "            f'{t:.2f}s', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 3. Q-value accuracy\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "maes = [results[n]['q_mae'] for n in names]\n",
    "bars = ax3.bar(range(len(names)), maes, color=colors, edgecolor='black')\n",
    "ax3.set_xticks(range(len(names)))\n",
    "ax3.set_xticklabels([n.replace(' ', '\\n') for n in names], fontsize=9)\n",
    "ax3.set_ylabel('Mean Absolute Error')\n",
    "ax3.set_title('Q-value Accuracy (vs Optimal)')\n",
    "\n",
    "# 4-8. Policies side by side\n",
    "for idx, name in enumerate(names):\n",
    "    ax = fig.add_subplot(2, 5, 6 + idx)\n",
    "    plot_policy(results[name]['Q'], title=name, ax=ax)\n",
    "\n",
    "plt.suptitle('Comprehensive Algorithm Comparison', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for model-free methods\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "window = 1000\n",
    "model_free = ['Monte Carlo', 'SARSA', 'Q-Learning']\n",
    "colors_mf = ['#3498db', '#e74c3c', '#9b59b6']\n",
    "\n",
    "for name, color in zip(model_free, colors_mf):\n",
    "    rewards = results[name]['rewards']\n",
    "    smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smooth, label=name, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel(f'Reward (moving avg, window={window})')\n",
    "ax.set_title('Learning Curves: Model-Free Methods')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Algorithm':<20} {'Type':<15} {'Model':<12} {'Success':<10} {'Time':<12} {'Q-MAE':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "algo_info = {\n",
    "    'Policy Iteration': ('DP', 'Required'),\n",
    "    'Value Iteration': ('DP', 'Required'),\n",
    "    'Monte Carlo': ('MC', 'Free'),\n",
    "    'SARSA': ('TD (On)', 'Free'),\n",
    "    'Q-Learning': ('TD (Off)', 'Free')\n",
    "}\n",
    "\n",
    "for name in results:\n",
    "    algo_type, model = algo_info[name]\n",
    "    success = results[name]['success_rate']\n",
    "    time_taken = results[name]['time']\n",
    "    mae = results[name]['q_mae']\n",
    "    print(f\"{name:<20} {algo_type:<15} {model:<12} {success:>6.2f}%   {time_taken:>8.4f}s   {mae:>8.6f}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. When to Use Which Algorithm?\n",
    "\n",
    "## Decision Guide\n",
    "\n",
    "```\n",
    "Do you have a complete model of the environment?\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ YES ‚Üí Use Dynamic Programming\n",
    "‚îÇ         ‚îú‚îÄ‚îÄ Policy Iteration: Fewer iterations, more work per iteration\n",
    "‚îÇ         ‚îî‚îÄ‚îÄ Value Iteration: More iterations, less work per iteration\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ NO ‚Üí Use Model-Free Methods\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ‚îÄ Do episodes terminate?\n",
    "         ‚îÇ   ‚îú‚îÄ‚îÄ YES ‚Üí Can use Monte Carlo or TD\n",
    "         ‚îÇ   ‚îî‚îÄ‚îÄ NO ‚Üí Must use TD methods\n",
    "         ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ Do you want to learn the optimal policy?\n",
    "             ‚îú‚îÄ‚îÄ YES ‚Üí Q-Learning (off-policy)\n",
    "             ‚îî‚îÄ‚îÄ Policy you're following ‚Üí SARSA (on-policy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision flowchart visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Draw boxes\n",
    "boxes = [\n",
    "    {'pos': (0.5, 0.9), 'text': 'Start: Choose RL Algorithm', 'color': 'lightgray'},\n",
    "    {'pos': (0.5, 0.75), 'text': 'Have complete\\nmodel (P, R)?', 'color': 'lightyellow'},\n",
    "    {'pos': (0.2, 0.55), 'text': 'Dynamic\\nProgramming', 'color': 'lightgreen'},\n",
    "    {'pos': (0.8, 0.55), 'text': 'Model-Free\\nMethods', 'color': 'lightblue'},\n",
    "    {'pos': (0.1, 0.35), 'text': 'Policy\\nIteration', 'color': '#2ecc71'},\n",
    "    {'pos': (0.3, 0.35), 'text': 'Value\\nIteration', 'color': '#27ae60'},\n",
    "    {'pos': (0.65, 0.35), 'text': 'Episodes\\nterminate?', 'color': 'lightyellow'},\n",
    "    {'pos': (0.5, 0.15), 'text': 'Monte Carlo', 'color': '#3498db'},\n",
    "    {'pos': (0.95, 0.35), 'text': 'TD Methods', 'color': 'lightblue'},\n",
    "    {'pos': (0.8, 0.15), 'text': 'SARSA\\n(on-policy)', 'color': '#e74c3c'},\n",
    "    {'pos': (1.0, 0.15), 'text': 'Q-Learning\\n(off-policy)', 'color': '#9b59b6'},\n",
    "]\n",
    "\n",
    "for box in boxes:\n",
    "    rect = plt.Rectangle((box['pos'][0]-0.08, box['pos'][1]-0.06), 0.16, 0.12,\n",
    "                         facecolor=box['color'], edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(box['pos'][0], box['pos'][1], box['text'], ha='center', va='center',\n",
    "           fontsize=9, fontweight='bold')\n",
    "\n",
    "# Draw arrows with labels\n",
    "arrows = [\n",
    "    ((0.5, 0.84), (0.5, 0.81), ''),\n",
    "    ((0.42, 0.69), (0.28, 0.61), 'Yes'),\n",
    "    ((0.58, 0.69), (0.72, 0.61), 'No'),\n",
    "    ((0.15, 0.49), (0.12, 0.41), ''),\n",
    "    ((0.25, 0.49), (0.28, 0.41), ''),\n",
    "    ((0.73, 0.49), (0.58, 0.41), ''),\n",
    "    ((0.87, 0.49), (0.93, 0.41), 'No'),\n",
    "    ((0.57, 0.29), (0.52, 0.21), 'Yes'),\n",
    "    ((0.93, 0.29), (0.85, 0.21), ''),\n",
    "    ((0.97, 0.29), (0.98, 0.21), ''),\n",
    "]\n",
    "\n",
    "for start, end, label in arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    if label:\n",
    "        mid = ((start[0]+end[0])/2, (start[1]+end[1])/2)\n",
    "        ax.text(mid[0]+0.02, mid[1]+0.02, label, fontsize=9, fontweight='bold', color='darkblue')\n",
    "\n",
    "ax.set_xlim(-0.05, 1.15)\n",
    "ax.set_ylim(0.05, 1.0)\n",
    "ax.axis('off')\n",
    "ax.set_title('Algorithm Selection Guide', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Complete RL Concepts Summary\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "| Concept | Definition | Formula |\n",
    "|---------|------------|----------|\n",
    "| **State** | Current situation | $s \\in S$ |\n",
    "| **Action** | Decision to take | $a \\in A$ |\n",
    "| **Reward** | Immediate feedback | $R_t$ |\n",
    "| **Return** | Cumulative discounted reward | $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ |\n",
    "| **Policy** | Behavior strategy | $\\pi(a|s) = P[A_t=a|S_t=s]$ |\n",
    "| **State Value** | Expected return from state | $V^\\pi(s) = E_\\pi[G_t|S_t=s]$ |\n",
    "| **Action Value** | Expected return from (state, action) | $Q^\\pi(s,a) = E_\\pi[G_t|S_t=s, A_t=a]$ |\n",
    "\n",
    "## Bellman Equations\n",
    "\n",
    "| Equation | Purpose | Form |\n",
    "|----------|---------|------|\n",
    "| **Bellman Expectation (V)** | Value of policy | $V^\\pi(s) = \\sum_a \\pi(a|s)[R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s')]$ |\n",
    "| **Bellman Expectation (Q)** | Q of policy | $Q^\\pi(s,a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s')$ |\n",
    "| **Bellman Optimality (V)** | Optimal value | $V^*(s) = \\max_a[R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^*(s')]$ |\n",
    "| **Bellman Optimality (Q)** | Optimal Q | $Q^*(s,a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a \\max_{a'} Q^*(s',a')$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Summary\n",
    "\n",
    "### Dynamic Programming (Model-Based)\n",
    "\n",
    "| Algorithm | Key Idea | Update |\n",
    "|-----------|----------|--------|\n",
    "| **Policy Iteration** | Alternate eval & improve | Full policy evaluation |\n",
    "| **Value Iteration** | One-step lookahead | $V(s) = \\max_a[R + \\gamma \\sum P \\cdot V]$ |\n",
    "\n",
    "### Monte Carlo (Model-Free)\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Learns from** | Complete episodes |\n",
    "| **Update** | After episode ends |\n",
    "| **Uses** | Actual returns |\n",
    "| **Variance** | High |\n",
    "| **Bias** | None |\n",
    "\n",
    "### Temporal Difference (Model-Free)\n",
    "\n",
    "| Algorithm | Type | Update |\n",
    "|-----------|------|--------|\n",
    "| **TD(0)** | Prediction | $V(s) \\leftarrow V(s) + \\alpha[r + \\gamma V(s') - V(s)]$ |\n",
    "| **SARSA** | On-policy | $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$ |\n",
    "| **Q-Learning** | Off-policy | $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max Q(s',\\cdot) - Q(s,a)]$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Limitations and Next Steps\n",
    "\n",
    "## Limitations of Tabular Methods\n",
    "\n",
    "All methods in this tutorial are **tabular**: they maintain a table of values for each state (or state-action pair).\n",
    "\n",
    "**Problems with large state spaces:**\n",
    "- Memory: Can't store a table with millions of entries\n",
    "- Generalization: Each state learned independently\n",
    "- Continuous states: Infinite states, can't enumerate\n",
    "\n",
    "## What's Next: Function Approximation\n",
    "\n",
    "Instead of tables, use **function approximators** (like neural networks):\n",
    "\n",
    "$$V(s) \\approx V(s; \\theta)$$\n",
    "$$Q(s, a) \\approx Q(s, a; \\theta)$$\n",
    "\n",
    "Where $\\theta$ are learnable parameters.\n",
    "\n",
    "## Deep Reinforcement Learning\n",
    "\n",
    "Combining RL with deep neural networks:\n",
    "\n",
    "- **DQN** (Deep Q-Network): Q-Learning + Neural Network\n",
    "- **Policy Gradient**: Directly optimize policy parameters\n",
    "- **Actor-Critic**: Combine policy and value learning\n",
    "- **PPO, SAC, TD3**: State-of-the-art algorithms\n",
    "\n",
    "## Resources for Further Learning\n",
    "\n",
    "1. **Sutton & Barto** - \"Reinforcement Learning: An Introduction\" (free online)\n",
    "2. **David Silver's RL Course** - YouTube lectures from DeepMind\n",
    "3. **OpenAI Spinning Up** - Practical Deep RL tutorial\n",
    "4. **Stable Baselines3** - Ready-to-use RL algorithms in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Congratulations!\n",
    "\n",
    "You have completed this comprehensive Reinforcement Learning tutorial!\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "1. **Fundamentals** (Notebook 01)\n",
    "   - What makes RL unique\n",
    "   - Agent-environment interaction\n",
    "   - States, actions, rewards, policies\n",
    "\n",
    "2. **Mathematical Framework** (Notebook 02)\n",
    "   - Markov Decision Processes\n",
    "   - Bellman Equations\n",
    "   - Optimal value functions\n",
    "\n",
    "3. **Dynamic Programming** (Notebook 03)\n",
    "   - Policy Evaluation\n",
    "   - Policy Iteration\n",
    "   - Value Iteration\n",
    "\n",
    "4. **Monte Carlo Methods** (Notebook 04)\n",
    "   - Learning from episodes\n",
    "   - First-visit vs Every-visit\n",
    "   - MC Control\n",
    "\n",
    "5. **Temporal Difference** (Notebook 05)\n",
    "   - TD(0) Prediction\n",
    "   - SARSA (On-policy)\n",
    "   - Q-Learning (Off-policy)\n",
    "\n",
    "6. **Comparison & Summary** (This Notebook)\n",
    "   - All algorithms compared\n",
    "   - When to use what\n",
    "   - Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"   CONGRATULATIONS! You've completed the RL Tutorial!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìö Notebooks completed:\")\n",
    "print(\"   01. Introduction to Reinforcement Learning\")\n",
    "print(\"   02. MDPs and Bellman Equations\")\n",
    "print(\"   03. Dynamic Programming\")\n",
    "print(\"   04. Monte Carlo Methods\")\n",
    "print(\"   05. Temporal Difference Learning\")\n",
    "print(\"   06. Algorithm Comparison (this one!)\")\n",
    "print(\"\\nüéØ Algorithms mastered:\")\n",
    "print(\"   ‚Ä¢ Policy Iteration\")\n",
    "print(\"   ‚Ä¢ Value Iteration\")\n",
    "print(\"   ‚Ä¢ Monte Carlo Control\")\n",
    "print(\"   ‚Ä¢ SARSA\")\n",
    "print(\"   ‚Ä¢ Q-Learning\")\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"   ‚Ä¢ Try different environments (CartPole, MountainCar, etc.)\")\n",
    "print(\"   ‚Ä¢ Learn about Deep RL (DQN, Policy Gradients)\")\n",
    "print(\"   ‚Ä¢ Read Sutton & Barto's book for deeper understanding\")\n",
    "print(\"   ‚Ä¢ Implement your own RL agent for a real problem!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   Happy Learning! üéâ\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
