{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 6: Algorithm Comparison and Conclusion\n\nIn this final notebook, we'll compare all the RL algorithms we've learned and provide a comprehensive summary of the tutorial.\n\n## What You'll Learn\n- Recap of all fundamental RL algorithms\n- Side-by-side comparison of all algorithms\n- Performance benchmarks\n- When to use which algorithm\n- Complete summary of RL concepts\n- Next steps for further learning\n\nLet's begin!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "action_arrows = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']\n",
    "\n",
    "print(f\"FrozenLake: {n_states} states, {n_actions} actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MDP for DP methods\n",
    "def extract_mdp(env):\n",
    "    n_s = env.observation_space.n\n",
    "    n_a = env.action_space.n\n",
    "    P = np.zeros((n_s, n_a, n_s))\n",
    "    R = np.zeros((n_s, n_a))\n",
    "    for s in range(n_s):\n",
    "        for a in range(n_a):\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    return P, R\n",
    "\n",
    "P, R = extract_mdp(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_policy(Q, title=\"Policy\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    nrow, ncol = desc.shape\n",
    "    colors = {'S': 'lightblue', 'F': 'white', 'H': 'lightcoral', 'G': 'lightgreen'}\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            state = i * ncol + j\n",
    "            cell = desc[i, j]\n",
    "            \n",
    "            rect = plt.Rectangle((j, nrow-1-i), 1, 1, fill=True,\n",
    "                                 facecolor=colors.get(cell, 'white'), edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            best_action = np.argmax(Q[state])\n",
    "            \n",
    "            if cell not in ['H', 'G']:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, \n",
    "                       f'{cell}\\n{action_arrows[best_action]}',\n",
    "                       ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j + 0.5, nrow - 1 - i + 0.5, cell,\n",
    "                       ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, ncol)\n",
    "    ax.set_ylim(0, nrow)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 0. Recap from Notebooks 03-05\n\nWe've now learned the fundamental RL algorithms:\n\n**Dynamic Programming (Notebook 03):**\n- Model-based: Requires knowing P and R\n- Exact computation through Bellman updates\n- Guaranteed optimal solution\n- Examples: Policy Iteration, Value Iteration\n\n**Monte Carlo (Notebook 04):**\n- Model-free: Learns from experience\n- Episode-based: Must wait until termination\n- High variance, zero bias\n- Examples: First-visit MC, GLIE MC Control\n\n**Temporal Difference (Notebook 05):**\n- Model-free: Learns from experience\n- Online: Updates after each step\n- Bootstraps: Uses current estimates\n- Examples: SARSA (on-policy), Q-learning (off-policy)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 0.1 What This Notebook Does NOT Cover\n\n| Topic | Why Not Here | How It Differs From What We Cover |\n|-------|--------------|-----------------------------------|\n| **Deep RL implementations** | We compare tabular methods on small discrete state spaces. Deep RL uses neural networks for continuous/high-dimensional states. | Our algorithms maintain explicit Q[s,a] tables. DQN, PPO, A3C use neural networks Q(s,a;Œ∏) with gradient descent ‚Äî necessary for Atari, robotics, but conceptually build on these foundations. |\n| **Advanced algorithms (PPO, SAC, A3C)** | These are modern improvements combining multiple techniques. We focus on foundational algorithms that underpin them. | We compare core methods: DP, MC, TD. PPO (policy optimization), SAC (soft actor-critic), A3C (asynchronous AC) combine ideas from all these plus additional innovations ‚Äî they're the next step after mastering fundamentals. |\n| **Multi-agent reinforcement learning** | Multi-agent settings require game theory and coordination. We focus on single-agent optimization. | Our algorithms optimize one agent against a fixed environment. Multi-agent RL involves multiple learning agents with potentially conflicting goals ‚Äî changes learning dynamics fundamentally (requires Nash equilibria, communication). |\n| **Continuous control and action spaces** | Our comparison uses discrete actions (4 moves in FrozenLake). Continuous control requires policy gradients or discretization. | We compare algorithms that select from finite action sets. Continuous control (robot joints, vehicle steering) needs methods like DDPG, TD3, or SAC that handle continuous action spaces in R^n. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 0.2 How to Read This Notebook\n\nThis notebook provides a **systematic comparison framework** for evaluating RL algorithms:\n\n**1. Implementation Review (Section 1)**: All algorithms in one place for easy reference\n\n**2. Experimental Comparison (Sections 2-4)**: \n   - Run all algorithms on the same environment\n   - Measure performance, speed, and accuracy\n   - Visualize learned policies side-by-side\n\n**3. Comparative Analysis (Sections 5-6)**:\n   - When to use which algorithm\n   - Trade-offs between methods\n   - Decision guides for real-world problems\n\n**4. Comprehensive Summary (Sections 7-8)**:\n   - All RL concepts from the tutorial\n   - Algorithm properties table\n   - What comes next in your RL journey\n\n**How to engage**:\n- Run cells in order to reproduce the comparison\n- Pay attention to the performance metrics and trade-offs\n- Use the decision guides to understand when to apply each method\n- The \"Your Turn\" section at the end has exercises to test your understanding",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 0.3 Preview: What We'll Compare\n\nWe will systematically compare algorithms across multiple dimensions:\n\n**Performance Metrics**:\n- **Success Rate**: How often does the learned policy reach the goal?\n- **Q-value Accuracy**: How close are learned Q-values to the optimal solution?\n- **Training Time**: How long does the algorithm take to converge?\n\n**Algorithm Properties**:\n- **Model Requirements**: Does it need to know P and R?\n- **Update Frequency**: After each step, episode, or sweep?\n- **Variance vs Bias**: What are the statistical properties?\n- **Convergence Guarantees**: Does it provably reach the optimal solution?\n\n**Practical Considerations**:\n- **Sample Efficiency**: How many episodes/steps needed?\n- **Computational Cost**: Time per update vs number of updates\n- **Exploration Strategy**: How does it balance exploration and exploitation?\n- **Applicability**: When should you use each algorithm?\n\nBy the end, you'll have a clear decision framework for choosing the right algorithm for your problem.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Implement All Algorithms\n",
    "\n",
    "Let's implement all the algorithms we've learned in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# DYNAMIC PROGRAMMING\n",
    "# =====================\n",
    "\n",
    "def policy_iteration(P, R, gamma, theta=1e-8):\n",
    "    \"\"\"Policy Iteration (DP method, requires model).\"\"\"\n",
    "    n_states, n_actions = R.shape\n",
    "    \n",
    "    # Initialize random policy\n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "    \n",
    "    iterations = 0\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        V = np.zeros(n_states)\n",
    "        while True:\n",
    "            V_new = np.zeros(n_states)\n",
    "            for s in range(n_states):\n",
    "                for a in range(n_actions):\n",
    "                    V_new[s] += policy[s, a] * (R[s, a] + gamma * np.sum(P[s, a] * V))\n",
    "            if np.max(np.abs(V_new - V)) < theta:\n",
    "                break\n",
    "            V = V_new\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy = np.zeros((n_states, n_actions))\n",
    "        for s in range(n_states):\n",
    "            q_values = [R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)]\n",
    "            new_policy[s, np.argmax(q_values)] = 1.0\n",
    "        \n",
    "        iterations += 1\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            break\n",
    "        policy = new_policy\n",
    "    \n",
    "    # Compute Q from V\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    \n",
    "    return Q, iterations\n",
    "\n",
    "\n",
    "def value_iteration(P, R, gamma, theta=1e-8):\n",
    "    \"\"\"Value Iteration (DP method, requires model).\"\"\"\n",
    "    n_states, n_actions = R.shape\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    iterations = 0\n",
    "    while True:\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = np.max([R[s, a] + gamma * np.sum(P[s, a] * V) for a in range(n_actions)])\n",
    "        \n",
    "        iterations += 1\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    \n",
    "    # Extract Q\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = R[s, a] + gamma * np.sum(P[s, a] * V)\n",
    "    \n",
    "    return Q, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# MONTE CARLO\n",
    "# =====================\n",
    "\n",
    "def mc_control(env, gamma, n_episodes, epsilon=0.1, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "    \"\"\"Monte Carlo Control with Œµ-greedy (model-free).\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    returns_sum = np.zeros((n_states, n_actions))\n",
    "    returns_count = np.zeros((n_states, n_actions))\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode_num in range(n_episodes):\n",
    "        # Generate episode\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        episode_rewards.append(sum(r for _, _, r in episode))\n",
    "        \n",
    "        # Update Q\n",
    "        sa_visited = set()\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in sa_visited:\n",
    "                sa_visited.add((s, a))\n",
    "                returns_sum[s, a] += G\n",
    "                returns_count[s, a] += 1\n",
    "                Q[s, a] = returns_sum[s, a] / returns_count[s, a]\n",
    "        \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    return Q, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# TEMPORAL DIFFERENCE\n",
    "# =====================\n",
    "\n",
    "def sarsa(env, gamma, alpha, n_episodes, epsilon=0.1, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "    \"\"\"SARSA: On-policy TD Control (model-free).\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    episode_rewards = []\n",
    "    \n",
    "    def eps_greedy(state, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(n_actions)\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = eps_greedy(state, epsilon)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_action = eps_greedy(next_state, epsilon)\n",
    "            \n",
    "            # SARSA update\n",
    "            td_target = reward + gamma * Q[next_state, next_action] * (not done)\n",
    "            Q[state, action] += alpha * (td_target - Q[state, action])\n",
    "            \n",
    "            state, action = next_state, next_action\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    return Q, episode_rewards\n",
    "\n",
    "\n",
    "def q_learning(env, gamma, alpha, n_episodes, epsilon=0.1, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "    \"\"\"Q-Learning: Off-policy TD Control (model-free).\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Œµ-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-Learning update\n",
    "            td_target = reward + gamma * np.max(Q[next_state]) * (not done)\n",
    "            Q[state, action] += alpha * (td_target - Q[state, action])\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    return Q, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Run All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "n_episodes_mf = 100000  # For model-free methods\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Running All Algorithms\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Policy Iteration\n",
    "print(\"\\n1. Policy Iteration (DP)...\")\n",
    "start = time.time()\n",
    "Q_pi, iters_pi = policy_iteration(P, R, gamma)\n",
    "time_pi = time.time() - start\n",
    "results['Policy Iteration'] = {'Q': Q_pi, 'time': time_pi, 'iterations': iters_pi}\n",
    "print(f\"   Done in {time_pi:.4f}s, {iters_pi} iterations\")\n",
    "\n",
    "# Value Iteration\n",
    "print(\"\\n2. Value Iteration (DP)...\")\n",
    "start = time.time()\n",
    "Q_vi, iters_vi = value_iteration(P, R, gamma)\n",
    "time_vi = time.time() - start\n",
    "results['Value Iteration'] = {'Q': Q_vi, 'time': time_vi, 'iterations': iters_vi}\n",
    "print(f\"   Done in {time_vi:.4f}s, {iters_vi} iterations\")\n",
    "\n",
    "# Monte Carlo\n",
    "print(f\"\\n3. Monte Carlo ({n_episodes_mf} episodes)...\")\n",
    "start = time.time()\n",
    "Q_mc, rewards_mc = mc_control(env, gamma, n_episodes_mf, epsilon=1.0, epsilon_decay=0.99995)\n",
    "time_mc = time.time() - start\n",
    "results['Monte Carlo'] = {'Q': Q_mc, 'time': time_mc, 'rewards': rewards_mc}\n",
    "print(f\"   Done in {time_mc:.2f}s\")\n",
    "\n",
    "# SARSA\n",
    "print(f\"\\n4. SARSA ({n_episodes_mf} episodes)...\")\n",
    "start = time.time()\n",
    "Q_sarsa, rewards_sarsa = sarsa(env, gamma, alpha=0.1, n_episodes=n_episodes_mf, \n",
    "                                epsilon=1.0, epsilon_decay=0.99995)\n",
    "time_sarsa = time.time() - start\n",
    "results['SARSA'] = {'Q': Q_sarsa, 'time': time_sarsa, 'rewards': rewards_sarsa}\n",
    "print(f\"   Done in {time_sarsa:.2f}s\")\n",
    "\n",
    "# Q-Learning\n",
    "print(f\"\\n5. Q-Learning ({n_episodes_mf} episodes)...\")\n",
    "start = time.time()\n",
    "Q_qlearn, rewards_qlearn = q_learning(env, gamma, alpha=0.1, n_episodes=n_episodes_mf,\n",
    "                                       epsilon=1.0, epsilon_decay=0.99995)\n",
    "time_qlearn = time.time() - start\n",
    "results['Q-Learning'] = {'Q': Q_qlearn, 'time': time_qlearn, 'rewards': rewards_qlearn}\n",
    "print(f\"   Done in {time_qlearn:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All algorithms complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Evaluate All Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, n_episodes=10000):\n",
    "    \"\"\"Evaluate a greedy policy derived from Q.\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        rewards.append(total_reward)\n",
    "    return np.array(rewards)\n",
    "\n",
    "print(\"Evaluating All Policies (10,000 episodes each)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name in results:\n",
    "    rewards = evaluate_policy(env, results[name]['Q'])\n",
    "    results[name]['success_rate'] = np.mean(rewards) * 100\n",
    "    results[name]['eval_rewards'] = rewards\n",
    "    print(f\"{name}: Success rate = {results[name]['success_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Comprehensive Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Q-values with optimal (Policy Iteration is our reference)\n",
    "Q_optimal = results['Policy Iteration']['Q']\n",
    "\n",
    "print(\"Q-value Accuracy (Mean Absolute Error vs Optimal)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name in results:\n",
    "    mae = np.mean(np.abs(results[name]['Q'] - Q_optimal))\n",
    "    results[name]['q_mae'] = mae\n",
    "    print(f\"{name}: MAE = {mae:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Success rates bar chart\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "names = list(results.keys())\n",
    "success_rates = [results[n]['success_rate'] for n in names]\n",
    "colors = ['#2ecc71', '#27ae60', '#3498db', '#e74c3c', '#9b59b6']\n",
    "bars = ax1.bar(range(len(names)), success_rates, color=colors, edgecolor='black')\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels([n.replace(' ', '\\n') for n in names], fontsize=9)\n",
    "ax1.set_ylabel('Success Rate (%)')\n",
    "ax1.set_title('Policy Performance')\n",
    "ax1.set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 2. Training time bar chart\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "times = [results[n]['time'] for n in names]\n",
    "bars = ax2.bar(range(len(names)), times, color=colors, edgecolor='black')\n",
    "ax2.set_xticks(range(len(names)))\n",
    "ax2.set_xticklabels([n.replace(' ', '\\n') for n in names], fontsize=9)\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Training Time')\n",
    "for bar, t in zip(bars, times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.02,\n",
    "            f'{t:.2f}s', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 3. Q-value accuracy\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "maes = [results[n]['q_mae'] for n in names]\n",
    "bars = ax3.bar(range(len(names)), maes, color=colors, edgecolor='black')\n",
    "ax3.set_xticks(range(len(names)))\n",
    "ax3.set_xticklabels([n.replace(' ', '\\n') for n in names], fontsize=9)\n",
    "ax3.set_ylabel('Mean Absolute Error')\n",
    "ax3.set_title('Q-value Accuracy (vs Optimal)')\n",
    "\n",
    "# 4-8. Policies side by side\n",
    "for idx, name in enumerate(names):\n",
    "    ax = fig.add_subplot(2, 5, 6 + idx)\n",
    "    plot_policy(results[name]['Q'], title=name, ax=ax)\n",
    "\n",
    "plt.suptitle('Comprehensive Algorithm Comparison', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for model-free methods\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "window = 1000\n",
    "model_free = ['Monte Carlo', 'SARSA', 'Q-Learning']\n",
    "colors_mf = ['#3498db', '#e74c3c', '#9b59b6']\n",
    "\n",
    "for name, color in zip(model_free, colors_mf):\n",
    "    rewards = results[name]['rewards']\n",
    "    smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smooth, label=name, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel(f'Reward (moving avg, window={window})')\n",
    "ax.set_title('Learning Curves: Model-Free Methods')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Algorithm':<20} {'Type':<15} {'Model':<12} {'Success':<10} {'Time':<12} {'Q-MAE':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "algo_info = {\n",
    "    'Policy Iteration': ('DP', 'Required'),\n",
    "    'Value Iteration': ('DP', 'Required'),\n",
    "    'Monte Carlo': ('MC', 'Free'),\n",
    "    'SARSA': ('TD (On)', 'Free'),\n",
    "    'Q-Learning': ('TD (Off)', 'Free')\n",
    "}\n",
    "\n",
    "for name in results:\n",
    "    algo_type, model = algo_info[name]\n",
    "    success = results[name]['success_rate']\n",
    "    time_taken = results[name]['time']\n",
    "    mae = results[name]['q_mae']\n",
    "    print(f\"{name:<20} {algo_type:<15} {model:<12} {success:>6.2f}%   {time_taken:>8.4f}s   {mae:>8.6f}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. When to Use Which Algorithm?\n",
    "\n",
    "## Decision Guide\n",
    "\n",
    "```\n",
    "Do you have a complete model of the environment?\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ YES ‚Üí Use Dynamic Programming\n",
    "‚îÇ         ‚îú‚îÄ‚îÄ Policy Iteration: Fewer iterations, more work per iteration\n",
    "‚îÇ         ‚îî‚îÄ‚îÄ Value Iteration: More iterations, less work per iteration\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ NO ‚Üí Use Model-Free Methods\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ‚îÄ Do episodes terminate?\n",
    "         ‚îÇ   ‚îú‚îÄ‚îÄ YES ‚Üí Can use Monte Carlo or TD\n",
    "         ‚îÇ   ‚îî‚îÄ‚îÄ NO ‚Üí Must use TD methods\n",
    "         ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ Do you want to learn the optimal policy?\n",
    "             ‚îú‚îÄ‚îÄ YES ‚Üí Q-Learning (off-policy)\n",
    "             ‚îî‚îÄ‚îÄ Policy you're following ‚Üí SARSA (on-policy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision flowchart visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Draw boxes\n",
    "boxes = [\n",
    "    {'pos': (0.5, 0.9), 'text': 'Start: Choose RL Algorithm', 'color': 'lightgray'},\n",
    "    {'pos': (0.5, 0.75), 'text': 'Have complete\\nmodel (P, R)?', 'color': 'lightyellow'},\n",
    "    {'pos': (0.2, 0.55), 'text': 'Dynamic\\nProgramming', 'color': 'lightgreen'},\n",
    "    {'pos': (0.8, 0.55), 'text': 'Model-Free\\nMethods', 'color': 'lightblue'},\n",
    "    {'pos': (0.1, 0.35), 'text': 'Policy\\nIteration', 'color': '#2ecc71'},\n",
    "    {'pos': (0.3, 0.35), 'text': 'Value\\nIteration', 'color': '#27ae60'},\n",
    "    {'pos': (0.65, 0.35), 'text': 'Episodes\\nterminate?', 'color': 'lightyellow'},\n",
    "    {'pos': (0.5, 0.15), 'text': 'Monte Carlo', 'color': '#3498db'},\n",
    "    {'pos': (0.95, 0.35), 'text': 'TD Methods', 'color': 'lightblue'},\n",
    "    {'pos': (0.8, 0.15), 'text': 'SARSA\\n(on-policy)', 'color': '#e74c3c'},\n",
    "    {'pos': (1.0, 0.15), 'text': 'Q-Learning\\n(off-policy)', 'color': '#9b59b6'},\n",
    "]\n",
    "\n",
    "for box in boxes:\n",
    "    rect = plt.Rectangle((box['pos'][0]-0.08, box['pos'][1]-0.06), 0.16, 0.12,\n",
    "                         facecolor=box['color'], edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(box['pos'][0], box['pos'][1], box['text'], ha='center', va='center',\n",
    "           fontsize=9, fontweight='bold')\n",
    "\n",
    "# Draw arrows with labels\n",
    "arrows = [\n",
    "    ((0.5, 0.84), (0.5, 0.81), ''),\n",
    "    ((0.42, 0.69), (0.28, 0.61), 'Yes'),\n",
    "    ((0.58, 0.69), (0.72, 0.61), 'No'),\n",
    "    ((0.15, 0.49), (0.12, 0.41), ''),\n",
    "    ((0.25, 0.49), (0.28, 0.41), ''),\n",
    "    ((0.73, 0.49), (0.58, 0.41), ''),\n",
    "    ((0.87, 0.49), (0.93, 0.41), 'No'),\n",
    "    ((0.57, 0.29), (0.52, 0.21), 'Yes'),\n",
    "    ((0.93, 0.29), (0.85, 0.21), ''),\n",
    "    ((0.97, 0.29), (0.98, 0.21), ''),\n",
    "]\n",
    "\n",
    "for start, end, label in arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    if label:\n",
    "        mid = ((start[0]+end[0])/2, (start[1]+end[1])/2)\n",
    "        ax.text(mid[0]+0.02, mid[1]+0.02, label, fontsize=9, fontweight='bold', color='darkblue')\n",
    "\n",
    "ax.set_xlim(-0.05, 1.15)\n",
    "ax.set_ylim(0.05, 1.0)\n",
    "ax.axis('off')\n",
    "ax.set_title('Algorithm Selection Guide', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Complete RL Concepts Summary\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "| Concept | Definition | Formula |\n",
    "|---------|------------|----------|\n",
    "| **State** | Current situation | $s \\in S$ |\n",
    "| **Action** | Decision to take | $a \\in A$ |\n",
    "| **Reward** | Immediate feedback | $R_t$ |\n",
    "| **Return** | Cumulative discounted reward | $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ |\n",
    "| **Policy** | Behavior strategy | $\\pi(a|s) = P[A_t=a|S_t=s]$ |\n",
    "| **State Value** | Expected return from state | $V^\\pi(s) = E_\\pi[G_t|S_t=s]$ |\n",
    "| **Action Value** | Expected return from (state, action) | $Q^\\pi(s,a) = E_\\pi[G_t|S_t=s, A_t=a]$ |\n",
    "\n",
    "## Bellman Equations\n",
    "\n",
    "| Equation | Purpose | Form |\n",
    "|----------|---------|------|\n",
    "| **Bellman Expectation (V)** | Value of policy | $V^\\pi(s) = \\sum_a \\pi(a|s)[R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s')]$ |\n",
    "| **Bellman Expectation (Q)** | Q of policy | $Q^\\pi(s,a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^\\pi(s')$ |\n",
    "| **Bellman Optimality (V)** | Optimal value | $V^*(s) = \\max_a[R_s^a + \\gamma \\sum_{s'} P_{ss'}^a V^*(s')]$ |\n",
    "| **Bellman Optimality (Q)** | Optimal Q | $Q^*(s,a) = R_s^a + \\gamma \\sum_{s'} P_{ss'}^a \\max_{a'} Q^*(s',a')$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Summary\n",
    "\n",
    "### Dynamic Programming (Model-Based)\n",
    "\n",
    "| Algorithm | Key Idea | Update |\n",
    "|-----------|----------|--------|\n",
    "| **Policy Iteration** | Alternate eval & improve | Full policy evaluation |\n",
    "| **Value Iteration** | One-step lookahead | $V(s) = \\max_a[R + \\gamma \\sum P \\cdot V]$ |\n",
    "\n",
    "### Monte Carlo (Model-Free)\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Learns from** | Complete episodes |\n",
    "| **Update** | After episode ends |\n",
    "| **Uses** | Actual returns |\n",
    "| **Variance** | High |\n",
    "| **Bias** | None |\n",
    "\n",
    "### Temporal Difference (Model-Free)\n",
    "\n",
    "| Algorithm | Type | Update |\n",
    "|-----------|------|--------|\n",
    "| **TD(0)** | Prediction | $V(s) \\leftarrow V(s) + \\alpha[r + \\gamma V(s') - V(s)]$ |\n",
    "| **SARSA** | On-policy | $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$ |\n",
    "| **Q-Learning** | Off-policy | $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max Q(s',\\cdot) - Q(s,a)]$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 8. Enhanced Summary and Concept Map\n\n## Algorithm Properties Comparison\n\n| Algorithm | Model-Based | Online | Variance | Bias | Convergence Guarantee | Best Use Case |\n|-----------|-------------|--------|----------|------|----------------------|---------------|\n| **Policy Iteration** | Yes | Yes | N/A | N/A | Yes (finite steps) | Small state spaces, need exact solution |\n| **Value Iteration** | Yes | Yes | N/A | N/A | Yes (asymptotic) | Medium state spaces, can stop early |\n| **Monte Carlo** | No | No | High | None | Yes (asymptotic) | Episodic tasks, simple to implement |\n| **SARSA** | No | Yes | Low | Some | Yes (with conditions) | Safe exploration, learn actual behavior |\n| **Q-Learning** | No | Yes | Low | Some | Yes (with conditions) | Learn optimal policy, sample efficiency |\n\n## When to Use Each Algorithm\n\n### Use Dynamic Programming when:\n- You have complete knowledge of P and R\n- State space is small to medium\n- You need guaranteed convergence to optimal\n- Computation is cheaper than sampling\n\n### Use Monte Carlo when:\n- Environment is unknown (model-free)\n- Episodes naturally terminate\n- You can afford high variance\n- Simple implementation is preferred\n- Each episode provides complete information\n\n### Use SARSA when:\n- Environment is unknown (model-free)\n- You want to learn the policy you're actually following\n- Safety matters (avoid risky exploration)\n- On-policy learning is required\n\n### Use Q-Learning when:\n- Environment is unknown (model-free)\n- You want to learn the optimal policy\n- Can use off-policy data (experience replay)\n- Sample efficiency is important\n\n## Trade-offs Visualization\n\n**Sample Efficiency vs Computational Cost:**\n- DP: Low samples needed (uses model), high computation per update\n- MC: Many samples needed, low computation per sample\n- TD: Moderate samples, moderate computation\n\n**Variance vs Bias:**\n- DP: N/A (uses exact expectations)\n- MC: High variance, zero bias\n- TD: Low variance, some bias (from bootstrapping)\n\n**Flexibility vs Requirements:**\n- DP: Least flexible (needs model), strongest guarantees\n- MC: More flexible (model-free), requires episodic tasks\n- TD: Most flexible (model-free, online), works for continuing tasks"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Congratulations!\n",
    "\n",
    "You have completed this comprehensive Reinforcement Learning tutorial!\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "1. **Fundamentals** (Notebook 01)\n",
    "   - What makes RL unique\n",
    "   - Agent-environment interaction\n",
    "   - States, actions, rewards, policies\n",
    "\n",
    "2. **Mathematical Framework** (Notebook 02)\n",
    "   - Markov Decision Processes\n",
    "   - Bellman Equations\n",
    "   - Optimal value functions\n",
    "\n",
    "3. **Dynamic Programming** (Notebook 03)\n",
    "   - Policy Evaluation\n",
    "   - Policy Iteration\n",
    "   - Value Iteration\n",
    "\n",
    "4. **Monte Carlo Methods** (Notebook 04)\n",
    "   - Learning from episodes\n",
    "   - First-visit vs Every-visit\n",
    "   - MC Control\n",
    "\n",
    "5. **Temporal Difference** (Notebook 05)\n",
    "   - TD(0) Prediction\n",
    "   - SARSA (On-policy)\n",
    "   - Q-Learning (Off-policy)\n",
    "\n",
    "6. **Comparison & Summary** (This Notebook)\n",
    "   - All algorithms compared\n",
    "   - When to use what\n",
    "   - Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"   CONGRATULATIONS! You've completed the RL Tutorial!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìö Notebooks completed:\")\n",
    "print(\"   01. Introduction to Reinforcement Learning\")\n",
    "print(\"   02. MDPs and Bellman Equations\")\n",
    "print(\"   03. Dynamic Programming\")\n",
    "print(\"   04. Monte Carlo Methods\")\n",
    "print(\"   05. Temporal Difference Learning\")\n",
    "print(\"   06. Algorithm Comparison (this one!)\")\n",
    "print(\"\\nüéØ Algorithms mastered:\")\n",
    "print(\"   ‚Ä¢ Policy Iteration\")\n",
    "print(\"   ‚Ä¢ Value Iteration\")\n",
    "print(\"   ‚Ä¢ Monte Carlo Control\")\n",
    "print(\"   ‚Ä¢ SARSA\")\n",
    "print(\"   ‚Ä¢ Q-Learning\")\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"   ‚Ä¢ Try different environments (CartPole, MountainCar, etc.)\")\n",
    "print(\"   ‚Ä¢ Learn about Deep RL (DQN, Policy Gradients)\")\n",
    "print(\"   ‚Ä¢ Read Sutton & Barto's book for deeper understanding\")\n",
    "print(\"   ‚Ä¢ Implement your own RL agent for a real problem!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   Happy Learning! üéâ\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 9. What's Next?\n\nCongratulations! You've learned the foundations of reinforcement learning. Here's what comes next:\n\n## Immediate Next Steps\n\n**Function Approximation**: Handle large state spaces with linear/neural approximators\n- Instead of Q-tables, use Q(s,a;w) with parameters w\n- Linear: Q(s,a;w) = w^T œÜ(s,a) where œÜ are hand-crafted features\n- Neural: Q(s,a;Œ∏) approximated by deep neural networks\n- Enables scaling to millions of states (Atari games, robotics)\n\n**Deep RL**: DQN, A3C, PPO for high-dimensional problems (images, etc.)\n- DQN (Deep Q-Network): Q-Learning with neural networks + experience replay\n- A3C (Asynchronous Advantage Actor-Critic): Parallel agents learning together\n- PPO (Proximal Policy Optimization): Stable policy gradient method\n- Handles raw pixels, continuous control, complex environments\n\n**Policy Gradients**: Direct policy optimization (REINFORCE, A2C, PPO)\n- Instead of learning Q and deriving policy, directly optimize œÄ(a|s;Œ∏)\n- Better for continuous action spaces\n- Can learn stochastic policies\n- Foundation for modern deep RL (PPO, TRPO, SAC)\n\n## Advanced Topics\n\n**Model-based RL**: Learn environment models and plan\n- Learn P(s'|s,a) and R(s,a) from experience\n- Use learned model for planning (like DP)\n- Sample efficiency: planning with imagined rollouts\n- Examples: Dyna-Q, PETS, MuZero\n\n**Multi-agent RL**: Game theory, coordination, competition\n- Multiple agents learning simultaneously\n- Cooperative: Team rewards, coordination\n- Competitive: Zero-sum games, adversarial learning\n- Mixed: Negotiation, communication protocols\n\n**Exploration**: Better strategies than Œµ-greedy (UCB, Thompson sampling)\n- Upper Confidence Bound (UCB): Explore states with high uncertainty\n- Thompson Sampling: Bayesian approach to exploration\n- Curiosity-driven: Intrinsic motivation to explore\n- Count-based: Bonus for rarely visited states\n\n**Transfer Learning**: Apply knowledge across tasks\n- Pre-training on simpler tasks\n- Meta-learning: Learning to learn (MAML)\n- Hierarchical RL: Decompose complex tasks\n- Multi-task RL: Share knowledge across related tasks\n\n## Practical Applications\n\n**Robotics and Control**:\n- Robot manipulation (grasping, assembly)\n- Locomotion (walking, running, jumping)\n- Autonomous vehicles (self-driving cars, drones)\n- Industrial automation (optimization, scheduling)\n\n**Game Playing**:\n- Chess, Go (AlphaZero)\n- Starcraft, Dota (AlphaStar, OpenAI Five)\n- Atari games (DQN, Rainbow)\n- Poker (Pluribus)\n\n**Resource Management**:\n- Data center cooling (Google DeepMind)\n- Traffic light optimization\n- Energy grid management\n- Cloud resource allocation\n\n**Personalization**:\n- Recommendation systems (content, products)\n- Healthcare (treatment optimization, drug discovery)\n- Education (adaptive learning, tutoring systems)\n- Finance (portfolio optimization, trading)\n\n## Learning Resources\n\n**Books**:\n- Sutton & Barto: \"Reinforcement Learning: An Introduction\" (2nd ed, 2018)\n- Bertsekas: \"Dynamic Programming and Optimal Control\"\n- Szepesv√°ri: \"Algorithms for Reinforcement Learning\"\n\n**Online Courses**:\n- David Silver's RL Course (DeepMind, YouTube)\n- CS285 Deep RL (UC Berkeley, Sergey Levine)\n- OpenAI Spinning Up in Deep RL\n\n**Code & Libraries**:\n- Stable Baselines3: Ready-to-use RL algorithms\n- RLlib (Ray): Scalable RL library\n- OpenAI Gym/Gymnasium: Standard environments\n- CleanRL: Single-file implementations\n\nThe RL journey continues!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 10. Your Turn\n\nNow it's time to test your understanding with comprehensive exercises!\n\n## Exercise 1: Implement Custom Environment Comparison\n\nApply all algorithms to a different Gymnasium environment and compare results.\n\n**Task**: Complete the code below to compare algorithms on CliffWalking-v0\n\n```python\n# YOUR CODE HERE\n# 1. Create CliffWalking environment\n# 2. Run all 5 algorithms (Policy Iteration, Value Iteration, MC, SARSA, Q-Learning)\n# 3. Compare success rates and learning curves\n# 4. Which algorithm performs best? Why?\n\nimport gymnasium as gym\n\n# TODO: Create environment\ncliff_env = gym.make(\"CliffWalking-v0\")\n\n# TODO: Extract MDP for DP methods (adapt extract_mdp function)\n\n# TODO: Run all algorithms\n\n# TODO: Evaluate and compare results\n\n# Question: CliffWalking has a \"cliff\" that gives -100 reward.\n# Which algorithm (SARSA or Q-Learning) do you expect to be safer? Why?\n```\n\n<details>\n<summary>Click to see hint</summary>\n\nCliffWalking is a 4x12 gridworld where the agent must navigate from bottom-left to bottom-right, avoiding a cliff along the bottom edge.\n\nKey considerations:\n- SARSA is on-policy: learns the policy it follows (including exploration)\n- Q-Learning is off-policy: learns the optimal policy regardless of behavior\n- During learning with Œµ-greedy, which one will fall off the cliff more often?\n\n</details>\n\n<details>\n<summary>Click to see solution</summary>\n\n```python\nimport gymnasium as gym\n\n# Create environment\ncliff_env = gym.make(\"CliffWalking-v0\")\nn_states_cliff = cliff_env.observation_space.n\nn_actions_cliff = cliff_env.action_space.n\n\n# Extract MDP for DP\nP_cliff, R_cliff = extract_mdp(cliff_env)\n\n# Run algorithms\nprint(\"Comparing on CliffWalking-v0\")\nprint(\"=\" * 60)\n\n# DP methods\nQ_pi_cliff, _ = policy_iteration(P_cliff, R_cliff, gamma=0.99)\nQ_vi_cliff, _ = value_iteration(P_cliff, R_cliff, gamma=0.99)\n\n# Model-free (50k episodes)\nQ_mc_cliff, rewards_mc_cliff = mc_control(cliff_env, gamma=0.99, n_episodes=50000,\n                                          epsilon=1.0, epsilon_decay=0.9999)\nQ_sarsa_cliff, rewards_sarsa_cliff = sarsa(cliff_env, gamma=0.99, alpha=0.5, \n                                            n_episodes=50000, epsilon=0.1)\nQ_qlearn_cliff, rewards_qlearn_cliff = q_learning(cliff_env, gamma=0.99, alpha=0.5,\n                                                   n_episodes=50000, epsilon=0.1)\n\n# Compare learning curves\nwindow = 500\nplt.figure(figsize=(12, 5))\nplt.plot(np.convolve(rewards_sarsa_cliff, np.ones(window)/window, mode='valid'), \n         label='SARSA', alpha=0.8)\nplt.plot(np.convolve(rewards_qlearn_cliff, np.ones(window)/window, mode='valid'),\n         label='Q-Learning', alpha=0.8)\nplt.xlabel('Episode')\nplt.ylabel('Reward (moving avg)')\nplt.title('CliffWalking: SARSA vs Q-Learning')\nplt.legend()\nplt.show()\n\n# Key insight: SARSA learns a safer path (away from cliff) because it's on-policy\n# Q-Learning learns the optimal path (close to cliff) but falls off during learning\nprint(f\"SARSA mean reward: {np.mean(rewards_sarsa_cliff[-1000:]):.2f}\")\nprint(f\"Q-Learning mean reward: {np.mean(rewards_qlearn_cliff[-1000:]):.2f}\")\n```\n\n**Answer**: SARSA is safer during learning because it's on-policy. It learns to avoid the cliff because its exploration policy sometimes takes random actions near the edge. Q-Learning learns the optimal policy (walk along the cliff edge) but falls off more during training.\n\n</details>\n\n## Exercise 2: Hyperparameter Tuning Competition\n\n**Task**: Find the best hyperparameters for Q-Learning on FrozenLake\n\nExperiment with:\n- Learning rate Œ±: [0.01, 0.1, 0.5, 0.9]\n- Initial epsilon: [0.1, 0.5, 1.0]\n- Epsilon decay: [0.9999, 0.99995, 0.99999]\n- Number of episodes: [10000, 50000, 100000]\n\n```python\n# YOUR CODE HERE\n# Run grid search over hyperparameters\n# Track success rate for each combination\n# Find the best configuration\n\nbest_success = 0\nbest_params = {}\n\n# TODO: Implement grid search\n# for alpha in [0.01, 0.1, 0.5, 0.9]:\n#     for epsilon_init in [0.1, 0.5, 1.0]:\n#         for epsilon_decay in [0.9999, 0.99995, 0.99999]:\n#             # Train Q-Learning\n#             # Evaluate success rate\n#             # Track best\n\nprint(f\"Best parameters: {best_params}\")\nprint(f\"Best success rate: {best_success:.2f}%\")\n```\n\n<details>\n<summary>Click to see hint</summary>\n\nTips for hyperparameter tuning:\n- Higher learning rate Œ± ‚Üí faster learning but less stable\n- Higher initial epsilon ‚Üí more exploration early on\n- Slower epsilon decay ‚Üí explore for longer\n- More episodes ‚Üí more time to learn but diminishing returns\n\nFrozenLake is stochastic (slippery ice), so you need:\n- Enough episodes to overcome randomness\n- Balanced exploration (not too greedy too fast)\n\n</details>\n\n<details>\n<summary>Click to see solution</summary>\n\n```python\nimport itertools\n\n# Define parameter grid\nparam_grid = {\n    'alpha': [0.01, 0.1, 0.5, 0.9],\n    'epsilon': [0.1, 0.5, 1.0],\n    'epsilon_decay': [0.9999, 0.99995, 0.99999],\n    'n_episodes': [50000]\n}\n\nbest_success = 0\nbest_params = {}\nresults_grid = []\n\n# Grid search\nfor alpha in param_grid['alpha']:\n    for eps in param_grid['epsilon']:\n        for decay in param_grid['epsilon_decay']:\n            for n_ep in param_grid['n_episodes']:\n                # Train\n                Q, _ = q_learning(env, gamma=0.99, alpha=alpha, n_episodes=n_ep,\n                                 epsilon=eps, epsilon_decay=decay)\n                \n                # Evaluate\n                rewards = evaluate_policy(env, Q, n_episodes=5000)\n                success = np.mean(rewards) * 100\n                \n                results_grid.append({\n                    'alpha': alpha, 'epsilon': eps, 'decay': decay,\n                    'n_episodes': n_ep, 'success': success\n                })\n                \n                if success > best_success:\n                    best_success = success\n                    best_params = {'alpha': alpha, 'epsilon': eps, \n                                  'epsilon_decay': decay, 'n_episodes': n_ep}\n\nprint(\"Top 5 configurations:\")\nprint(\"-\" * 70)\nsorted_results = sorted(results_grid, key=lambda x: x['success'], reverse=True)\nfor i, r in enumerate(sorted_results[:5], 1):\n    print(f\"{i}. Œ±={r['alpha']:.2f}, Œµ={r['epsilon']:.1f}, \"\n          f\"decay={r['decay']:.5f} ‚Üí Success={r['success']:.2f}%\")\n\nprint(f\"\\nBest: {best_params} ‚Üí {best_success:.2f}%\")\n```\n\n**Typical findings**:\n- Œ± around 0.1-0.5 works best (not too fast, not too slow)\n- High initial Œµ (1.0) helps explore initially\n- Moderate decay (0.99995) balances exploration/exploitation\n- 50k+ episodes needed for reliable convergence\n\n</details>\n\n## Exercise 3: Conceptual Analysis\n\n**Question**: You're building an RL agent for the following scenarios. Which algorithm would you choose and why?\n\n### Scenario A: Robot Navigation\n- **Environment**: Robot moving in a warehouse\n- **State space**: Continuous (x, y, orientation)\n- **Actions**: Continuous (velocity, angular velocity)\n- **Episodes**: Never terminate (robot runs continuously)\n- **Model**: Unknown (real world)\n\n**Which algorithm? Why?**\n\n<details>\n<summary>Click to see answer</summary>\n\n**None of the tabular methods we learned!**\n\nThis requires:\n1. Function approximation (continuous states)\n2. Policy gradients or actor-critic (continuous actions)\n3. Online learning (non-episodic)\n\nBest choice: **PPO or SAC** (modern deep RL algorithms)\n- PPO: Robust policy gradient method\n- SAC: Handles continuous actions well, maximum entropy\n\nWhy not our algorithms:\n- DP: Need model + continuous states\n- MC: Need episodes to terminate\n- SARSA/Q-Learning: Tabular (can't handle continuous states/actions)\n\n**Key takeaway**: Tabular methods are foundations. Real robotics needs function approximation!\n\n</details>\n\n### Scenario B: Board Game AI (Chess)\n- **Environment**: Chess board\n- **State space**: Discrete but huge (~10^40 positions)\n- **Actions**: Discrete (legal moves, varies by position)\n- **Episodes**: Terminate (checkmate, draw)\n- **Model**: Known (chess rules)\n\n**Which algorithm? Why?**\n\n<details>\n<summary>Click to see answer</summary>\n\n**For tabular methods: Value Iteration** (if state space were small)\n\nBut in practice: **AlphaZero-style approach**\n- Monte Carlo Tree Search (MCTS) for planning\n- Deep neural networks for value/policy approximation\n- Self-play for training\n\nWhy not pure tabular:\n- State space too large (can't store table for 10^40 states)\n- Need function approximation with neural networks\n\nIf we had a small board game (Tic-Tac-Toe):\n- Use DP (Value Iteration) since we know the model\n- Guaranteed optimal solution\n- Fast because state space is small (~3^9 = 19,683 states)\n\n</details>\n\n### Scenario C: Personalized News Recommendations\n- **Environment**: User engagement with news articles\n- **State space**: User features (reading history, preferences)\n- **Actions**: Which article to recommend\n- **Episodes**: Each user session\n- **Model**: Unknown (user behavior is complex)\n\n**Which algorithm? Why?**\n\n<details>\n<summary>Click to see answer</summary>\n\n**Contextual Bandits** or **Off-policy RL (Q-Learning with replay)**\n\nBest approach:\n1. Collect logged data from existing system\n2. Use off-policy learning (Q-Learning, importance sampling)\n3. Function approximation (user/article features)\n\nWhy Q-Learning style:\n- Off-policy: Can learn from historical data\n- Don't need to explore on live users (can use logged data)\n- Model-free: User behavior too complex to model\n\nWhy not SARSA:\n- On-policy: Would need to explore with current policy\n- Risky: Bad recommendations hurt user experience\n\nModern approach: Contextual bandits\n- Simpler than full RL (no long-term rewards)\n- Faster learning\n- Better suited for immediate feedback\n\n</details>\n\n## Bonus Challenge: Implement Your Own Environment\n\nCreate a custom Gymnasium environment and compare algorithms!\n\n```python\n# YOUR CODE HERE\n# Define a simple custom MDP (e.g., a grid world with special rules)\n# Implement as a Gymnasium environment\n# Run all algorithms and compare\n\n# Example: 5x5 grid with:\n# - Start at (0,0)\n# - Goal at (4,4) with reward +10\n# - Special power-up at (2,2) that doubles future rewards\n# - Traps that give -5 reward\n\n# Hint: Inherit from gym.Env and implement:\n# - reset()\n# - step(action)\n# - Define observation_space and action_space\n```\n\nGood luck! These exercises will solidify your understanding of when and how to apply RL algorithms.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"   Thank you for completing the RL Tutorial!\")\nprint(\"=\"*70)\nprint(\"\\nKey Takeaways:\")\nprint(\"- DP (Policy/Value Iteration): Model-based, exact, optimal\")\nprint(\"- MC (Monte Carlo): Model-free, episodic, unbiased\")\nprint(\"- TD (SARSA/Q-Learning): Model-free, online, efficient\")\nprint(\"\\nRemember:\")\nprint(\"- No single algorithm is best for all problems\")\nprint(\"- Consider: model availability, state space size, episode structure\")\nprint(\"- Tabular methods are foundations for modern deep RL\")\nprint(\"\\nNext Steps:\")\nprint(\"- Experiment with different environments\")\nprint(\"- Try function approximation\")\nprint(\"- Learn about deep RL (DQN, PPO, SAC)\")\nprint(\"- Build your own RL application!\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"   The journey continues... Happy learning!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}